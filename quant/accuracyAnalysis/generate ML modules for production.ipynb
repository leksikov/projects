{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import BDay\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline\n",
    "\n",
    "from pyfinance import TSeries\n",
    "\n",
    "from hurst import compute_Hc, random_walk\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from hurst import compute_Hc, random_walk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShannonReallyFast(shannon_df):\n",
    "    #print(shannon_df, 'suka')\n",
    "    patterns = {'000', '001', '010', '100', '101', '110', '011', '111'}\n",
    "    shannon_df = pd.DataFrame(shannon_df.copy(), columns=['returns'])\n",
    "    shannon_df['pattern1'] = np.sign(shannon_df[['returns']])\n",
    "    shannon_df['pattern2'] = np.sign(shannon_df[['returns']].shift(1))\n",
    "    shannon_df['pattern3'] = np.sign(shannon_df[['returns']].shift(2))\n",
    "    shannon_df.dropna(inplace=True)\n",
    "    shannon_df = shannon_df[(shannon_df.pattern1 != 0.0) & (shannon_df.pattern2 != 0.0) & (shannon_df.pattern3 != 0.0)]\n",
    "    shannon_df['merged'] = shannon_df.pattern1.astype('str') + shannon_df.pattern2.astype('str') + shannon_df.pattern3.astype('str')\n",
    "    shannon_df['merged'] = shannon_df['merged'].str.replace('.0', '')\n",
    "    shannon_df['merged'] = shannon_df['merged'].str.replace('-1', '0')\n",
    "    prob_df = shannon_df.groupby('merged').count()['returns']\n",
    "    pattern_df = pd.DataFrame(patterns).set_index(0).sort_values(by=0)\n",
    "    #prob_df = prob_df.drop(index='111')\n",
    "    pattern_df = pattern_df.join(prob_df).fillna(0)\n",
    "    \n",
    "    ProbSum = 0.0\n",
    "    for pattern in patterns:\n",
    "        \n",
    "        p = pattern_df.loc[pattern] / pattern_df.returns.sum()      \n",
    "        value = p * np.log2(p)        \n",
    "        if math.isnan(value):\n",
    "            continue\n",
    "        ProbSum = ProbSum + value\n",
    "    Shannon_val = -ProbSum\n",
    "    \n",
    "    return Shannon_val\n",
    "    \n",
    "\n",
    "def ShannonFast(df):\n",
    "    # wrong pattern match\n",
    "    df=df[df!=0.0]\n",
    "    shift_returns = df.shift(1)\n",
    "    shift_returns2 = df.shift(2)\n",
    "    shift_returns3 = df.shift(3)\n",
    "    \n",
    "    #df['test']  = np.sign(df['returns']).astype('str').dropna() + np.sign(df['shift_returns2']).astype('str').dropna() +  np.sign(df['shift_returns3']).dropna().astype('str')\n",
    "    \n",
    "    df =df.dropna(axis=0)\n",
    "    \n",
    "    Pattern  = np.sign(df.shift(1).dropna()).astype('str') + np.sign(shift_returns2.dropna()).astype('str') +  np.sign(shift_returns3.dropna()).astype('str')\n",
    "    \n",
    "    Pattern=Pattern.dropna().str.replace('.0','')\n",
    "    patternList = Pattern.dropna().str.replace('.0','').unique().tolist()\n",
    "    total = 0.0\n",
    "    ShannonPatterns = {}\n",
    "    \n",
    "    for e in patternList:\n",
    "        ShannonPatterns[e] = 0\n",
    "\n",
    "    for pattern in ShannonPatterns.keys():        \n",
    "        value = (len(np.where(Pattern==pattern)[0]))\n",
    "        ShannonPatterns[pattern] +=  value\n",
    "        total += value\n",
    "        \n",
    "    \n",
    "    ProbSum = 0.0\n",
    "    for pattern in ShannonPatterns.keys():\n",
    "        p = ShannonPatterns[pattern] / total        \n",
    "        value = p * np.log2(p)        \n",
    "        if math.isnan(value):\n",
    "            continue\n",
    "        ProbSum = ProbSum + value\n",
    "    Shannon_val = -ProbSum\n",
    "    return Shannon_val\n",
    "\n",
    "def Shannon(df, patternSize):\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(df)):\n",
    "        chunks.append(df[i:i+patternSize])\n",
    "    \n",
    "    chunks = chunks[:-patternSize-1]\n",
    "    \n",
    "    \n",
    "    chunks = [np.array2string(x) for x in chunks.copy()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    chunks_set = list(set(chunks))\n",
    "    \n",
    "    \n",
    "    visited = {}\n",
    "    total = 0\n",
    "    for el in chunks_set:\n",
    "        if (el not in visited):\n",
    "            f = chunks.count(el)\n",
    "            visited[el] = f\n",
    "            total = total + f\n",
    "    ProbSum = 0.0\n",
    "    for el in visited:\n",
    "        p = visited[el]/total\n",
    "        value = p * np.log2(p)\n",
    "        #visited[el] = value\n",
    "        ProbSum = ProbSum + value\n",
    "    Shannon_val = -ProbSum\n",
    "    del visited, chunks\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Shannon_val\n",
    "\n",
    "def marketMeannes(df_):\n",
    "    \n",
    "    m = np.median(df_) \n",
    "    nh = 0\n",
    "    nl = 0\n",
    "    \n",
    "    for i in range(1, len(df_)-1):\n",
    "        Pt = df_[i]\n",
    "        Py = df_[i-1]\n",
    "        \n",
    "        if (Py > m) & (Py > Pt):\n",
    "            nl += 1\n",
    "        elif (Py < m) & (Py < Pt):\n",
    "            nh += 1\n",
    "        else:\n",
    "            None\n",
    "    return (nl+nh)/(len(df_)-1)\n",
    "        \n",
    "    \n",
    "    \n",
    "def Momersion(df):\n",
    "    #print(np.where(df == 1)[0])\n",
    "    #df = df['returns'].copy() * df['returns'].shift(1)\n",
    "    df = df.copy() * df.shift(1)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = np.sign(df)\n",
    "    pos = len(np.where(df == 1)[0])\n",
    "    neg = len(np.where(df == -1)[0])\n",
    "    #zero = len(np.where(df == 0.0)[0])\n",
    "    if (pos + neg) == 0.0:\n",
    "        return -1.0\n",
    "    #print(pos, neg)\n",
    "    mom = (pos / (pos+neg )) #*100.0\n",
    "    return mom\n",
    "# https://pypi.org/project/hurst/\n",
    "def hurst(ts):\n",
    "    lags = range(2, 20)\n",
    "    tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "    # plot on log-log scale\n",
    "    #plt.plot(np.log(lags), np.log(tau)); plt.show()\n",
    "    # calculate Hurst as slope of log-log plot\n",
    "    #print(lags, tau)\n",
    "    m = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "\n",
    " \n",
    "    hurst = m[0]*2.0\n",
    "    #print ('hurst = ',hurst)\n",
    "    #plt.clf(), plt.close()\n",
    "    return hurst\n",
    "\n",
    "def MomersionDouble(df):\n",
    "\n",
    "    df = df[df!=0.0]\n",
    "    shift_returns = df.shift(1).fillna(0)\n",
    "    shift_returns2 = df.shift(2).fillna(0)\n",
    "    shift_returns3 = df.shift(3).fillna(0)\n",
    "    Pattern = np.sign(shift_returns * shift_returns2)\n",
    "    Pattern2= np.sign(shift_returns2 * shift_returns3)\n",
    "    df = df.dropna()\n",
    "\n",
    "    pp = len(np.where( (Pattern == 1 ) & (Pattern2 == 1 ) )[0])\n",
    "    pm = len(np.where( (Pattern == 1 ) & (Pattern2 == -1 ) )[0])\n",
    "    mp = len(np.where( (Pattern == -1 ) & (Pattern2 == 1 ) )[0])\n",
    "    mm = len(np.where( (Pattern == -1 ) & (Pattern2 == -1 ) )[0])\n",
    "\n",
    "    total = 50+100*(pp+pm-mp-mm)/(pp + pm + mp+ mm)\n",
    "    #threshUp = total>=np.sqrt(len(df))\n",
    "\n",
    "    return total #(total, len(df), np.sqrt(len(df)))\n",
    "\n",
    "def proportion(df):\n",
    "    pp = len(np.where( (df >0.0  ) )[0])\n",
    "    mm = len(np.where( (df < 0.0 ) )[0])\n",
    "    \n",
    "    if mm == 0.0 or mm is None:\n",
    "        mm = 1\n",
    "    return pp/mm\n",
    "    \n",
    "def proportionPos(df):\n",
    "    pp = len(np.where( (df >0.0  ) )[0])\n",
    "    mm = len(np.where( (df < 0.0 ) )[0])\n",
    "    \n",
    "    if mm == 0.0 or mm is None:\n",
    "        mm = 1\n",
    "    return 100.0 * pp/(pp+mm) \n",
    "\n",
    "def autoCorrel(df, lag):\n",
    "    return pd.Series.autocorr(df, lag)\n",
    "\n",
    "\n",
    "def hurstF(ts):\n",
    "    lags = range(2, 20)\n",
    "    tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "    # plot on log-log scale\n",
    "    #plt.plot(np.log(lags), np.log(tau)); plt.show()\n",
    "    # calculate Hurst as slope of log-log plot\n",
    "    #print(lags, tau)\n",
    "    m = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "\n",
    " \n",
    "    hurst = m[0]*2.0\n",
    "    #print ('hurst = ',hurst)\n",
    "    #plt.clf(), plt.close()\n",
    "    return hurst\n",
    "\n",
    "\n",
    "def hurstF2(p):\n",
    "    lags = range(2,100)\n",
    "\n",
    "\n",
    "    variancetau = []; tau = []\n",
    "\n",
    "    for lag in lags: \n",
    "\n",
    "        #  Write the different lags into a vector to compute a set of tau or lags\n",
    "        tau.append(lag)\n",
    "\n",
    "        # Compute the log returns on all days, then compute the variance on the difference in log returns\n",
    "        # call this pp or the price difference\n",
    "        pp = np.subtract(p[lag:], p[:-lag])\n",
    "        variancetau.append(np.var(pp))\n",
    "\n",
    "    # we now have a set of tau or lags and a corresponding set of variances.\n",
    "    #print tau\n",
    "    #print variancetau\n",
    "\n",
    "    # plot the log of those variance against the log of tau and get the slope\n",
    "    m = np.polyfit(np.log10(tau),np.log10(variancetau),1)\n",
    "\n",
    "    hurst = m[0] / 2\n",
    "\n",
    "    return hurst\n",
    "\n",
    "def hurstF3(series):\n",
    "   \n",
    "\n",
    "    #H, c, data = compute_Hc(series.replace([np.inf, -np.inf], np.na).dropna(), kind='price', simplified=True)\n",
    "    H, c, data = compute_Hc(series.replace([np.inf, -np.inf], np.nan).dropna(), kind='random_walk', simplified=False)\n",
    "    return H\n",
    "\n",
    "def hurstF4(series):\n",
    "   \n",
    "    \n",
    "    H, c, data = compute_Hc(series, kind='random_walk', simplified=True)\n",
    "    return H\n",
    "\n",
    "\n",
    "\n",
    "def generateRWI2(df):\n",
    "    MomVal = Momersion(df['returns']) \n",
    "    MomDouble = MomersionDouble(df['returns'])\n",
    " \n",
    "    h1 =  hurstF(df[['price']])\n",
    "    h2 =  hurstF2(df[['price']])\n",
    "    h3 =  hurstF3(df['price'])\n",
    "\n",
    "    h4 = hurstF4(df['price'])\n",
    "   \n",
    "  \n",
    "    #df = df.copy().join(autoCorr_features(df[['returns']].copy()), rsuffix='_suka_')\n",
    "    \n",
    "    MMIR = marketMeannes(df['returns'])\n",
    "    \n",
    "    MMIP = marketMeannes(df['price'])\n",
    "    ShannonVal = ShannonFast(df['returns'])\n",
    "    prop = proportionPos(df['returns'])\n",
    "    correl_1 = autoCorrel(df.returns, 1)\n",
    "    correl_2 = autoCorrel(df.returns, 2)\n",
    "    correl_3 = autoCorrel(df.returns, 3)\n",
    "    correl_4 = autoCorrel(df.returns, 4)\n",
    "    correl_5 = autoCorrel(df.returns, 5)\n",
    "    correl_10 = autoCorrel(df.returns, 10)\n",
    "    correl_20 = autoCorrel(df.returns, 20)\n",
    "    correl_100 = autoCorrel(df.returns, 100)\n",
    "    correl_list = [correl_1, correl_2, correl_3, correl_4, correl_5, correl_10, correl_20, correl_100]\n",
    "    \n",
    "    var_std = df['returns'].std()\n",
    "    var_mean = df['returns'].mean()\n",
    "    var_median = df['returns'].mean()\n",
    "    \n",
    "    \n",
    "    return [MomVal, MomDouble, h1[0], h2[0], h3, h4, MMIR, MMIP, ShannonVal, prop, var_std, var_mean, var_median] + correl_list\n",
    "\n",
    "def generate_features(df):\n",
    "    \n",
    "    \n",
    "    df['returns'] = np.log(df['price']).pct_change(1)\n",
    "    \n",
    "    df['ROC_2'] =  np.log(df['price'].copy()).pct_change(2)\n",
    "    \n",
    "    df['ROC_3'] = np.log(df['price'].copy()).pct_change(3)\n",
    "    df['ROC_5'] = np.log(df['price'].copy()).pct_change(5)\n",
    "    df['ROC_20'] = np.log(df['price'].copy()).pct_change(20)\n",
    "    df['ROC_50'] = np.log(df['price'].copy()).pct_change(50)\n",
    "    df['ROC_100'] = np.log(df['price'].copy()).pct_change(100)\n",
    "    df['ROC_200'] = np.log(df['price'].copy()).pct_change(200)\n",
    "    #df['ROC_300'] = np.log(df['price'].copy()).pct_change(300)\n",
    "    df['ROC_500'] = np.log(df['price'].copy()).pct_change(500)\n",
    "    \n",
    "    \n",
    "    df['abs_returns'] = np.abs( np.log(np.abs(df['price'].copy())).pct_change())\n",
    "    \n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def transform_series(tmp):\n",
    "    #tmp = [e[0] if (type(e)==np.ndarray) else e for e in tmp.copy()]\n",
    "    df = pd.DataFrame(np.asarray(tmp)+100)\n",
    "    scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "    df_ = scaler.fit_transform(df)\n",
    "    df_ = pd.DataFrame(df_, columns=df.columns,index=df.index)\n",
    "    df = (100+df_*100)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.read_csv('btc hourly dataset/Bitfinex_BTCUSD_1h-3.csv')\n",
    "main['Date'] = pd.to_datetime(main['Date'],  format='%Y-%m-%d %I-%p')\n",
    "main = main.sort_values(by='Date').set_index('Date')\n",
    "\n",
    "main['range'] = 100*(main['High'] - main['Low'])/main['Low']\n",
    "main['adj_close'] = main['Close'] / (main['High']*0.5 + main['Low']*0.5)\n",
    "main['adj_close'] = main['adj_close'].replace([np.inf, -np.inf, 0], 0)\n",
    "main = main.rename(columns={'Close':'price'})\n",
    "\n",
    "\n",
    "main = main.drop(columns=['Symbol','Volume BTC']).rename(columns={'Volume USD':'volume'})\n",
    "main['volume_sma_5' ] = main['volume'].rolling(5).mean()\n",
    "main['volume_sma_20' ] = main['volume'].rolling(20).mean()\n",
    "main['volume_sma_50' ] = main['volume'].rolling(50).mean()\n",
    "main['volume_sma_500' ] = main['volume'].rolling(500).mean()\n",
    "\n",
    "main['volume_roc' ] = main['volume'].pct_change().fillna(0)\n",
    "main['volume_roc_5' ] = main['volume'].pct_change(5).fillna(0)\n",
    "main['volume_roc_20' ] = main['volume'].pct_change(20).fillna(0)\n",
    "main['volume_roc_50' ] = main['volume'].pct_change(50).fillna(0)\n",
    "main['volume_roc_500' ] = main['volume'].pct_change(500).fillna(0)\n",
    "\n",
    "\n",
    "main['returns'] = main.price.pct_change().fillna(0)\n",
    "main['hurst'] = main[['returns']].rolling(500).apply(hurstF4, raw=False) #.plot()\n",
    "\n",
    "main=main.drop(columns=['Open', 'High', 'Low'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main['month'] = main.index.month\n",
    "main['day'] = main.index.day\n",
    "main['hour'] = main.index.hour\n",
    "main['week_day'] = main.index.dayofweek\n",
    "\n",
    "df=main.copy()\n",
    "df = df.replace([np.inf, -np.inf],0).fillna(0)\n",
    "res = generate_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19139, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15311.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "19139 * 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergey\\.conda\\envs\\quant\\lib\\site-packages\\pandas\\core\\generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\sergey\\.conda\\envs\\quant\\lib\\site-packages\\ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "#### ROC_5 model 1\n",
    "results = []\n",
    "df = res.drop_duplicates().copy()\n",
    "df.loc[:,'shift_returns'] = df['ROC_5'].shift(-1).dropna()\n",
    "df = df[df.shift_returns != 0.0].dropna()\n",
    "df =df.drop(columns=['ROC_5'])\n",
    "df=df.drop_duplicates()\n",
    "y = df.iloc[:, [-1]]\n",
    "y.shift_returns = np.sign(y.shift_returns)\n",
    "X = df.iloc[:, :-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train =X.iloc[0:15311]\n",
    "y_train = y[0:15311]\n",
    "X_test = X.iloc[15400:]\n",
    "y_test = y.iloc[15400:]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=1000,\n",
    "                             random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7459222082810539"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_pickle('model_1_Xtest.pkl')\n",
    "y_test.to_pickle('model_1_ytest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'model_no1_roc_5.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergey\\.conda\\envs\\quant\\lib\\site-packages\\pandas\\core\\generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\sergey\\.conda\\envs\\quant\\lib\\site-packages\\ipykernel_launcher.py:21: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "#### basic\n",
    "results = []\n",
    "df = res.drop_duplicates().copy()\n",
    "df.loc[:,'shift_returns'] = df['returns'].shift(-1).dropna()\n",
    "df = df[df.shift_returns != 0.0].dropna()\n",
    "\n",
    "df=df.drop_duplicates()\n",
    "y = df.iloc[:, [-1]]\n",
    "y.shift_returns = np.sign(y.shift_returns)\n",
    "X = df.iloc[:, :-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train =X.iloc[0:15311]\n",
    "y_train = y[0:15311]\n",
    "X_test = X.iloc[15400:]\n",
    "y_test = y.iloc[15400:]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=1000,\n",
    "                             random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_pickle('model_0_Xtest.pkl')\n",
    "y_test.to_pickle('model_0_ytest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'volume', 'range', 'adj_close', 'volume_sma_5',\n",
       "       'volume_sma_20', 'volume_sma_50', 'volume_sma_500', 'volume_roc',\n",
       "       'volume_roc_5', 'volume_roc_20', 'volume_roc_50', 'volume_roc_500',\n",
       "       'returns', 'hurst', 'month', 'day', 'hour', 'week_day', 'ROC_2',\n",
       "       'ROC_3', 'ROC_5', 'ROC_20', 'ROC_50', 'ROC_100', 'ROC_200', 'ROC_500',\n",
       "       'abs_returns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3125, 1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54304"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_no0_basic.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start test date 2019-07-13 01:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.read_csv('btc hourly dataset/Bitfinex_BTCUSD_1h-3.csv')\n",
    "main['Date'] = pd.to_datetime(main['Date'],  format='%Y-%m-%d %I-%p')\n",
    "main = main.sort_values(by='Date').set_index('Date')\n",
    "main = main.resample('5h').last().drop_duplicates()\n",
    "\n",
    "main['range'] = 100*(main['High'] - main['Low'])/main['Low']\n",
    "main['adj_close'] = main['Close'] / (main['High']*0.5 + main['Low']*0.5)\n",
    "main['adj_close'] = main['adj_close'].replace([np.inf, -np.inf, 0], 0)\n",
    "main = main.rename(columns={'Close':'price'})\n",
    "\n",
    "\n",
    "main = main.drop(columns=['Volume BTC']).rename(columns={'Volume USD':'volume'})\n",
    "main['volume_sma_5' ] = main['volume'].rolling(5).mean()\n",
    "main['volume_sma_20' ] = main['volume'].rolling(20).mean()\n",
    "main['volume_sma_50' ] = main['volume'].rolling(50).mean()\n",
    "main['volume_sma_500' ] = main['volume'].rolling(500).mean()\n",
    "\n",
    "main['volume_roc' ] = main['volume'].pct_change().fillna(0)\n",
    "main['volume_roc_5' ] = main['volume'].pct_change(5).fillna(0)\n",
    "main['volume_roc_20' ] = main['volume'].pct_change(20).fillna(0)\n",
    "main['volume_roc_50' ] = main['volume'].pct_change(50).fillna(0)\n",
    "main['volume_roc_500' ] = main['volume'].pct_change(500).fillna(0)\n",
    "\n",
    "\n",
    "main['returns'] = main.price.pct_change().fillna(0)\n",
    "main['hurst'] = main[['returns']].rolling(500).apply(hurstF4, raw=False) #.plot()\n",
    "\n",
    "main=main.drop(columns=['Open', 'High', 'Low'])\n",
    "main['month'] = main.index.month\n",
    "main['day'] = main.index.day\n",
    "main['hour'] = main.index.hour\n",
    "main['week_day'] = main.index.dayofweek\n",
    "\n",
    "df=main.copy()\n",
    "df = df.replace([np.inf, -np.inf],0).fillna(0)\n",
    "res = generate_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Symbol'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-406aaeed09c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Symbol'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\quant\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4115\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4116\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4117\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4118\u001b[0m         )\n\u001b[0;32m   4119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\quant\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3912\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3913\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3914\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3916\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\quant\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3944\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3945\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3946\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3947\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\quant\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5339\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5340\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} not found in axis\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5341\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5342\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Symbol'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop(columns='Symbol', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3826*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.iloc[3100:][['price', 'returns']].plot(subplots=(2,1), figsize=(15,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergey\\.conda\\envs\\quant\\lib\\site-packages\\pandas\\core\\generic.py:5208: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "C:\\Users\\sergey\\.conda\\envs\\quant\\lib\\site-packages\\ipykernel_launcher.py:19: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results = []\n",
    "res.drop(columns='Symbol', inplace=True)\n",
    "df = res.drop_duplicates().copy()\n",
    "\n",
    "df.loc[:,'shift_returns'] = df['returns'].shift(-1).dropna()\n",
    "df = df[df.shift_returns != 0.0].dropna()\n",
    "y = df.iloc[:, [-1]]\n",
    "y.shift_returns = np.sign(y.shift_returns)\n",
    "X = df.iloc[:, :-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train =X.iloc[0:3060]\n",
    "y_train = y[0:3060]\n",
    "X_test = X.iloc[3100:]\n",
    "y_test = y.iloc[3100:]\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000, max_depth=1000,\n",
    "                             random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_pickle('model_2_Xtest.pkl')\n",
    "y_test.to_pickle('model_2_ytest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4863636363636364"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_no2_5hour.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "# loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
