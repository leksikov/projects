{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import BDay\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hurst import compute_Hc, random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from hurst import compute_Hc, random_walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShannonFast(df):\n",
    "    df=df[df!=0.0]\n",
    "    shift_returns = df.shift(1)\n",
    "    shift_returns2 = df.shift(2)\n",
    "    shift_returns3 = df.shift(3)\n",
    "    \n",
    "    #df['test']  = np.sign(df['returns']).astype('str').dropna() + np.sign(df['shift_returns2']).astype('str').dropna() +  np.sign(df['shift_returns3']).dropna().astype('str')\n",
    "    \n",
    "    df =df.dropna(axis=0)\n",
    "    \n",
    "    Pattern  = np.sign(df.dropna()).astype('str') + np.sign(shift_returns2.dropna()).astype('str') +  np.sign(shift_returns3.dropna()).astype('str')\n",
    "    \n",
    "    Pattern=Pattern.dropna().str.replace('.0','')\n",
    "    patternList = Pattern.dropna().str.replace('.0','').unique().tolist()\n",
    "    total = 0.0\n",
    "    ShannonPatterns = {}\n",
    "    \n",
    "    for e in patternList:\n",
    "        ShannonPatterns[e] = 0\n",
    "\n",
    "    for pattern in ShannonPatterns.keys():        \n",
    "        value = (len(np.where(Pattern==pattern)[0]))\n",
    "        ShannonPatterns[pattern] +=  value\n",
    "        total += value\n",
    "        \n",
    "    \n",
    "    ProbSum = 0.0\n",
    "    for pattern in ShannonPatterns.keys():\n",
    "        p = ShannonPatterns[pattern] / total        \n",
    "        value = p * np.log2(p)        \n",
    "        if math.isnan(value):\n",
    "            continue\n",
    "        ProbSum = ProbSum + value\n",
    "    Shannon_val = -ProbSum\n",
    "    return Shannon_val\n",
    "\n",
    "def Shannon(df, patternSize):\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(df)):\n",
    "        chunks.append(df[i:i+patternSize])\n",
    "    \n",
    "    chunks = chunks[:-patternSize-1]\n",
    "    \n",
    "    \n",
    "    chunks = [np.array2string(x) for x in chunks.copy()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    chunks_set = list(set(chunks))\n",
    "    \n",
    "    \n",
    "    visited = {}\n",
    "    total = 0\n",
    "    for el in chunks_set:\n",
    "        if (el not in visited):\n",
    "            f = chunks.count(el)\n",
    "            visited[el] = f\n",
    "            total = total + f\n",
    "    ProbSum = 0.0\n",
    "    for el in visited:\n",
    "        p = visited[el]/total\n",
    "        value = p * np.log2(p)\n",
    "        #visited[el] = value\n",
    "        ProbSum = ProbSum + value\n",
    "    Shannon_val = -ProbSum\n",
    "    del visited, chunks\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Shannon_val\n",
    "\n",
    "def marketMeannes(df):\n",
    "    \n",
    "    m = np.median(df) \n",
    "    nh = 0\n",
    "    nl = 0\n",
    "    \n",
    "    for i in range(1, len(df)-1):\n",
    "        Pt = df[i]\n",
    "        Py = df[i-1]\n",
    "        \n",
    "        if (Py > m) & (Py > Pt):\n",
    "            nl += 1\n",
    "        elif (Py < m) & (Py < Pt):\n",
    "            nh += 1\n",
    "        else:\n",
    "            None\n",
    "    return (nl+nh)/(len(df)-1)\n",
    "        \n",
    "    \n",
    "    \n",
    "def Momersion(df):\n",
    "    #print(np.where(df == 1)[0])\n",
    "    #df = df['returns'].copy() * df['returns'].shift(1)\n",
    "    df = df.copy() * df.shift(1)\n",
    "    \n",
    "    df = df.dropna()\n",
    "    df = np.sign(df)\n",
    "    pos = len(np.where(df == 1)[0])\n",
    "    neg = len(np.where(df == -1)[0])\n",
    "    #zero = len(np.where(df == 0.0)[0])\n",
    "    if (pos + neg) == 0.0:\n",
    "        return -1.0\n",
    "    #print(pos, neg)\n",
    "    mom = (pos / (pos+neg )) #*100.0\n",
    "    return mom\n",
    "# https://pypi.org/project/hurst/\n",
    "def hurst(ts):\n",
    "    lags = range(2, 20)\n",
    "    tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "    # plot on log-log scale\n",
    "    #plt.plot(np.log(lags), np.log(tau)); plt.show()\n",
    "    # calculate Hurst as slope of log-log plot\n",
    "    #print(lags, tau)\n",
    "    m = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "\n",
    " \n",
    "    hurst = m[0]*2.0\n",
    "    #print ('hurst = ',hurst)\n",
    "    #plt.clf(), plt.close()\n",
    "    return hurst\n",
    "\n",
    "def MomersionDouble(df):\n",
    "\n",
    "    df = df[df!=0.0]\n",
    "    shift_returns = df.shift(1).fillna(0)\n",
    "    shift_returns2 = df.shift(2).fillna(0)\n",
    "    shift_returns3 = df.shift(3).fillna(0)\n",
    "    Pattern = np.sign(shift_returns * shift_returns2)\n",
    "    Pattern2= np.sign(shift_returns2 * shift_returns3)\n",
    "    df = df.dropna()\n",
    "\n",
    "    pp = len(np.where( (Pattern == 1 ) & (Pattern2 == 1 ) )[0])\n",
    "    pm = len(np.where( (Pattern == 1 ) & (Pattern2 == -1 ) )[0])\n",
    "    mp = len(np.where( (Pattern == -1 ) & (Pattern2 == 1 ) )[0])\n",
    "    mm = len(np.where( (Pattern == -1 ) & (Pattern2 == -1 ) )[0])\n",
    "\n",
    "    total = (pp+pm-mp-mm)/(df.count()-1.0)\n",
    "    #threshUp = total>=np.sqrt(len(df))\n",
    "\n",
    "    return total #(total, len(df), np.sqrt(len(df)))\n",
    "\n",
    "def proportion(df):\n",
    "    pp = len(np.where( (df >0.0  ) )[0])\n",
    "    mm = len(np.where( (df < 0.0 ) )[0])\n",
    "    \n",
    "    if mm == 0.0 or mm is None:\n",
    "        mm = 1\n",
    "    return pp/mm\n",
    "    \n",
    "def proportionPos(df):\n",
    "    pp = len(np.where( (df >0.0  ) )[0])\n",
    "    mm = len(np.where( (df < 0.0 ) )[0])\n",
    "    \n",
    "    if mm == 0.0 or mm is None:\n",
    "        mm = 1\n",
    "    return 100.0 * pp/(pp+mm) \n",
    "\n",
    "def autoCorrel(df, lag):\n",
    "    return pd.Series.autocorr(df, lag)\n",
    "\n",
    "\n",
    "def hurstF(ts):\n",
    "    lags = range(2, 20)\n",
    "    tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]\n",
    "    # plot on log-log scale\n",
    "    #plt.plot(np.log(lags), np.log(tau)); plt.show()\n",
    "    # calculate Hurst as slope of log-log plot\n",
    "    #print(lags, tau)\n",
    "    m = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "\n",
    " \n",
    "    hurst = m[0]*2.0\n",
    "    #print ('hurst = ',hurst)\n",
    "    #plt.clf(), plt.close()\n",
    "    return hurst\n",
    "\n",
    "\n",
    "def hurstF2(p):\n",
    "    lags = range(2,100)\n",
    "\n",
    "\n",
    "    variancetau = []; tau = []\n",
    "\n",
    "    for lag in lags: \n",
    "\n",
    "        #  Write the different lags into a vector to compute a set of tau or lags\n",
    "        tau.append(lag)\n",
    "\n",
    "        # Compute the log returns on all days, then compute the variance on the difference in log returns\n",
    "        # call this pp or the price difference\n",
    "        pp = np.subtract(p[lag:], p[:-lag])\n",
    "        variancetau.append(np.var(pp))\n",
    "\n",
    "    # we now have a set of tau or lags and a corresponding set of variances.\n",
    "    #print tau\n",
    "    #print variancetau\n",
    "\n",
    "    # plot the log of those variance against the log of tau and get the slope\n",
    "    m = np.polyfit(np.log10(tau),np.log10(variancetau),1)\n",
    "\n",
    "    hurst = m[0] / 2\n",
    "\n",
    "    return hurst\n",
    "\n",
    "def hurstF3(series):\n",
    "   \n",
    "\n",
    "    #H, c, data = compute_Hc(series.replace([np.inf, -np.inf], np.na).dropna(), kind='price', simplified=True)\n",
    "    H, c, data = compute_Hc(series.replace([np.inf, -np.inf], np.nan).dropna(), kind='random_walk', simplified=False)\n",
    "    return H\n",
    "\n",
    "def hurstF4(series):\n",
    "   \n",
    "    \n",
    "    H, c, data = compute_Hc(series, kind='random_walk', simplified=True)\n",
    "    return H\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateRWI2(df):\n",
    "    MomVal = Momersion(df[['returns']]) \n",
    "    MomDouble = MomersionDouble(df['returns'])\n",
    " \n",
    "    h1 =  hurstF(df['price'])\n",
    "    h2 =  hurstF2(df['price'])\n",
    "    h3 =  hurstF3(df['price'])\n",
    "\n",
    "    h4 = hurstF4(df['price'])\n",
    "   \n",
    "  \n",
    "    #df = df.copy().join(autoCorr_features(df[['returns']].copy()), rsuffix='_suka_')\n",
    "    \n",
    "    MMIR = marketMeannes(df['returns'])\n",
    "    \n",
    "    MMIP = marketMeannes(df['price'])\n",
    "    ShannonVal = ShannonFast(df['returns'])\n",
    "    prop = proportionPos(df['returns'])\n",
    "    correl_1 = autoCorrel(df.returns, 1)\n",
    "    correl_2 = autoCorrel(df.returns, 2)\n",
    "    correl_3 = autoCorrel(df.returns, 3)\n",
    "    correl_4 = autoCorrel(df.returns, 4)\n",
    "    correl_5 = autoCorrel(df.returns, 5)\n",
    "    correl_10 = autoCorrel(df.returns, 10)\n",
    "    correl_20 = autoCorrel(df.returns, 20)\n",
    "    correl_100 = autoCorrel(df.returns, 100)\n",
    "    correl_list = [correl_1, correl_2, correl_3, correl_4, correl_5, correl_10, correl_20, correl_100]\n",
    "    \n",
    "    var_std = df['returns'].std()\n",
    "    var_mean = df['returns'].mean()\n",
    "    var_median = df['returns'].mean()\n",
    "    \n",
    "    \n",
    "    return [MomVal, MomDouble, h1, h2, h3, h4, MMIR, MMIP, ShannonVal, prop, var_std, var_mean, var_median] + correl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    df.columns = ['price']\n",
    "    \n",
    "    df['returns'] = np.log(df['price']/df['price'].shift(1))\n",
    "    \n",
    "    df['ROC_2'] =  np.log(df['price'].copy()).pct_change(2)\n",
    "    \n",
    "    df['ROC_3'] = np.log(df['price'].copy()).pct_change(3)\n",
    "    df['ROC_5'] = np.log(df['price'].copy()).pct_change(5)\n",
    "    df['ROC_20'] = np.log(df['price'].copy()).pct_change(20)\n",
    "    df['ROC_50'] = np.log(df['price'].copy()).pct_change(50)\n",
    "    df['ROC_100'] = np.log(df['price'].copy()).pct_change(100)\n",
    "    #df['ROC_200'] = np.log(df['price'].copy()).pct_change(200)\n",
    "    #df['ROC_300'] = np.log(df['price'].copy()).pct_change(300)\n",
    "    #df['ROC_500'] = np.log(df['price'].copy()).pct_change(500)\n",
    "    \n",
    "    \n",
    "    df['abs_returns'] =  np.log(np.abs(df['price'].copy())).pct_change()\n",
    "    \n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = [1, 1,  2, 2, 3, 4, 5, 10, 20,  50 , 100, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(0, 100):\n",
    "    print(i)\n",
    "    for lk in lookup:\n",
    "        for prob in proba:\n",
    "            tmp = random_walk(1500,proba=prob,  min_lookback=lk, max_lookback=lk) #np.random.uniform(0.61, 0.90,1)\n",
    "            tmp = [e[0] if (type(e)==np.ndarray) else e for e in tmp.copy()]\n",
    "            df = pd.DataFrame(np.asarray(tmp)+100)\n",
    "            scaler = MinMaxScaler(feature_range=(0.0, 1.0))\n",
    "            df_ = scaler.fit_transform(df)\n",
    "            df_ = pd.DataFrame(df_, columns=df.columns,index=df.index)\n",
    "            df = (1+df_)\n",
    "            res = generate_features(df)\n",
    "            df = df.replace([np.inf, -np.inf],0).fillna(0)\n",
    "\n",
    "            df['shift_returns'] = df['returns'].shift(-1).dropna()\n",
    "            df = df[df.shift_returns != 0.0].dropna()\n",
    "            y = df.iloc[:, [-1]]\n",
    "            y.shift_returns = np.sign(y.shift_returns)\n",
    "            X = df.iloc[:, 0:7]\n",
    "            \n",
    "            rwi_list = generateRWI2(df)\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "            \n",
    "            clf = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                                         random_state=0)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            score = accuracy_score(y_test, y_pred)\n",
    "            rwi_list.append(prob)\n",
    "            rwi_list.append(lk)\n",
    "            rwi_list.append(score)\n",
    "            results.append(rwi_list)\n",
    "        \n",
    "            \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MomVal', 'MomDouble', 'h1', 'h2', 'h3', 'h4', 'MMIR', 'MMIP', 'ShannonVal', 'prop', 'var_std', 'var_mean', 'var_median',   'correl_1', 'correl_2', 'correl_3', 'correl_4', 'correl_5', 'correl_10', 'correl_20', 'correl_100' ,  'prob', 'lk', 'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.DataFrame(results, columns=cols) #.corr(method='spearman')['score'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "correl_4     -0.232320\n",
       "score        -0.216674\n",
       "correl_2     -0.202318\n",
       "correl_3     -0.098243\n",
       "h3           -0.058173\n",
       "MMIR         -0.041131\n",
       "var_std      -0.037534\n",
       "h2           -0.033798\n",
       "correl_10    -0.028656\n",
       "h1           -0.000683\n",
       "prob          0.000000\n",
       "var_median    0.001696\n",
       "var_mean      0.001696\n",
       "prop          0.002538\n",
       "correl_5      0.012731\n",
       "MMIP          0.019375\n",
       "h4            0.024910\n",
       "correl_1      0.037290\n",
       "correl_100    0.078167\n",
       "correl_20     0.082354\n",
       "MomVal        0.184379\n",
       "MomDouble     0.184415\n",
       "ShannonVal    0.231301\n",
       "lk            1.000000\n",
       "Name: lk, dtype: float64"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df.corr(method='spearman')['lk'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_df = pd.DataFrame(scaler.fit_transform(accuracy_df), index=accuracy_df.index, columns=accuracy_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm_df.drop(columns=['prob', 'lk'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = norm_df.drop(columns='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = norm_df['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Ridge(alpha=0.001).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7924697254957049"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols2 = ['MomVal',\n",
    " 'MomDouble',\n",
    " 'h1',\n",
    " 'h2',\n",
    " 'h3',\n",
    " 'h4',\n",
    " 'MMIR',\n",
    " 'MMIP',\n",
    " 'ShannonVal',\n",
    " 'prop',\n",
    " 'var_std',\n",
    " 'var_mean',\n",
    " 'var_median',\n",
    " 'correl_1',\n",
    " 'correl_2',\n",
    " 'correl_3',\n",
    " 'correl_4',\n",
    " 'correl_5',\n",
    " 'correl_10',\n",
    " 'correl_20',\n",
    " 'correl_100',\n",
    " 'prob',\n",
    " 'lk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MomVal</th>\n",
       "      <td>-2.799705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMIR</th>\n",
       "      <td>-0.704664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ShannonVal</th>\n",
       "      <td>-0.585681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_1</th>\n",
       "      <td>-0.479390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prob</th>\n",
       "      <td>-0.467112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lk</th>\n",
       "      <td>-0.230983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h1</th>\n",
       "      <td>-0.142493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MMIP</th>\n",
       "      <td>-0.082755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_5</th>\n",
       "      <td>-0.059499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_10</th>\n",
       "      <td>-0.041930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_100</th>\n",
       "      <td>-0.019691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h2</th>\n",
       "      <td>-0.003787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prop</th>\n",
       "      <td>-0.000741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_20</th>\n",
       "      <td>0.003381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_mean</th>\n",
       "      <td>0.004233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_median</th>\n",
       "      <td>0.004233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h3</th>\n",
       "      <td>0.019910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_3</th>\n",
       "      <td>0.256828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_4</th>\n",
       "      <td>0.279328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correl_2</th>\n",
       "      <td>0.288051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>var_std</th>\n",
       "      <td>0.456262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h4</th>\n",
       "      <td>0.595630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MomDouble</th>\n",
       "      <td>2.697060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0\n",
       "MomVal     -2.799705\n",
       "MMIR       -0.704664\n",
       "ShannonVal -0.585681\n",
       "correl_1   -0.479390\n",
       "prob       -0.467112\n",
       "lk         -0.230983\n",
       "h1         -0.142493\n",
       "MMIP       -0.082755\n",
       "correl_5   -0.059499\n",
       "correl_10  -0.041930\n",
       "correl_100 -0.019691\n",
       "h2         -0.003787\n",
       "prop       -0.000741\n",
       "correl_20   0.003381\n",
       "var_mean    0.004233\n",
       "var_median  0.004233\n",
       "h3          0.019910\n",
       "correl_3    0.256828\n",
       "correl_4    0.279328\n",
       "correl_2    0.288051\n",
       "var_std     0.456262\n",
       "h4          0.595630\n",
       "MomDouble   2.697060"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reg.coef_, index=cols2).sort_values(by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c' argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with 'x' & 'y'.  Please use a 2-D array with a single row if you really want to specify the same RGB or RGBA value for all points.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ace1b7b8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(results, columns = cols)[['ShannonVal', 'score']].plot.scatter(x='ShannonVal', y='score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.price.pct_change().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features:\n",
    "# h1, h2, h3, h4 ,h5\n",
    "#mom, mom doub,e MMI, Shannon,\n",
    "# lookup, proportion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what else to-do?\n",
    "### correlations and import fatures with scores\n",
    "# predict accuracy\n",
    "\n",
    "# accuracy but time test-split\n",
    "\n",
    "\n",
    "# other ideas. Imported momentum indicators. And remov them\n",
    "# rolling regression and accuracy as an indicator for predictability\n",
    "# combine multiple states. trend, random, mean-reversal. Implement indicator for detection states. Implement robust ML algo. Or HMM state detection\n",
    "\n",
    "# aggregate into higher tf\n",
    "\n",
    "# test with korean stocks:\n",
    "# accuracy relations and prediction, state detection\n",
    "# predict accuracy of the sample TS\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
