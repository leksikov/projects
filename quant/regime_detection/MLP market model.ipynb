{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rwi\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rwi\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(name):\n",
    "    eurusd_df = pd.read_csv(name)\n",
    "    eurusd_df = eurusd_df.rename(columns={'<TICKER>':'Ticker', '<DTYYYYMMDD>':'Date', '<TIME>':'Time', '<OPEN>':'Open', '<HIGH>':'High', '<LOW>':'Low', '<CLOSE>':'Close', '<VOL>':'Vol'})\n",
    "    eurusd_df[['Date']] = pd.to_datetime(eurusd_df['Date'].astype('str'), yearfirst=True)\n",
    "    eurusd_df =eurusd_df.sort_values(by=['Date', 'Time'])\n",
    "    eur_df = eurusd_df[['Date', 'Time', 'Close', 'Open', 'High', 'Low', 'Vol']].rename(columns={'Close':'Close'})\n",
    "    eur_df['Change'] = eur_df['Close'].pct_change()\n",
    "    eur_df.dropna(inplace=True)\n",
    "    #eur_df = eur_df[eur_df.Date>'2020-01-01']\n",
    "    #eur_df = eur_df.set_index('Date')\n",
    "    return eur_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = pd.read_csv('SP500.csv', encoding='utf-8')\n",
    "sp500['Date'] = pd.to_datetime(sp500['Date'], yearfirst=True)\n",
    "sp500 = sp500.set_index('Date')\n",
    "#sp500 = sp500.loc[:'2019-12-31']\n",
    "sp500['price'] = sp500['Close']\n",
    "sp500 = sp500.resample('W').last()\n",
    "sp500 = rwi.generate_features(sp500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500['atr_5'] = sp500.returns.rolling(5).std()\n",
    "sp500['atr_20'] = sp500.returns.rolling(20).std()\n",
    "sp500['atr_50'] = sp500.returns.rolling(50).std()\n",
    "sp500['range'] = 100*(sp500['High'] - sp500['Low'])/sp500['Low']\n",
    "sp500['adj_close'] = sp500['Close'] / (sp500['High']*0.5 + sp500['Low']*0.5)\n",
    "sp500['adj_close'] = sp500['adj_close'].replace([np.inf, -np.inf, 0], 0)\n",
    "\n",
    "sp500.drop(columns=['ROC_100', 'ROC_200', 'ROC_500'], inplace=True)\n",
    "\n",
    "sp500.loc[:,'shift_returns'] = sp500['returns'].shift(-1)\n",
    "\n",
    "sp500.dropna(inplace=True) # .fillna(0, inplace=True)#\n",
    "\n",
    "sp500 = sp500[sp500.shift_returns != 0.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Change</th>\n",
       "      <th>price</th>\n",
       "      <th>returns</th>\n",
       "      <th>ROC_2</th>\n",
       "      <th>ROC_3</th>\n",
       "      <th>ROC_5</th>\n",
       "      <th>ROC_20</th>\n",
       "      <th>ROC_50</th>\n",
       "      <th>abs_returns</th>\n",
       "      <th>atr_5</th>\n",
       "      <th>atr_20</th>\n",
       "      <th>atr_50</th>\n",
       "      <th>range</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>shift_returns</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-12-20</th>\n",
       "      <td>1102.47</td>\n",
       "      <td>1097.86</td>\n",
       "      <td>1103.74</td>\n",
       "      <td>1093.88</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>1102.47</td>\n",
       "      <td>-0.003567</td>\n",
       "      <td>-0.003179</td>\n",
       "      <td>0.010009</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>0.110152</td>\n",
       "      <td>0.168190</td>\n",
       "      <td>0.003567</td>\n",
       "      <td>0.006654</td>\n",
       "      <td>0.020880</td>\n",
       "      <td>0.035614</td>\n",
       "      <td>0.901379</td>\n",
       "      <td>1.003331</td>\n",
       "      <td>0.021545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>1115.10</td>\n",
       "      <td>1126.60</td>\n",
       "      <td>1127.64</td>\n",
       "      <td>1114.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0100</td>\n",
       "      <td>1115.10</td>\n",
       "      <td>-0.010154</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>0.007824</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.104862</td>\n",
       "      <td>0.271322</td>\n",
       "      <td>0.010154</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>0.020948</td>\n",
       "      <td>0.034287</td>\n",
       "      <td>1.150869</td>\n",
       "      <td>0.994537</td>\n",
       "      <td>0.026443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-07</th>\n",
       "      <td>1066.19</td>\n",
       "      <td>1064.12</td>\n",
       "      <td>1067.13</td>\n",
       "      <td>1044.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>1066.19</td>\n",
       "      <td>-0.007177</td>\n",
       "      <td>-0.023700</td>\n",
       "      <td>-0.063448</td>\n",
       "      <td>-0.044853</td>\n",
       "      <td>-0.001977</td>\n",
       "      <td>0.325391</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.022574</td>\n",
       "      <td>0.031645</td>\n",
       "      <td>2.166587</td>\n",
       "      <td>1.009827</td>\n",
       "      <td>0.008703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-14</th>\n",
       "      <td>1075.51</td>\n",
       "      <td>1075.95</td>\n",
       "      <td>1077.81</td>\n",
       "      <td>1062.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0027</td>\n",
       "      <td>1075.51</td>\n",
       "      <td>0.008703</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>-0.014996</td>\n",
       "      <td>-0.062592</td>\n",
       "      <td>0.029372</td>\n",
       "      <td>0.380557</td>\n",
       "      <td>0.008703</td>\n",
       "      <td>0.017736</td>\n",
       "      <td>0.022008</td>\n",
       "      <td>0.030708</td>\n",
       "      <td>1.396088</td>\n",
       "      <td>1.004783</td>\n",
       "      <td>0.030817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>1104.49</td>\n",
       "      <td>1103.10</td>\n",
       "      <td>1107.24</td>\n",
       "      <td>1097.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>1104.49</td>\n",
       "      <td>-0.004228</td>\n",
       "      <td>0.026589</td>\n",
       "      <td>0.035292</td>\n",
       "      <td>0.011593</td>\n",
       "      <td>0.030333</td>\n",
       "      <td>0.378370</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>0.018309</td>\n",
       "      <td>0.020358</td>\n",
       "      <td>0.025371</td>\n",
       "      <td>0.881956</td>\n",
       "      <td>1.001896</td>\n",
       "      <td>0.030504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-02</th>\n",
       "      <td>3225.52</td>\n",
       "      <td>3282.33</td>\n",
       "      <td>3282.33</td>\n",
       "      <td>3214.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0177</td>\n",
       "      <td>3225.52</td>\n",
       "      <td>-0.021455</td>\n",
       "      <td>-0.031764</td>\n",
       "      <td>-0.012273</td>\n",
       "      <td>-0.004485</td>\n",
       "      <td>0.070022</td>\n",
       "      <td>0.150227</td>\n",
       "      <td>0.021455</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.014912</td>\n",
       "      <td>2.104409</td>\n",
       "      <td>0.992924</td>\n",
       "      <td>0.031190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-09</th>\n",
       "      <td>3327.71</td>\n",
       "      <td>3335.54</td>\n",
       "      <td>3341.42</td>\n",
       "      <td>3322.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>3327.71</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.009736</td>\n",
       "      <td>-0.000574</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.106319</td>\n",
       "      <td>0.175286</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.021489</td>\n",
       "      <td>0.011697</td>\n",
       "      <td>0.015431</td>\n",
       "      <td>0.580954</td>\n",
       "      <td>0.998781</td>\n",
       "      <td>0.015639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>2954.22</td>\n",
       "      <td>2916.90</td>\n",
       "      <td>2959.72</td>\n",
       "      <td>2855.84</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0082</td>\n",
       "      <td>2954.22</td>\n",
       "      <td>-0.122062</td>\n",
       "      <td>-0.134688</td>\n",
       "      <td>-0.119050</td>\n",
       "      <td>-0.109314</td>\n",
       "      <td>-0.005418</td>\n",
       "      <td>0.045619</td>\n",
       "      <td>0.122062</td>\n",
       "      <td>0.059888</td>\n",
       "      <td>0.031047</td>\n",
       "      <td>0.023140</td>\n",
       "      <td>3.637459</td>\n",
       "      <td>1.015971</td>\n",
       "      <td>0.006125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-22</th>\n",
       "      <td>2304.92</td>\n",
       "      <td>2431.94</td>\n",
       "      <td>2453.01</td>\n",
       "      <td>2295.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0434</td>\n",
       "      <td>2304.92</td>\n",
       "      <td>-0.162279</td>\n",
       "      <td>-0.254314</td>\n",
       "      <td>-0.248189</td>\n",
       "      <td>-0.382877</td>\n",
       "      <td>-0.285625</td>\n",
       "      <td>-0.227158</td>\n",
       "      <td>0.162279</td>\n",
       "      <td>0.071732</td>\n",
       "      <td>0.050623</td>\n",
       "      <td>0.034838</td>\n",
       "      <td>6.858893</td>\n",
       "      <td>0.970785</td>\n",
       "      <td>0.097697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-29</th>\n",
       "      <td>2541.47</td>\n",
       "      <td>2555.87</td>\n",
       "      <td>2615.91</td>\n",
       "      <td>2520.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0337</td>\n",
       "      <td>2541.47</td>\n",
       "      <td>0.097697</td>\n",
       "      <td>-0.064582</td>\n",
       "      <td>-0.156617</td>\n",
       "      <td>-0.272554</td>\n",
       "      <td>-0.196425</td>\n",
       "      <td>-0.134520</td>\n",
       "      <td>0.097697</td>\n",
       "      <td>0.105391</td>\n",
       "      <td>0.056342</td>\n",
       "      <td>0.037704</td>\n",
       "      <td>3.805129</td>\n",
       "      <td>0.989682</td>\n",
       "      <td>0.032376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>319 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Close     Open     High      Low  Volume  Change    price  \\\n",
       "Date                                                                      \n",
       "2009-12-20  1102.47  1097.86  1103.74  1093.88     0.0  0.0058  1102.47   \n",
       "2010-01-03  1115.10  1126.60  1127.64  1114.81     0.0 -0.0100  1115.10   \n",
       "2010-02-07  1066.19  1064.12  1067.13  1044.50     0.0  0.0029  1066.19   \n",
       "2010-02-14  1075.51  1075.95  1077.81  1062.97     0.0 -0.0027  1075.51   \n",
       "2010-02-28  1104.49  1103.10  1107.24  1097.56     0.0  0.0014  1104.49   \n",
       "...             ...      ...      ...      ...     ...     ...      ...   \n",
       "2020-02-02  3225.52  3282.33  3282.33  3214.68     0.0 -0.0177  3225.52   \n",
       "2020-02-09  3327.71  3335.54  3341.42  3322.12     0.0 -0.0054  3327.71   \n",
       "2020-03-01  2954.22  2916.90  2959.72  2855.84     0.0 -0.0082  2954.22   \n",
       "2020-03-22  2304.92  2431.94  2453.01  2295.56     0.0 -0.0434  2304.92   \n",
       "2020-03-29  2541.47  2555.87  2615.91  2520.02     0.0 -0.0337  2541.47   \n",
       "\n",
       "             returns     ROC_2     ROC_3     ROC_5    ROC_20    ROC_50  \\\n",
       "Date                                                                     \n",
       "2009-12-20 -0.003567 -0.003179  0.010009  0.008188  0.110152  0.168190   \n",
       "2010-01-03 -0.010154  0.011391  0.007824  0.021400  0.104862  0.271322   \n",
       "2010-02-07 -0.007177 -0.023700 -0.063448 -0.044853 -0.001977  0.325391   \n",
       "2010-02-14  0.008703  0.001526 -0.014996 -0.062592  0.029372  0.380557   \n",
       "2010-02-28 -0.004228  0.026589  0.035292  0.011593  0.030333  0.378370   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2020-02-02 -0.021455 -0.031764 -0.012273 -0.004485  0.070022  0.150227   \n",
       "2020-02-09  0.031190  0.009736 -0.000574  0.028302  0.106319  0.175286   \n",
       "2020-03-01 -0.122062 -0.134688 -0.119050 -0.109314 -0.005418  0.045619   \n",
       "2020-03-22 -0.162279 -0.254314 -0.248189 -0.382877 -0.285625 -0.227158   \n",
       "2020-03-29  0.097697 -0.064582 -0.156617 -0.272554 -0.196425 -0.134520   \n",
       "\n",
       "            abs_returns     atr_5    atr_20    atr_50     range  adj_close  \\\n",
       "Date                                                                         \n",
       "2009-12-20     0.003567  0.006654  0.020880  0.035614  0.901379   1.003331   \n",
       "2010-01-03     0.010154  0.012869  0.020948  0.034287  1.150869   0.994537   \n",
       "2010-02-07     0.007177  0.023785  0.022574  0.031645  2.166587   1.009827   \n",
       "2010-02-14     0.008703  0.017736  0.022008  0.030708  1.396088   1.004783   \n",
       "2010-02-28     0.004228  0.018309  0.020358  0.025371  0.881956   1.001896   \n",
       "...                 ...       ...       ...       ...       ...        ...   \n",
       "2020-02-02     0.021455  0.016071  0.010190  0.014912  2.104409   0.992924   \n",
       "2020-02-09     0.031190  0.021489  0.011697  0.015431  0.580954   0.998781   \n",
       "2020-03-01     0.122062  0.059888  0.031047  0.023140  3.637459   1.015971   \n",
       "2020-03-22     0.162279  0.071732  0.050623  0.034838  6.858893   0.970785   \n",
       "2020-03-29     0.097697  0.105391  0.056342  0.037704  3.805129   0.989682   \n",
       "\n",
       "            shift_returns  \n",
       "Date                       \n",
       "2009-12-20       0.021545  \n",
       "2010-01-03       0.026443  \n",
       "2010-02-07       0.008703  \n",
       "2010-02-14       0.030817  \n",
       "2010-02-28       0.030504  \n",
       "...                   ...  \n",
       "2020-02-02       0.031190  \n",
       "2020-02-09       0.015639  \n",
       "2020-03-01       0.006125  \n",
       "2020-03-22       0.097697  \n",
       "2020-03-29       0.032376  \n",
       "\n",
       "[319 rows x 20 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500[sp500.shift_returns > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x213ecb2a748>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "neg = len(sp500[sp500.shift_returns < 0])\n",
    "pos = len(sp500[sp500.shift_returns > 0])\n",
    "under_df = sp500[sp500.shift_returns < 0].sample(pos-neg) \n",
    "sp500 = pd.concat([under_df, sp500])\n",
    "sp500.reset_index().returns.cumsum().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Close', 'Open', 'High', 'Low', 'Volume', 'Change', 'price', 'returns',\n",
       "       'ROC_2', 'ROC_3', 'ROC_5', 'ROC_20', 'ROC_50', 'abs_returns', 'atr_5',\n",
       "       'atr_20', 'atr_50', 'range', 'adj_close', 'shift_returns'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp500.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500.drop(columns=['Close', 'Open', 'High', 'Low', 'Volume', 'price', 'range', 'adj_close', 'Change'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = sp500.drop(columns='shift_returns')\n",
    "y = sp500['shift_returns']\n",
    "\n",
    "X = X.select_dtypes('number')\n",
    "\n",
    "y = np.sign(y)\n",
    "#y = y.astype('category')\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train=X.iloc[:-60]\n",
    "y_train = y[:-60]\n",
    "X_test = X.iloc[-50:]\n",
    "y_test = y[-50:]\n",
    "\n",
    "\n",
    "min_max_scaler = preprocessing.StandardScaler() #MinMaxScaler()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(  X, y, test_size=0.10, random_state=42)\n",
    "X_train = min_max_scaler.fit_transform(X_train)\n",
    "X_test = min_max_scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578, 10)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    " #   model.add(Dropout(0.2, input_shape=(19,)))\n",
    "    \n",
    "    model.add(Dense(units=100, activation='tanh', kernel_initializer='random_uniform', bias_initializer='ones',  input_shape=(10,))) # kernel_constraint=maxnorm(3),\n",
    "    model.add(Dense(units=50, activation='tanh', kernel_initializer='random_uniform', bias_initializer='ones'))\n",
    "    model.add(Dense(units=11, activation='tanh', kernel_initializer='random_uniform', bias_initializer='ones'))\n",
    "    model.add(Dense(units=5, activation='tanh', kernel_initializer='random_uniform', bias_initializer='ones')) # kernel_constraint=maxnorm(3),\n",
    "    \n",
    "    model.add(Dense(units=3, activation='tanh', kernel_initializer='random_uniform', bias_initializer='ones'))\n",
    "   \n",
    " #   model.add(Dropout(0.2, input_shape=(19,)))\n",
    "    model.add(Dense(units=2, kernel_initializer='random_uniform', bias_initializer='ones',activation='tanh')) # kernel_constraint=maxnorm(3), \n",
    "    model.add(Dense(units=1, kernel_initializer='random_uniform', bias_initializer='ones', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_simple():\n",
    "    model = Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(Dense(units=11, kernel_initializer='uniform', activation='relu', bias_initializer='ones', input_shape=(10,)))\n",
    "    model.add(Dense(units=5, kernel_initializer='uniform', bias_initializer='ones', activation='relu'))\n",
    "    model.add(Dense(units=2, kernel_initializer='uniform', bias_initializer='ones', activation='relu'))\n",
    "    model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 100)               1100      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 11)                561       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 60        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 6,800\n",
      "Trainable params: 6,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21, 29], dtype=int64)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators= 1000, random_state=0, criterion='entropy', n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(3, 2, 1), max_iter=10000, alpha=0.00001, # 10, 2, 1 best 62%\n",
    "                     solver='sgd', verbose=10,  random_state=21,tol=0.000000001) # lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(3, 2, 1), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.89750356\n",
      "Iteration 2, loss = 0.89315392\n",
      "Iteration 3, loss = 0.88686237\n",
      "Iteration 4, loss = 0.87910801\n",
      "Iteration 5, loss = 0.87009978\n",
      "Iteration 6, loss = 0.86217919\n",
      "Iteration 7, loss = 0.85201393\n",
      "Iteration 8, loss = 0.84331113\n",
      "Iteration 9, loss = 0.83486115\n",
      "Iteration 10, loss = 0.82729894\n",
      "Iteration 11, loss = 0.81919940\n",
      "Iteration 12, loss = 0.81147488\n",
      "Iteration 13, loss = 0.80471888\n",
      "Iteration 14, loss = 0.79800286\n",
      "Iteration 15, loss = 0.79237362\n",
      "Iteration 16, loss = 0.78693252\n",
      "Iteration 17, loss = 0.78128950\n",
      "Iteration 18, loss = 0.77638207\n",
      "Iteration 19, loss = 0.77188269\n",
      "Iteration 20, loss = 0.76772481\n",
      "Iteration 21, loss = 0.76342883\n",
      "Iteration 22, loss = 0.75955079\n",
      "Iteration 23, loss = 0.75636805\n",
      "Iteration 24, loss = 0.75296259\n",
      "Iteration 25, loss = 0.74993225\n",
      "Iteration 26, loss = 0.74720308\n",
      "Iteration 27, loss = 0.74438773\n",
      "Iteration 28, loss = 0.74202737\n",
      "Iteration 29, loss = 0.73948250\n",
      "Iteration 30, loss = 0.73732517\n",
      "Iteration 31, loss = 0.73519453\n",
      "Iteration 32, loss = 0.73321144\n",
      "Iteration 33, loss = 0.73161459\n",
      "Iteration 34, loss = 0.72974775\n",
      "Iteration 35, loss = 0.72812598\n",
      "Iteration 36, loss = 0.72665735\n",
      "Iteration 37, loss = 0.72517234\n",
      "Iteration 38, loss = 0.72389308\n",
      "Iteration 39, loss = 0.72272349\n",
      "Iteration 40, loss = 0.72148022\n",
      "Iteration 41, loss = 0.72044770\n",
      "Iteration 42, loss = 0.71922430\n",
      "Iteration 43, loss = 0.71826359\n",
      "Iteration 44, loss = 0.71733755\n",
      "Iteration 45, loss = 0.71657310\n",
      "Iteration 46, loss = 0.71553494\n",
      "Iteration 47, loss = 0.71487829\n",
      "Iteration 48, loss = 0.71406675\n",
      "Iteration 49, loss = 0.71320500\n",
      "Iteration 50, loss = 0.71263085\n",
      "Iteration 51, loss = 0.71199037\n",
      "Iteration 52, loss = 0.71119804\n",
      "Iteration 53, loss = 0.71062555\n",
      "Iteration 54, loss = 0.71006322\n",
      "Iteration 55, loss = 0.70946409\n",
      "Iteration 56, loss = 0.70894750\n",
      "Iteration 57, loss = 0.70845474\n",
      "Iteration 58, loss = 0.70790790\n",
      "Iteration 59, loss = 0.70737461\n",
      "Iteration 60, loss = 0.70693831\n",
      "Iteration 61, loss = 0.70652060\n",
      "Iteration 62, loss = 0.70610012\n",
      "Iteration 63, loss = 0.70567486\n",
      "Iteration 64, loss = 0.70523651\n",
      "Iteration 65, loss = 0.70487054\n",
      "Iteration 66, loss = 0.70448820\n",
      "Iteration 67, loss = 0.70417616\n",
      "Iteration 68, loss = 0.70379472\n",
      "Iteration 69, loss = 0.70350905\n",
      "Iteration 70, loss = 0.70315516\n",
      "Iteration 71, loss = 0.70285802\n",
      "Iteration 72, loss = 0.70260034\n",
      "Iteration 73, loss = 0.70230872\n",
      "Iteration 74, loss = 0.70204514\n",
      "Iteration 75, loss = 0.70176547\n",
      "Iteration 76, loss = 0.70150120\n",
      "Iteration 77, loss = 0.70127316\n",
      "Iteration 78, loss = 0.70107101\n",
      "Iteration 79, loss = 0.70084793\n",
      "Iteration 80, loss = 0.70063454\n",
      "Iteration 81, loss = 0.70039537\n",
      "Iteration 82, loss = 0.70019592\n",
      "Iteration 83, loss = 0.70000873\n",
      "Iteration 84, loss = 0.69985435\n",
      "Iteration 85, loss = 0.69961891\n",
      "Iteration 86, loss = 0.69944668\n",
      "Iteration 87, loss = 0.69929567\n",
      "Iteration 88, loss = 0.69912586\n",
      "Iteration 89, loss = 0.69896801\n",
      "Iteration 90, loss = 0.69878950\n",
      "Iteration 91, loss = 0.69863058\n",
      "Iteration 92, loss = 0.69848288\n",
      "Iteration 93, loss = 0.69835962\n",
      "Iteration 94, loss = 0.69817734\n",
      "Iteration 95, loss = 0.69808218\n",
      "Iteration 96, loss = 0.69792982\n",
      "Iteration 97, loss = 0.69779145\n",
      "Iteration 98, loss = 0.69765232\n",
      "Iteration 99, loss = 0.69752759\n",
      "Iteration 100, loss = 0.69741432\n",
      "Iteration 101, loss = 0.69728425\n",
      "Iteration 102, loss = 0.69716318\n",
      "Iteration 103, loss = 0.69705078\n",
      "Iteration 104, loss = 0.69693733\n",
      "Iteration 105, loss = 0.69683944\n",
      "Iteration 106, loss = 0.69672797\n",
      "Iteration 107, loss = 0.69662955\n",
      "Iteration 108, loss = 0.69652514\n",
      "Iteration 109, loss = 0.69643087\n",
      "Iteration 110, loss = 0.69631795\n",
      "Iteration 111, loss = 0.69624955\n",
      "Iteration 112, loss = 0.69615997\n",
      "Iteration 113, loss = 0.69608352\n",
      "Iteration 114, loss = 0.69598615\n",
      "Iteration 115, loss = 0.69591000\n",
      "Iteration 116, loss = 0.69582324\n",
      "Iteration 117, loss = 0.69574454\n",
      "Iteration 118, loss = 0.69568807\n",
      "Iteration 119, loss = 0.69561639\n",
      "Iteration 120, loss = 0.69554583\n",
      "Iteration 121, loss = 0.69546882\n",
      "Iteration 122, loss = 0.69539183\n",
      "Iteration 123, loss = 0.69532927\n",
      "Iteration 124, loss = 0.69526576\n",
      "Iteration 125, loss = 0.69518982\n",
      "Iteration 126, loss = 0.69512938\n",
      "Iteration 127, loss = 0.69506258\n",
      "Iteration 128, loss = 0.69501315\n",
      "Iteration 129, loss = 0.69496221\n",
      "Iteration 130, loss = 0.69487800\n",
      "Iteration 131, loss = 0.69482450\n",
      "Iteration 132, loss = 0.69477972\n",
      "Iteration 133, loss = 0.69470398\n",
      "Iteration 134, loss = 0.69465473\n",
      "Iteration 135, loss = 0.69458984\n",
      "Iteration 136, loss = 0.69454277\n",
      "Iteration 137, loss = 0.69449241\n",
      "Iteration 138, loss = 0.69442247\n",
      "Iteration 139, loss = 0.69436067\n",
      "Iteration 140, loss = 0.69433018\n",
      "Iteration 141, loss = 0.69426675\n",
      "Iteration 142, loss = 0.69421215\n",
      "Iteration 143, loss = 0.69416238\n",
      "Iteration 144, loss = 0.69411373\n",
      "Iteration 145, loss = 0.69407131\n",
      "Iteration 146, loss = 0.69402768\n",
      "Iteration 147, loss = 0.69399336\n",
      "Iteration 148, loss = 0.69394823\n",
      "Iteration 149, loss = 0.69388886\n",
      "Iteration 150, loss = 0.69385917\n",
      "Iteration 151, loss = 0.69381001\n",
      "Iteration 152, loss = 0.69376804\n",
      "Iteration 153, loss = 0.69373541\n",
      "Iteration 154, loss = 0.69369695\n",
      "Iteration 155, loss = 0.69366001\n",
      "Iteration 156, loss = 0.69360584\n",
      "Iteration 157, loss = 0.69355837\n",
      "Iteration 158, loss = 0.69353365\n",
      "Iteration 159, loss = 0.69348654\n",
      "Iteration 160, loss = 0.69345374\n",
      "Iteration 161, loss = 0.69341321\n",
      "Iteration 162, loss = 0.69337880\n",
      "Iteration 163, loss = 0.69333909\n",
      "Iteration 164, loss = 0.69330694\n",
      "Iteration 165, loss = 0.69326763\n",
      "Iteration 166, loss = 0.69323624\n",
      "Iteration 167, loss = 0.69320017\n",
      "Iteration 168, loss = 0.69317087\n",
      "Iteration 169, loss = 0.69313628\n",
      "Iteration 170, loss = 0.69309833\n",
      "Iteration 171, loss = 0.69306560\n",
      "Iteration 172, loss = 0.69302793\n",
      "Iteration 173, loss = 0.69299886\n",
      "Iteration 174, loss = 0.69296928\n",
      "Iteration 175, loss = 0.69293723\n",
      "Iteration 176, loss = 0.69291625\n",
      "Iteration 177, loss = 0.69287574\n",
      "Iteration 178, loss = 0.69284895\n",
      "Iteration 179, loss = 0.69282165\n",
      "Iteration 180, loss = 0.69278597\n",
      "Iteration 181, loss = 0.69276315\n",
      "Iteration 182, loss = 0.69273964\n",
      "Iteration 183, loss = 0.69271299\n",
      "Iteration 184, loss = 0.69268196\n",
      "Iteration 185, loss = 0.69265681\n",
      "Iteration 186, loss = 0.69264105\n",
      "Iteration 187, loss = 0.69260496\n",
      "Iteration 188, loss = 0.69257933\n",
      "Iteration 189, loss = 0.69254713\n",
      "Iteration 190, loss = 0.69252121\n",
      "Iteration 191, loss = 0.69249300\n",
      "Iteration 192, loss = 0.69246454\n",
      "Iteration 193, loss = 0.69244662\n",
      "Iteration 194, loss = 0.69241069\n",
      "Iteration 195, loss = 0.69239360\n",
      "Iteration 196, loss = 0.69236208\n",
      "Iteration 197, loss = 0.69233924\n",
      "Iteration 198, loss = 0.69231837\n",
      "Iteration 199, loss = 0.69229162\n",
      "Iteration 200, loss = 0.69227365\n",
      "Iteration 201, loss = 0.69224463\n",
      "Iteration 202, loss = 0.69221967\n",
      "Iteration 203, loss = 0.69219660\n",
      "Iteration 204, loss = 0.69216934\n",
      "Iteration 205, loss = 0.69216478\n",
      "Iteration 206, loss = 0.69213243\n",
      "Iteration 207, loss = 0.69211032\n",
      "Iteration 208, loss = 0.69208338\n",
      "Iteration 209, loss = 0.69206069\n",
      "Iteration 210, loss = 0.69204253\n",
      "Iteration 211, loss = 0.69201830\n",
      "Iteration 212, loss = 0.69200060\n",
      "Iteration 213, loss = 0.69197628\n",
      "Iteration 214, loss = 0.69196056\n",
      "Iteration 215, loss = 0.69193616\n",
      "Iteration 216, loss = 0.69191742\n",
      "Iteration 217, loss = 0.69189847\n",
      "Iteration 218, loss = 0.69188451\n",
      "Iteration 219, loss = 0.69186395\n",
      "Iteration 220, loss = 0.69184879\n",
      "Iteration 221, loss = 0.69182169\n",
      "Iteration 222, loss = 0.69180522\n",
      "Iteration 223, loss = 0.69179263\n",
      "Iteration 224, loss = 0.69176824\n",
      "Iteration 225, loss = 0.69175552\n",
      "Iteration 226, loss = 0.69173887\n",
      "Iteration 227, loss = 0.69171913\n",
      "Iteration 228, loss = 0.69170645\n",
      "Iteration 229, loss = 0.69168926\n",
      "Iteration 230, loss = 0.69166925\n",
      "Iteration 231, loss = 0.69165458\n",
      "Iteration 232, loss = 0.69163772\n",
      "Iteration 233, loss = 0.69162968\n",
      "Iteration 234, loss = 0.69161404\n",
      "Iteration 235, loss = 0.69158934\n",
      "Iteration 236, loss = 0.69157966\n",
      "Iteration 237, loss = 0.69155689\n",
      "Iteration 238, loss = 0.69154096\n",
      "Iteration 239, loss = 0.69152572\n",
      "Iteration 240, loss = 0.69151248\n",
      "Iteration 241, loss = 0.69150219\n",
      "Iteration 242, loss = 0.69147956\n",
      "Iteration 243, loss = 0.69146273\n",
      "Iteration 244, loss = 0.69144944\n",
      "Iteration 245, loss = 0.69142706\n",
      "Iteration 246, loss = 0.69141475\n",
      "Iteration 247, loss = 0.69139749\n",
      "Iteration 248, loss = 0.69138147\n",
      "Iteration 249, loss = 0.69136282\n",
      "Iteration 250, loss = 0.69134885\n",
      "Iteration 251, loss = 0.69133476\n",
      "Iteration 252, loss = 0.69132655\n",
      "Iteration 253, loss = 0.69130452\n",
      "Iteration 254, loss = 0.69128123\n",
      "Iteration 255, loss = 0.69127003\n",
      "Iteration 256, loss = 0.69124994\n",
      "Iteration 257, loss = 0.69123801\n",
      "Iteration 258, loss = 0.69122348\n",
      "Iteration 259, loss = 0.69120834\n",
      "Iteration 260, loss = 0.69120658\n",
      "Iteration 261, loss = 0.69118124\n",
      "Iteration 262, loss = 0.69116675\n",
      "Iteration 263, loss = 0.69115368\n",
      "Iteration 264, loss = 0.69114024\n",
      "Iteration 265, loss = 0.69112626\n",
      "Iteration 266, loss = 0.69111567\n",
      "Iteration 267, loss = 0.69110790\n",
      "Iteration 268, loss = 0.69109181\n",
      "Iteration 269, loss = 0.69107460\n",
      "Iteration 270, loss = 0.69106221\n",
      "Iteration 271, loss = 0.69105207\n",
      "Iteration 272, loss = 0.69104034\n",
      "Iteration 273, loss = 0.69102560\n",
      "Iteration 274, loss = 0.69101952\n",
      "Iteration 275, loss = 0.69100752\n",
      "Iteration 276, loss = 0.69099072\n",
      "Iteration 277, loss = 0.69098191\n",
      "Iteration 278, loss = 0.69096545\n",
      "Iteration 279, loss = 0.69095720\n",
      "Iteration 280, loss = 0.69094805\n",
      "Iteration 281, loss = 0.69092878\n",
      "Iteration 282, loss = 0.69091940\n",
      "Iteration 283, loss = 0.69090678\n",
      "Iteration 284, loss = 0.69089886\n",
      "Iteration 285, loss = 0.69088266\n",
      "Iteration 286, loss = 0.69087526\n",
      "Iteration 287, loss = 0.69086247\n",
      "Iteration 288, loss = 0.69084887\n",
      "Iteration 289, loss = 0.69084271\n",
      "Iteration 290, loss = 0.69082800\n",
      "Iteration 291, loss = 0.69081602\n",
      "Iteration 292, loss = 0.69081419\n",
      "Iteration 293, loss = 0.69079472\n",
      "Iteration 294, loss = 0.69079023\n",
      "Iteration 295, loss = 0.69077933\n",
      "Iteration 296, loss = 0.69076411\n",
      "Iteration 297, loss = 0.69075414\n",
      "Iteration 298, loss = 0.69074513\n",
      "Iteration 299, loss = 0.69073548\n",
      "Iteration 300, loss = 0.69072279\n",
      "Iteration 301, loss = 0.69071313\n",
      "Iteration 302, loss = 0.69070275\n",
      "Iteration 303, loss = 0.69069477\n",
      "Iteration 304, loss = 0.69068507\n",
      "Iteration 305, loss = 0.69067744\n",
      "Iteration 306, loss = 0.69066539\n",
      "Iteration 307, loss = 0.69066000\n",
      "Iteration 308, loss = 0.69064982\n",
      "Iteration 309, loss = 0.69063998\n",
      "Iteration 310, loss = 0.69062853\n",
      "Iteration 311, loss = 0.69062629\n",
      "Iteration 312, loss = 0.69060959\n",
      "Iteration 313, loss = 0.69060228\n",
      "Iteration 314, loss = 0.69059308\n",
      "Iteration 315, loss = 0.69059113\n",
      "Iteration 316, loss = 0.69057785\n",
      "Iteration 317, loss = 0.69056543\n",
      "Iteration 318, loss = 0.69056200\n",
      "Iteration 319, loss = 0.69055408\n",
      "Iteration 320, loss = 0.69054214\n",
      "Iteration 321, loss = 0.69054095\n",
      "Iteration 322, loss = 0.69052725\n",
      "Iteration 323, loss = 0.69051982\n",
      "Iteration 324, loss = 0.69051163\n",
      "Iteration 325, loss = 0.69050438\n",
      "Iteration 326, loss = 0.69049809\n",
      "Iteration 327, loss = 0.69049320\n",
      "Iteration 328, loss = 0.69048191\n",
      "Iteration 329, loss = 0.69047442\n",
      "Iteration 330, loss = 0.69046974\n",
      "Iteration 331, loss = 0.69045877\n",
      "Iteration 332, loss = 0.69045196\n",
      "Iteration 333, loss = 0.69044971\n",
      "Iteration 334, loss = 0.69044054\n",
      "Iteration 335, loss = 0.69043013\n",
      "Iteration 336, loss = 0.69042469\n",
      "Iteration 337, loss = 0.69041998\n",
      "Iteration 338, loss = 0.69040980\n",
      "Iteration 339, loss = 0.69040391\n",
      "Iteration 340, loss = 0.69039250\n",
      "Iteration 341, loss = 0.69038569\n",
      "Iteration 342, loss = 0.69037899\n",
      "Iteration 343, loss = 0.69037494\n",
      "Iteration 344, loss = 0.69036507\n",
      "Iteration 345, loss = 0.69035703\n",
      "Iteration 346, loss = 0.69035007\n",
      "Iteration 347, loss = 0.69034435\n",
      "Iteration 348, loss = 0.69034056\n",
      "Iteration 349, loss = 0.69033219\n",
      "Iteration 350, loss = 0.69033017\n",
      "Iteration 351, loss = 0.69032011\n",
      "Iteration 352, loss = 0.69031073\n",
      "Iteration 353, loss = 0.69030336\n",
      "Iteration 354, loss = 0.69030090\n",
      "Iteration 355, loss = 0.69029466\n",
      "Iteration 356, loss = 0.69029025\n",
      "Iteration 357, loss = 0.69027827\n",
      "Iteration 358, loss = 0.69027509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 359, loss = 0.69026506\n",
      "Iteration 360, loss = 0.69026049\n",
      "Iteration 361, loss = 0.69025167\n",
      "Iteration 362, loss = 0.69024316\n",
      "Iteration 363, loss = 0.69024285\n",
      "Iteration 364, loss = 0.69023259\n",
      "Iteration 365, loss = 0.69022305\n",
      "Iteration 366, loss = 0.69021912\n",
      "Iteration 367, loss = 0.69021412\n",
      "Iteration 368, loss = 0.69020228\n",
      "Iteration 369, loss = 0.69019756\n",
      "Iteration 370, loss = 0.69018745\n",
      "Iteration 371, loss = 0.69018403\n",
      "Iteration 372, loss = 0.69017250\n",
      "Iteration 373, loss = 0.69016634\n",
      "Iteration 374, loss = 0.69016095\n",
      "Iteration 375, loss = 0.69015131\n",
      "Iteration 376, loss = 0.69014635\n",
      "Iteration 377, loss = 0.69013698\n",
      "Iteration 378, loss = 0.69012942\n",
      "Iteration 379, loss = 0.69012429\n",
      "Iteration 380, loss = 0.69011610\n",
      "Iteration 381, loss = 0.69011090\n",
      "Iteration 382, loss = 0.69010277\n",
      "Iteration 383, loss = 0.69009580\n",
      "Iteration 384, loss = 0.69008954\n",
      "Iteration 385, loss = 0.69008402\n",
      "Iteration 386, loss = 0.69007544\n",
      "Iteration 387, loss = 0.69006835\n",
      "Iteration 388, loss = 0.69006270\n",
      "Iteration 389, loss = 0.69005514\n",
      "Iteration 390, loss = 0.69004994\n",
      "Iteration 391, loss = 0.69004469\n",
      "Iteration 392, loss = 0.69003582\n",
      "Iteration 393, loss = 0.69003062\n",
      "Iteration 394, loss = 0.69002940\n",
      "Iteration 395, loss = 0.69002066\n",
      "Iteration 396, loss = 0.69001424\n",
      "Iteration 397, loss = 0.69001253\n",
      "Iteration 398, loss = 0.69000763\n",
      "Iteration 399, loss = 0.69000011\n",
      "Iteration 400, loss = 0.69000234\n",
      "Iteration 401, loss = 0.68998836\n",
      "Iteration 402, loss = 0.68998501\n",
      "Iteration 403, loss = 0.68998226\n",
      "Iteration 404, loss = 0.68997464\n",
      "Iteration 405, loss = 0.68996949\n",
      "Iteration 406, loss = 0.68996776\n",
      "Iteration 407, loss = 0.68995962\n",
      "Iteration 408, loss = 0.68995730\n",
      "Iteration 409, loss = 0.68995061\n",
      "Iteration 410, loss = 0.68994556\n",
      "Iteration 411, loss = 0.68993952\n",
      "Iteration 412, loss = 0.68994080\n",
      "Iteration 413, loss = 0.68994026\n",
      "Iteration 414, loss = 0.68992914\n",
      "Iteration 415, loss = 0.68992222\n",
      "Iteration 416, loss = 0.68992087\n",
      "Iteration 417, loss = 0.68990953\n",
      "Iteration 418, loss = 0.68990994\n",
      "Iteration 419, loss = 0.68990205\n",
      "Iteration 420, loss = 0.68989623\n",
      "Iteration 421, loss = 0.68989887\n",
      "Iteration 422, loss = 0.68988669\n",
      "Iteration 423, loss = 0.68988145\n",
      "Iteration 424, loss = 0.68987884\n",
      "Iteration 425, loss = 0.68987283\n",
      "Iteration 426, loss = 0.68986799\n",
      "Iteration 427, loss = 0.68986415\n",
      "Iteration 428, loss = 0.68986173\n",
      "Iteration 429, loss = 0.68986214\n",
      "Iteration 430, loss = 0.68985096\n",
      "Iteration 431, loss = 0.68984728\n",
      "Iteration 432, loss = 0.68984454\n",
      "Iteration 433, loss = 0.68983671\n",
      "Iteration 434, loss = 0.68983713\n",
      "Iteration 435, loss = 0.68983287\n",
      "Iteration 436, loss = 0.68982272\n",
      "Iteration 437, loss = 0.68981903\n",
      "Iteration 438, loss = 0.68981387\n",
      "Iteration 439, loss = 0.68981148\n",
      "Iteration 440, loss = 0.68980811\n",
      "Iteration 441, loss = 0.68980248\n",
      "Iteration 442, loss = 0.68979742\n",
      "Iteration 443, loss = 0.68979247\n",
      "Iteration 444, loss = 0.68979087\n",
      "Iteration 445, loss = 0.68978639\n",
      "Iteration 446, loss = 0.68977891\n",
      "Iteration 447, loss = 0.68977445\n",
      "Iteration 448, loss = 0.68977320\n",
      "Iteration 449, loss = 0.68977150\n",
      "Iteration 450, loss = 0.68976527\n",
      "Iteration 451, loss = 0.68975655\n",
      "Iteration 452, loss = 0.68975345\n",
      "Iteration 453, loss = 0.68974897\n",
      "Iteration 454, loss = 0.68974445\n",
      "Iteration 455, loss = 0.68973819\n",
      "Iteration 456, loss = 0.68973466\n",
      "Iteration 457, loss = 0.68973106\n",
      "Iteration 458, loss = 0.68972618\n",
      "Iteration 459, loss = 0.68972234\n",
      "Iteration 460, loss = 0.68971590\n",
      "Iteration 461, loss = 0.68971261\n",
      "Iteration 462, loss = 0.68970681\n",
      "Iteration 463, loss = 0.68970237\n",
      "Iteration 464, loss = 0.68969696\n",
      "Iteration 465, loss = 0.68969347\n",
      "Iteration 466, loss = 0.68969322\n",
      "Iteration 467, loss = 0.68968751\n",
      "Iteration 468, loss = 0.68968569\n",
      "Iteration 469, loss = 0.68967769\n",
      "Iteration 470, loss = 0.68967213\n",
      "Iteration 471, loss = 0.68967240\n",
      "Iteration 472, loss = 0.68966229\n",
      "Iteration 473, loss = 0.68965707\n",
      "Iteration 474, loss = 0.68965301\n",
      "Iteration 475, loss = 0.68965329\n",
      "Iteration 476, loss = 0.68964326\n",
      "Iteration 477, loss = 0.68964088\n",
      "Iteration 478, loss = 0.68963673\n",
      "Iteration 479, loss = 0.68963417\n",
      "Iteration 480, loss = 0.68963060\n",
      "Iteration 481, loss = 0.68962552\n",
      "Iteration 482, loss = 0.68961943\n",
      "Iteration 483, loss = 0.68961618\n",
      "Iteration 484, loss = 0.68961316\n",
      "Iteration 485, loss = 0.68960752\n",
      "Iteration 486, loss = 0.68960318\n",
      "Iteration 487, loss = 0.68959997\n",
      "Iteration 488, loss = 0.68959692\n",
      "Iteration 489, loss = 0.68959125\n",
      "Iteration 490, loss = 0.68959104\n",
      "Iteration 491, loss = 0.68958749\n",
      "Iteration 492, loss = 0.68959032\n",
      "Iteration 493, loss = 0.68957742\n",
      "Iteration 494, loss = 0.68957400\n",
      "Iteration 495, loss = 0.68957165\n",
      "Iteration 496, loss = 0.68956431\n",
      "Iteration 497, loss = 0.68956428\n",
      "Iteration 498, loss = 0.68955968\n",
      "Iteration 499, loss = 0.68955623\n",
      "Iteration 500, loss = 0.68955160\n",
      "Iteration 501, loss = 0.68954571\n",
      "Iteration 502, loss = 0.68954638\n",
      "Iteration 503, loss = 0.68954299\n",
      "Iteration 504, loss = 0.68953722\n",
      "Iteration 505, loss = 0.68953578\n",
      "Iteration 506, loss = 0.68954052\n",
      "Iteration 507, loss = 0.68952331\n",
      "Iteration 508, loss = 0.68952072\n",
      "Iteration 509, loss = 0.68951945\n",
      "Iteration 510, loss = 0.68951570\n",
      "Iteration 511, loss = 0.68950976\n",
      "Iteration 512, loss = 0.68950797\n",
      "Iteration 513, loss = 0.68950230\n",
      "Iteration 514, loss = 0.68950176\n",
      "Iteration 515, loss = 0.68949735\n",
      "Iteration 516, loss = 0.68949480\n",
      "Iteration 517, loss = 0.68949069\n",
      "Iteration 518, loss = 0.68949233\n",
      "Iteration 519, loss = 0.68948509\n",
      "Iteration 520, loss = 0.68948102\n",
      "Iteration 521, loss = 0.68947922\n",
      "Iteration 522, loss = 0.68947285\n",
      "Iteration 523, loss = 0.68947460\n",
      "Iteration 524, loss = 0.68946869\n",
      "Iteration 525, loss = 0.68946736\n",
      "Iteration 526, loss = 0.68946626\n",
      "Iteration 527, loss = 0.68945703\n",
      "Iteration 528, loss = 0.68945331\n",
      "Iteration 529, loss = 0.68945146\n",
      "Iteration 530, loss = 0.68945108\n",
      "Iteration 531, loss = 0.68944471\n",
      "Iteration 532, loss = 0.68944128\n",
      "Iteration 533, loss = 0.68943765\n",
      "Iteration 534, loss = 0.68943706\n",
      "Iteration 535, loss = 0.68943134\n",
      "Iteration 536, loss = 0.68942785\n",
      "Iteration 537, loss = 0.68942389\n",
      "Iteration 538, loss = 0.68942374\n",
      "Iteration 539, loss = 0.68942191\n",
      "Iteration 540, loss = 0.68941492\n",
      "Iteration 541, loss = 0.68941176\n",
      "Iteration 542, loss = 0.68941241\n",
      "Iteration 543, loss = 0.68940493\n",
      "Iteration 544, loss = 0.68940058\n",
      "Iteration 545, loss = 0.68940025\n",
      "Iteration 546, loss = 0.68939562\n",
      "Iteration 547, loss = 0.68939298\n",
      "Iteration 548, loss = 0.68939827\n",
      "Iteration 549, loss = 0.68938297\n",
      "Iteration 550, loss = 0.68937986\n",
      "Iteration 551, loss = 0.68937598\n",
      "Iteration 552, loss = 0.68937285\n",
      "Iteration 553, loss = 0.68936874\n",
      "Iteration 554, loss = 0.68936634\n",
      "Iteration 555, loss = 0.68936165\n",
      "Iteration 556, loss = 0.68936436\n",
      "Iteration 557, loss = 0.68935604\n",
      "Iteration 558, loss = 0.68934968\n",
      "Iteration 559, loss = 0.68935220\n",
      "Iteration 560, loss = 0.68934338\n",
      "Iteration 561, loss = 0.68933869\n",
      "Iteration 562, loss = 0.68933672\n",
      "Iteration 563, loss = 0.68933078\n",
      "Iteration 564, loss = 0.68933011\n",
      "Iteration 565, loss = 0.68932402\n",
      "Iteration 566, loss = 0.68932254\n",
      "Iteration 567, loss = 0.68931800\n",
      "Iteration 568, loss = 0.68931495\n",
      "Iteration 569, loss = 0.68931024\n",
      "Iteration 570, loss = 0.68930582\n",
      "Iteration 571, loss = 0.68930241\n",
      "Iteration 572, loss = 0.68929741\n",
      "Iteration 573, loss = 0.68929983\n",
      "Iteration 574, loss = 0.68929033\n",
      "Iteration 575, loss = 0.68928850\n",
      "Iteration 576, loss = 0.68928421\n",
      "Iteration 577, loss = 0.68927881\n",
      "Iteration 578, loss = 0.68928668\n",
      "Iteration 579, loss = 0.68927387\n",
      "Iteration 580, loss = 0.68927039\n",
      "Iteration 581, loss = 0.68926624\n",
      "Iteration 582, loss = 0.68926121\n",
      "Iteration 583, loss = 0.68926135\n",
      "Iteration 584, loss = 0.68925420\n",
      "Iteration 585, loss = 0.68925117\n",
      "Iteration 586, loss = 0.68925511\n",
      "Iteration 587, loss = 0.68924790\n",
      "Iteration 588, loss = 0.68924894\n",
      "Iteration 589, loss = 0.68924159\n",
      "Iteration 590, loss = 0.68923734\n",
      "Iteration 591, loss = 0.68923052\n",
      "Iteration 592, loss = 0.68922826\n",
      "Iteration 593, loss = 0.68923039\n",
      "Iteration 594, loss = 0.68921990\n",
      "Iteration 595, loss = 0.68921724\n",
      "Iteration 596, loss = 0.68921731\n",
      "Iteration 597, loss = 0.68920966\n",
      "Iteration 598, loss = 0.68920691\n",
      "Iteration 599, loss = 0.68920352\n",
      "Iteration 600, loss = 0.68920329\n",
      "Iteration 601, loss = 0.68919686\n",
      "Iteration 602, loss = 0.68919972\n",
      "Iteration 603, loss = 0.68919030\n",
      "Iteration 604, loss = 0.68919017\n",
      "Iteration 605, loss = 0.68918417\n",
      "Iteration 606, loss = 0.68918017\n",
      "Iteration 607, loss = 0.68917610\n",
      "Iteration 608, loss = 0.68917617\n",
      "Iteration 609, loss = 0.68917215\n",
      "Iteration 610, loss = 0.68916678\n",
      "Iteration 611, loss = 0.68916725\n",
      "Iteration 612, loss = 0.68916233\n",
      "Iteration 613, loss = 0.68915637\n",
      "Iteration 614, loss = 0.68915719\n",
      "Iteration 615, loss = 0.68915253\n",
      "Iteration 616, loss = 0.68914789\n",
      "Iteration 617, loss = 0.68914767\n",
      "Iteration 618, loss = 0.68914275\n",
      "Iteration 619, loss = 0.68914101\n",
      "Iteration 620, loss = 0.68913378\n",
      "Iteration 621, loss = 0.68913175\n",
      "Iteration 622, loss = 0.68913373\n",
      "Iteration 623, loss = 0.68912594\n",
      "Iteration 624, loss = 0.68912162\n",
      "Iteration 625, loss = 0.68911795\n",
      "Iteration 626, loss = 0.68911319\n",
      "Iteration 627, loss = 0.68911052\n",
      "Iteration 628, loss = 0.68910938\n",
      "Iteration 629, loss = 0.68911036\n",
      "Iteration 630, loss = 0.68910121\n",
      "Iteration 631, loss = 0.68910005\n",
      "Iteration 632, loss = 0.68909781\n",
      "Iteration 633, loss = 0.68909124\n",
      "Iteration 634, loss = 0.68909000\n",
      "Iteration 635, loss = 0.68908240\n",
      "Iteration 636, loss = 0.68908043\n",
      "Iteration 637, loss = 0.68907859\n",
      "Iteration 638, loss = 0.68907602\n",
      "Iteration 639, loss = 0.68907868\n",
      "Iteration 640, loss = 0.68906843\n",
      "Iteration 641, loss = 0.68906176\n",
      "Iteration 642, loss = 0.68905919\n",
      "Iteration 643, loss = 0.68905521\n",
      "Iteration 644, loss = 0.68905123\n",
      "Iteration 645, loss = 0.68905009\n",
      "Iteration 646, loss = 0.68904626\n",
      "Iteration 647, loss = 0.68904054\n",
      "Iteration 648, loss = 0.68903789\n",
      "Iteration 649, loss = 0.68903943\n",
      "Iteration 650, loss = 0.68903403\n",
      "Iteration 651, loss = 0.68903026\n",
      "Iteration 652, loss = 0.68902452\n",
      "Iteration 653, loss = 0.68902102\n",
      "Iteration 654, loss = 0.68901853\n",
      "Iteration 655, loss = 0.68901651\n",
      "Iteration 656, loss = 0.68901620\n",
      "Iteration 657, loss = 0.68901277\n",
      "Iteration 658, loss = 0.68901212\n",
      "Iteration 659, loss = 0.68900556\n",
      "Iteration 660, loss = 0.68900243\n",
      "Iteration 661, loss = 0.68900020\n",
      "Iteration 662, loss = 0.68899737\n",
      "Iteration 663, loss = 0.68900338\n",
      "Iteration 664, loss = 0.68899349\n",
      "Iteration 665, loss = 0.68899279\n",
      "Iteration 666, loss = 0.68898791\n",
      "Iteration 667, loss = 0.68898769\n",
      "Iteration 668, loss = 0.68898358\n",
      "Iteration 669, loss = 0.68898299\n",
      "Iteration 670, loss = 0.68898039\n",
      "Iteration 671, loss = 0.68897636\n",
      "Iteration 672, loss = 0.68897328\n",
      "Iteration 673, loss = 0.68897098\n",
      "Iteration 674, loss = 0.68897230\n",
      "Iteration 675, loss = 0.68896664\n",
      "Iteration 676, loss = 0.68896648\n",
      "Iteration 677, loss = 0.68896149\n",
      "Iteration 678, loss = 0.68896261\n",
      "Iteration 679, loss = 0.68896025\n",
      "Iteration 680, loss = 0.68895502\n",
      "Iteration 681, loss = 0.68895311\n",
      "Iteration 682, loss = 0.68894950\n",
      "Iteration 683, loss = 0.68894925\n",
      "Iteration 684, loss = 0.68894365\n",
      "Iteration 685, loss = 0.68894280\n",
      "Iteration 686, loss = 0.68893869\n",
      "Iteration 687, loss = 0.68894041\n",
      "Iteration 688, loss = 0.68893839\n",
      "Iteration 689, loss = 0.68893507\n",
      "Iteration 690, loss = 0.68893002\n",
      "Iteration 691, loss = 0.68892858\n",
      "Iteration 692, loss = 0.68892573\n",
      "Iteration 693, loss = 0.68892659\n",
      "Iteration 694, loss = 0.68892422\n",
      "Iteration 695, loss = 0.68891994\n",
      "Iteration 696, loss = 0.68891754\n",
      "Iteration 697, loss = 0.68891850\n",
      "Iteration 698, loss = 0.68891428\n",
      "Iteration 699, loss = 0.68891152\n",
      "Iteration 700, loss = 0.68890964\n",
      "Iteration 701, loss = 0.68891087\n",
      "Iteration 702, loss = 0.68890737\n",
      "Iteration 703, loss = 0.68890566\n",
      "Iteration 704, loss = 0.68890251\n",
      "Iteration 705, loss = 0.68889977\n",
      "Iteration 706, loss = 0.68890545\n",
      "Iteration 707, loss = 0.68889732\n",
      "Iteration 708, loss = 0.68889225\n",
      "Iteration 709, loss = 0.68889383\n",
      "Iteration 710, loss = 0.68889062\n",
      "Iteration 711, loss = 0.68888850\n",
      "Iteration 712, loss = 0.68888556\n",
      "Iteration 713, loss = 0.68888195\n",
      "Iteration 714, loss = 0.68888275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 715, loss = 0.68888005\n",
      "Iteration 716, loss = 0.68887578\n",
      "Iteration 717, loss = 0.68887421\n",
      "Iteration 718, loss = 0.68888227\n",
      "Iteration 719, loss = 0.68887225\n",
      "Iteration 720, loss = 0.68886808\n",
      "Iteration 721, loss = 0.68886638\n",
      "Iteration 722, loss = 0.68886515\n",
      "Iteration 723, loss = 0.68886596\n",
      "Iteration 724, loss = 0.68886376\n",
      "Iteration 725, loss = 0.68885887\n",
      "Iteration 726, loss = 0.68886470\n",
      "Iteration 727, loss = 0.68885609\n",
      "Iteration 728, loss = 0.68885255\n",
      "Iteration 729, loss = 0.68885158\n",
      "Iteration 730, loss = 0.68884867\n",
      "Iteration 731, loss = 0.68884611\n",
      "Iteration 732, loss = 0.68884617\n",
      "Iteration 733, loss = 0.68884336\n",
      "Iteration 734, loss = 0.68884201\n",
      "Iteration 735, loss = 0.68883916\n",
      "Iteration 736, loss = 0.68883623\n",
      "Iteration 737, loss = 0.68883445\n",
      "Iteration 738, loss = 0.68883197\n",
      "Iteration 739, loss = 0.68883246\n",
      "Iteration 740, loss = 0.68883364\n",
      "Iteration 741, loss = 0.68882922\n",
      "Iteration 742, loss = 0.68882696\n",
      "Iteration 743, loss = 0.68882744\n",
      "Iteration 744, loss = 0.68882816\n",
      "Iteration 745, loss = 0.68882090\n",
      "Iteration 746, loss = 0.68881742\n",
      "Iteration 747, loss = 0.68881706\n",
      "Iteration 748, loss = 0.68881425\n",
      "Iteration 749, loss = 0.68881646\n",
      "Iteration 750, loss = 0.68881080\n",
      "Iteration 751, loss = 0.68880935\n",
      "Iteration 752, loss = 0.68880724\n",
      "Iteration 753, loss = 0.68880631\n",
      "Iteration 754, loss = 0.68880703\n",
      "Iteration 755, loss = 0.68880556\n",
      "Iteration 756, loss = 0.68881019\n",
      "Iteration 757, loss = 0.68879921\n",
      "Iteration 758, loss = 0.68879860\n",
      "Iteration 759, loss = 0.68879608\n",
      "Iteration 760, loss = 0.68879460\n",
      "Iteration 761, loss = 0.68879295\n",
      "Iteration 762, loss = 0.68879194\n",
      "Iteration 763, loss = 0.68878970\n",
      "Iteration 764, loss = 0.68879098\n",
      "Iteration 765, loss = 0.68878717\n",
      "Iteration 766, loss = 0.68878392\n",
      "Iteration 767, loss = 0.68878208\n",
      "Iteration 768, loss = 0.68878328\n",
      "Iteration 769, loss = 0.68878550\n",
      "Iteration 770, loss = 0.68877804\n",
      "Iteration 771, loss = 0.68878564\n",
      "Iteration 772, loss = 0.68877695\n",
      "Iteration 773, loss = 0.68877193\n",
      "Iteration 774, loss = 0.68877501\n",
      "Iteration 775, loss = 0.68877070\n",
      "Iteration 776, loss = 0.68876830\n",
      "Iteration 777, loss = 0.68876603\n",
      "Iteration 778, loss = 0.68876703\n",
      "Iteration 779, loss = 0.68876356\n",
      "Iteration 780, loss = 0.68876295\n",
      "Iteration 781, loss = 0.68876263\n",
      "Iteration 782, loss = 0.68875965\n",
      "Iteration 783, loss = 0.68875852\n",
      "Iteration 784, loss = 0.68875725\n",
      "Iteration 785, loss = 0.68875535\n",
      "Iteration 786, loss = 0.68875237\n",
      "Iteration 787, loss = 0.68875286\n",
      "Iteration 788, loss = 0.68875328\n",
      "Iteration 789, loss = 0.68875033\n",
      "Iteration 790, loss = 0.68874883\n",
      "Iteration 791, loss = 0.68874460\n",
      "Iteration 792, loss = 0.68874522\n",
      "Iteration 793, loss = 0.68874528\n",
      "Iteration 794, loss = 0.68874073\n",
      "Iteration 795, loss = 0.68873837\n",
      "Iteration 796, loss = 0.68874343\n",
      "Iteration 797, loss = 0.68873577\n",
      "Iteration 798, loss = 0.68873315\n",
      "Iteration 799, loss = 0.68873951\n",
      "Iteration 800, loss = 0.68873376\n",
      "Iteration 801, loss = 0.68873184\n",
      "Iteration 802, loss = 0.68873152\n",
      "Iteration 803, loss = 0.68872629\n",
      "Iteration 804, loss = 0.68872479\n",
      "Iteration 805, loss = 0.68872461\n",
      "Iteration 806, loss = 0.68872209\n",
      "Iteration 807, loss = 0.68871910\n",
      "Iteration 808, loss = 0.68871767\n",
      "Iteration 809, loss = 0.68871720\n",
      "Iteration 810, loss = 0.68871803\n",
      "Iteration 811, loss = 0.68871397\n",
      "Iteration 812, loss = 0.68871304\n",
      "Iteration 813, loss = 0.68872291\n",
      "Iteration 814, loss = 0.68871154\n",
      "Iteration 815, loss = 0.68870753\n",
      "Iteration 816, loss = 0.68870973\n",
      "Iteration 817, loss = 0.68870770\n",
      "Iteration 818, loss = 0.68870404\n",
      "Iteration 819, loss = 0.68870323\n",
      "Iteration 820, loss = 0.68869969\n",
      "Iteration 821, loss = 0.68869833\n",
      "Iteration 822, loss = 0.68869844\n",
      "Iteration 823, loss = 0.68869822\n",
      "Iteration 824, loss = 0.68869637\n",
      "Iteration 825, loss = 0.68869410\n",
      "Iteration 826, loss = 0.68869300\n",
      "Iteration 827, loss = 0.68869028\n",
      "Iteration 828, loss = 0.68869118\n",
      "Iteration 829, loss = 0.68868843\n",
      "Iteration 830, loss = 0.68868608\n",
      "Iteration 831, loss = 0.68868584\n",
      "Iteration 832, loss = 0.68868559\n",
      "Iteration 833, loss = 0.68868281\n",
      "Iteration 834, loss = 0.68868168\n",
      "Iteration 835, loss = 0.68868186\n",
      "Iteration 836, loss = 0.68868203\n",
      "Iteration 837, loss = 0.68867790\n",
      "Iteration 838, loss = 0.68868118\n",
      "Iteration 839, loss = 0.68868196\n",
      "Iteration 840, loss = 0.68867577\n",
      "Iteration 841, loss = 0.68867478\n",
      "Iteration 842, loss = 0.68867276\n",
      "Iteration 843, loss = 0.68867079\n",
      "Iteration 844, loss = 0.68867790\n",
      "Iteration 845, loss = 0.68866918\n",
      "Iteration 846, loss = 0.68867008\n",
      "Iteration 847, loss = 0.68866658\n",
      "Iteration 848, loss = 0.68866769\n",
      "Iteration 849, loss = 0.68866558\n",
      "Iteration 850, loss = 0.68866330\n",
      "Iteration 851, loss = 0.68866197\n",
      "Iteration 852, loss = 0.68866320\n",
      "Iteration 853, loss = 0.68866045\n",
      "Iteration 854, loss = 0.68865955\n",
      "Iteration 855, loss = 0.68865654\n",
      "Iteration 856, loss = 0.68865801\n",
      "Iteration 857, loss = 0.68865394\n",
      "Iteration 858, loss = 0.68865522\n",
      "Iteration 859, loss = 0.68865553\n",
      "Iteration 860, loss = 0.68865468\n",
      "Iteration 861, loss = 0.68865233\n",
      "Iteration 862, loss = 0.68864754\n",
      "Iteration 863, loss = 0.68865453\n",
      "Iteration 864, loss = 0.68864686\n",
      "Iteration 865, loss = 0.68864576\n",
      "Iteration 866, loss = 0.68864418\n",
      "Iteration 867, loss = 0.68864851\n",
      "Iteration 868, loss = 0.68864200\n",
      "Iteration 869, loss = 0.68864076\n",
      "Iteration 870, loss = 0.68863923\n",
      "Iteration 871, loss = 0.68863833\n",
      "Iteration 872, loss = 0.68863776\n",
      "Iteration 873, loss = 0.68863638\n",
      "Iteration 874, loss = 0.68863499\n",
      "Iteration 875, loss = 0.68863439\n",
      "Iteration 876, loss = 0.68863242\n",
      "Iteration 877, loss = 0.68863662\n",
      "Iteration 878, loss = 0.68863179\n",
      "Iteration 879, loss = 0.68863087\n",
      "Iteration 880, loss = 0.68862803\n",
      "Iteration 881, loss = 0.68862758\n",
      "Iteration 882, loss = 0.68862499\n",
      "Iteration 883, loss = 0.68862323\n",
      "Iteration 884, loss = 0.68862377\n",
      "Iteration 885, loss = 0.68862310\n",
      "Iteration 886, loss = 0.68862599\n",
      "Iteration 887, loss = 0.68861893\n",
      "Iteration 888, loss = 0.68861698\n",
      "Iteration 889, loss = 0.68861700\n",
      "Iteration 890, loss = 0.68861991\n",
      "Iteration 891, loss = 0.68861749\n",
      "Iteration 892, loss = 0.68861240\n",
      "Iteration 893, loss = 0.68861530\n",
      "Iteration 894, loss = 0.68861491\n",
      "Iteration 895, loss = 0.68860889\n",
      "Iteration 896, loss = 0.68861024\n",
      "Iteration 897, loss = 0.68860684\n",
      "Iteration 898, loss = 0.68860949\n",
      "Iteration 899, loss = 0.68860845\n",
      "Iteration 900, loss = 0.68860393\n",
      "Iteration 901, loss = 0.68860776\n",
      "Iteration 902, loss = 0.68860389\n",
      "Iteration 903, loss = 0.68860167\n",
      "Iteration 904, loss = 0.68860277\n",
      "Iteration 905, loss = 0.68859923\n",
      "Iteration 906, loss = 0.68859633\n",
      "Iteration 907, loss = 0.68859461\n",
      "Iteration 908, loss = 0.68859511\n",
      "Iteration 909, loss = 0.68859546\n",
      "Iteration 910, loss = 0.68859306\n",
      "Iteration 911, loss = 0.68859344\n",
      "Iteration 912, loss = 0.68859373\n",
      "Iteration 913, loss = 0.68858863\n",
      "Iteration 914, loss = 0.68858893\n",
      "Iteration 915, loss = 0.68858948\n",
      "Iteration 916, loss = 0.68859954\n",
      "Iteration 917, loss = 0.68858844\n",
      "Iteration 918, loss = 0.68858446\n",
      "Iteration 919, loss = 0.68858593\n",
      "Iteration 920, loss = 0.68858427\n",
      "Iteration 921, loss = 0.68858466\n",
      "Iteration 922, loss = 0.68857938\n",
      "Iteration 923, loss = 0.68857612\n",
      "Iteration 924, loss = 0.68857619\n",
      "Iteration 925, loss = 0.68857807\n",
      "Iteration 926, loss = 0.68857640\n",
      "Iteration 927, loss = 0.68857383\n",
      "Iteration 928, loss = 0.68857543\n",
      "Iteration 929, loss = 0.68857049\n",
      "Iteration 930, loss = 0.68857140\n",
      "Iteration 931, loss = 0.68857070\n",
      "Iteration 932, loss = 0.68857324\n",
      "Iteration 933, loss = 0.68856983\n",
      "Iteration 934, loss = 0.68857246\n",
      "Iteration 935, loss = 0.68856412\n",
      "Iteration 936, loss = 0.68856120\n",
      "Iteration 937, loss = 0.68856079\n",
      "Iteration 938, loss = 0.68856145\n",
      "Iteration 939, loss = 0.68855968\n",
      "Iteration 940, loss = 0.68856126\n",
      "Iteration 941, loss = 0.68856701\n",
      "Iteration 942, loss = 0.68855795\n",
      "Iteration 943, loss = 0.68855363\n",
      "Iteration 944, loss = 0.68855937\n",
      "Iteration 945, loss = 0.68855414\n",
      "Iteration 946, loss = 0.68855037\n",
      "Iteration 947, loss = 0.68854904\n",
      "Iteration 948, loss = 0.68855024\n",
      "Iteration 949, loss = 0.68854660\n",
      "Iteration 950, loss = 0.68855163\n",
      "Iteration 951, loss = 0.68854776\n",
      "Iteration 952, loss = 0.68854377\n",
      "Iteration 953, loss = 0.68854494\n",
      "Iteration 954, loss = 0.68854298\n",
      "Iteration 955, loss = 0.68854048\n",
      "Iteration 956, loss = 0.68853931\n",
      "Iteration 957, loss = 0.68853821\n",
      "Iteration 958, loss = 0.68853903\n",
      "Iteration 959, loss = 0.68853912\n",
      "Iteration 960, loss = 0.68853428\n",
      "Iteration 961, loss = 0.68853281\n",
      "Iteration 962, loss = 0.68853397\n",
      "Iteration 963, loss = 0.68853239\n",
      "Iteration 964, loss = 0.68853275\n",
      "Iteration 965, loss = 0.68853056\n",
      "Iteration 966, loss = 0.68852706\n",
      "Iteration 967, loss = 0.68852867\n",
      "Iteration 968, loss = 0.68852484\n",
      "Iteration 969, loss = 0.68852562\n",
      "Iteration 970, loss = 0.68852285\n",
      "Iteration 971, loss = 0.68852173\n",
      "Iteration 972, loss = 0.68852182\n",
      "Iteration 973, loss = 0.68851858\n",
      "Iteration 974, loss = 0.68851917\n",
      "Iteration 975, loss = 0.68851676\n",
      "Iteration 976, loss = 0.68851636\n",
      "Iteration 977, loss = 0.68851708\n",
      "Iteration 978, loss = 0.68852221\n",
      "Iteration 979, loss = 0.68851700\n",
      "Iteration 980, loss = 0.68851115\n",
      "Iteration 981, loss = 0.68851381\n",
      "Iteration 982, loss = 0.68851095\n",
      "Iteration 983, loss = 0.68851205\n",
      "Iteration 984, loss = 0.68850710\n",
      "Iteration 985, loss = 0.68850931\n",
      "Iteration 986, loss = 0.68850429\n",
      "Iteration 987, loss = 0.68850368\n",
      "Iteration 988, loss = 0.68850479\n",
      "Iteration 989, loss = 0.68850202\n",
      "Iteration 990, loss = 0.68849943\n",
      "Iteration 991, loss = 0.68849838\n",
      "Iteration 992, loss = 0.68849905\n",
      "Iteration 993, loss = 0.68849708\n",
      "Iteration 994, loss = 0.68849920\n",
      "Iteration 995, loss = 0.68849373\n",
      "Iteration 996, loss = 0.68849334\n",
      "Iteration 997, loss = 0.68849617\n",
      "Iteration 998, loss = 0.68849090\n",
      "Iteration 999, loss = 0.68849153\n",
      "Iteration 1000, loss = 0.68849088\n",
      "Iteration 1001, loss = 0.68848685\n",
      "Iteration 1002, loss = 0.68849279\n",
      "Iteration 1003, loss = 0.68848749\n",
      "Iteration 1004, loss = 0.68848681\n",
      "Iteration 1005, loss = 0.68848259\n",
      "Iteration 1006, loss = 0.68849984\n",
      "Iteration 1007, loss = 0.68848185\n",
      "Iteration 1008, loss = 0.68848242\n",
      "Iteration 1009, loss = 0.68847957\n",
      "Iteration 1010, loss = 0.68847977\n",
      "Iteration 1011, loss = 0.68847694\n",
      "Iteration 1012, loss = 0.68847545\n",
      "Iteration 1013, loss = 0.68847703\n",
      "Iteration 1014, loss = 0.68847589\n",
      "Iteration 1015, loss = 0.68847245\n",
      "Iteration 1016, loss = 0.68847358\n",
      "Iteration 1017, loss = 0.68847288\n",
      "Iteration 1018, loss = 0.68847034\n",
      "Iteration 1019, loss = 0.68846626\n",
      "Iteration 1020, loss = 0.68846689\n",
      "Iteration 1021, loss = 0.68847155\n",
      "Iteration 1022, loss = 0.68846400\n",
      "Iteration 1023, loss = 0.68846377\n",
      "Iteration 1024, loss = 0.68846195\n",
      "Iteration 1025, loss = 0.68846008\n",
      "Iteration 1026, loss = 0.68845896\n",
      "Iteration 1027, loss = 0.68846000\n",
      "Iteration 1028, loss = 0.68845645\n",
      "Iteration 1029, loss = 0.68845578\n",
      "Iteration 1030, loss = 0.68845438\n",
      "Iteration 1031, loss = 0.68845287\n",
      "Iteration 1032, loss = 0.68845121\n",
      "Iteration 1033, loss = 0.68845127\n",
      "Iteration 1034, loss = 0.68845114\n",
      "Iteration 1035, loss = 0.68844869\n",
      "Iteration 1036, loss = 0.68845014\n",
      "Iteration 1037, loss = 0.68844992\n",
      "Iteration 1038, loss = 0.68844395\n",
      "Iteration 1039, loss = 0.68844330\n",
      "Iteration 1040, loss = 0.68845401\n",
      "Iteration 1041, loss = 0.68844724\n",
      "Iteration 1042, loss = 0.68844168\n",
      "Iteration 1043, loss = 0.68843890\n",
      "Iteration 1044, loss = 0.68843771\n",
      "Iteration 1045, loss = 0.68843861\n",
      "Iteration 1046, loss = 0.68843390\n",
      "Iteration 1047, loss = 0.68843548\n",
      "Iteration 1048, loss = 0.68843426\n",
      "Iteration 1049, loss = 0.68843166\n",
      "Iteration 1050, loss = 0.68843638\n",
      "Iteration 1051, loss = 0.68842826\n",
      "Iteration 1052, loss = 0.68842821\n",
      "Iteration 1053, loss = 0.68842945\n",
      "Iteration 1054, loss = 0.68842897\n",
      "Iteration 1055, loss = 0.68842404\n",
      "Iteration 1056, loss = 0.68842243\n",
      "Iteration 1057, loss = 0.68842191\n",
      "Iteration 1058, loss = 0.68842612\n",
      "Iteration 1059, loss = 0.68841883\n",
      "Iteration 1060, loss = 0.68842114\n",
      "Iteration 1061, loss = 0.68841668\n",
      "Iteration 1062, loss = 0.68841858\n",
      "Iteration 1063, loss = 0.68841312\n",
      "Iteration 1064, loss = 0.68842694\n",
      "Iteration 1065, loss = 0.68841175\n",
      "Iteration 1066, loss = 0.68841148\n",
      "Iteration 1067, loss = 0.68840967\n",
      "Iteration 1068, loss = 0.68840765\n",
      "Iteration 1069, loss = 0.68840922\n",
      "Iteration 1070, loss = 0.68840939\n",
      "Iteration 1071, loss = 0.68840394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1072, loss = 0.68840383\n",
      "Iteration 1073, loss = 0.68840471\n",
      "Iteration 1074, loss = 0.68840497\n",
      "Iteration 1075, loss = 0.68840290\n",
      "Iteration 1076, loss = 0.68840290\n",
      "Iteration 1077, loss = 0.68839630\n",
      "Iteration 1078, loss = 0.68839686\n",
      "Iteration 1079, loss = 0.68840050\n",
      "Iteration 1080, loss = 0.68839476\n",
      "Iteration 1081, loss = 0.68839268\n",
      "Iteration 1082, loss = 0.68839127\n",
      "Iteration 1083, loss = 0.68839118\n",
      "Iteration 1084, loss = 0.68839478\n",
      "Iteration 1085, loss = 0.68838697\n",
      "Iteration 1086, loss = 0.68838752\n",
      "Iteration 1087, loss = 0.68838423\n",
      "Iteration 1088, loss = 0.68838789\n",
      "Iteration 1089, loss = 0.68838286\n",
      "Iteration 1090, loss = 0.68837996\n",
      "Iteration 1091, loss = 0.68838176\n",
      "Iteration 1092, loss = 0.68837979\n",
      "Iteration 1093, loss = 0.68837745\n",
      "Iteration 1094, loss = 0.68837847\n",
      "Iteration 1095, loss = 0.68837941\n",
      "Iteration 1096, loss = 0.68837508\n",
      "Iteration 1097, loss = 0.68837391\n",
      "Iteration 1098, loss = 0.68837062\n",
      "Iteration 1099, loss = 0.68837376\n",
      "Iteration 1100, loss = 0.68837387\n",
      "Iteration 1101, loss = 0.68836800\n",
      "Iteration 1102, loss = 0.68836933\n",
      "Iteration 1103, loss = 0.68836421\n",
      "Iteration 1104, loss = 0.68836277\n",
      "Iteration 1105, loss = 0.68836084\n",
      "Iteration 1106, loss = 0.68836127\n",
      "Iteration 1107, loss = 0.68835919\n",
      "Iteration 1108, loss = 0.68835755\n",
      "Iteration 1109, loss = 0.68835751\n",
      "Iteration 1110, loss = 0.68835704\n",
      "Iteration 1111, loss = 0.68835507\n",
      "Iteration 1112, loss = 0.68835380\n",
      "Iteration 1113, loss = 0.68835545\n",
      "Iteration 1114, loss = 0.68835006\n",
      "Iteration 1115, loss = 0.68834857\n",
      "Iteration 1116, loss = 0.68834759\n",
      "Iteration 1117, loss = 0.68834666\n",
      "Iteration 1118, loss = 0.68834573\n",
      "Iteration 1119, loss = 0.68834400\n",
      "Iteration 1120, loss = 0.68834345\n",
      "Iteration 1121, loss = 0.68834166\n",
      "Iteration 1122, loss = 0.68833971\n",
      "Iteration 1123, loss = 0.68833951\n",
      "Iteration 1124, loss = 0.68833753\n",
      "Iteration 1125, loss = 0.68833645\n",
      "Iteration 1126, loss = 0.68833506\n",
      "Iteration 1127, loss = 0.68833171\n",
      "Iteration 1128, loss = 0.68833337\n",
      "Iteration 1129, loss = 0.68833121\n",
      "Iteration 1130, loss = 0.68832820\n",
      "Iteration 1131, loss = 0.68832800\n",
      "Iteration 1132, loss = 0.68832615\n",
      "Iteration 1133, loss = 0.68832548\n",
      "Iteration 1134, loss = 0.68832525\n",
      "Iteration 1135, loss = 0.68832280\n",
      "Iteration 1136, loss = 0.68832189\n",
      "Iteration 1137, loss = 0.68832127\n",
      "Iteration 1138, loss = 0.68832135\n",
      "Iteration 1139, loss = 0.68831873\n",
      "Iteration 1140, loss = 0.68831765\n",
      "Iteration 1141, loss = 0.68831652\n",
      "Iteration 1142, loss = 0.68831654\n",
      "Iteration 1143, loss = 0.68831265\n",
      "Iteration 1144, loss = 0.68831334\n",
      "Iteration 1145, loss = 0.68831091\n",
      "Iteration 1146, loss = 0.68831869\n",
      "Iteration 1147, loss = 0.68830659\n",
      "Iteration 1148, loss = 0.68830735\n",
      "Iteration 1149, loss = 0.68830517\n",
      "Iteration 1150, loss = 0.68830361\n",
      "Iteration 1151, loss = 0.68830367\n",
      "Iteration 1152, loss = 0.68830477\n",
      "Iteration 1153, loss = 0.68830139\n",
      "Iteration 1154, loss = 0.68829817\n",
      "Iteration 1155, loss = 0.68830411\n",
      "Iteration 1156, loss = 0.68829643\n",
      "Iteration 1157, loss = 0.68829369\n",
      "Iteration 1158, loss = 0.68829470\n",
      "Iteration 1159, loss = 0.68829236\n",
      "Iteration 1160, loss = 0.68829072\n",
      "Iteration 1161, loss = 0.68829015\n",
      "Iteration 1162, loss = 0.68828728\n",
      "Iteration 1163, loss = 0.68828795\n",
      "Iteration 1164, loss = 0.68828533\n",
      "Iteration 1165, loss = 0.68828475\n",
      "Iteration 1166, loss = 0.68828257\n",
      "Iteration 1167, loss = 0.68828317\n",
      "Iteration 1168, loss = 0.68828616\n",
      "Iteration 1169, loss = 0.68828008\n",
      "Iteration 1170, loss = 0.68827703\n",
      "Iteration 1171, loss = 0.68828006\n",
      "Iteration 1172, loss = 0.68827566\n",
      "Iteration 1173, loss = 0.68827800\n",
      "Iteration 1174, loss = 0.68827284\n",
      "Iteration 1175, loss = 0.68827217\n",
      "Iteration 1176, loss = 0.68826857\n",
      "Iteration 1177, loss = 0.68827037\n",
      "Iteration 1178, loss = 0.68826743\n",
      "Iteration 1179, loss = 0.68826668\n",
      "Iteration 1180, loss = 0.68826688\n",
      "Iteration 1181, loss = 0.68826340\n",
      "Iteration 1182, loss = 0.68826043\n",
      "Iteration 1183, loss = 0.68826156\n",
      "Iteration 1184, loss = 0.68825676\n",
      "Iteration 1185, loss = 0.68826101\n",
      "Iteration 1186, loss = 0.68825512\n",
      "Iteration 1187, loss = 0.68825727\n",
      "Iteration 1188, loss = 0.68825180\n",
      "Iteration 1189, loss = 0.68825424\n",
      "Iteration 1190, loss = 0.68825039\n",
      "Iteration 1191, loss = 0.68824842\n",
      "Iteration 1192, loss = 0.68824586\n",
      "Iteration 1193, loss = 0.68824662\n",
      "Iteration 1194, loss = 0.68824354\n",
      "Iteration 1195, loss = 0.68824682\n",
      "Iteration 1196, loss = 0.68824158\n",
      "Iteration 1197, loss = 0.68824457\n",
      "Iteration 1198, loss = 0.68823903\n",
      "Iteration 1199, loss = 0.68823705\n",
      "Iteration 1200, loss = 0.68823441\n",
      "Iteration 1201, loss = 0.68823334\n",
      "Iteration 1202, loss = 0.68823183\n",
      "Iteration 1203, loss = 0.68823242\n",
      "Iteration 1204, loss = 0.68823210\n",
      "Iteration 1205, loss = 0.68822744\n",
      "Iteration 1206, loss = 0.68822531\n",
      "Iteration 1207, loss = 0.68822538\n",
      "Iteration 1208, loss = 0.68822639\n",
      "Iteration 1209, loss = 0.68822152\n",
      "Iteration 1210, loss = 0.68821985\n",
      "Iteration 1211, loss = 0.68822109\n",
      "Iteration 1212, loss = 0.68821687\n",
      "Iteration 1213, loss = 0.68821664\n",
      "Iteration 1214, loss = 0.68821401\n",
      "Iteration 1215, loss = 0.68821169\n",
      "Iteration 1216, loss = 0.68821043\n",
      "Iteration 1217, loss = 0.68821330\n",
      "Iteration 1218, loss = 0.68820807\n",
      "Iteration 1219, loss = 0.68820916\n",
      "Iteration 1220, loss = 0.68820596\n",
      "Iteration 1221, loss = 0.68820460\n",
      "Iteration 1222, loss = 0.68820231\n",
      "Iteration 1223, loss = 0.68820375\n",
      "Iteration 1224, loss = 0.68820065\n",
      "Iteration 1225, loss = 0.68819779\n",
      "Iteration 1226, loss = 0.68819692\n",
      "Iteration 1227, loss = 0.68819624\n",
      "Iteration 1228, loss = 0.68819644\n",
      "Iteration 1229, loss = 0.68819069\n",
      "Iteration 1230, loss = 0.68819780\n",
      "Iteration 1231, loss = 0.68818818\n",
      "Iteration 1232, loss = 0.68818639\n",
      "Iteration 1233, loss = 0.68818742\n",
      "Iteration 1234, loss = 0.68818694\n",
      "Iteration 1235, loss = 0.68818674\n",
      "Iteration 1236, loss = 0.68818136\n",
      "Iteration 1237, loss = 0.68817923\n",
      "Iteration 1238, loss = 0.68817719\n",
      "Iteration 1239, loss = 0.68817676\n",
      "Iteration 1240, loss = 0.68817935\n",
      "Iteration 1241, loss = 0.68817323\n",
      "Iteration 1242, loss = 0.68817571\n",
      "Iteration 1243, loss = 0.68816984\n",
      "Iteration 1244, loss = 0.68817059\n",
      "Iteration 1245, loss = 0.68816604\n",
      "Iteration 1246, loss = 0.68816568\n",
      "Iteration 1247, loss = 0.68816665\n",
      "Iteration 1248, loss = 0.68816270\n",
      "Iteration 1249, loss = 0.68816144\n",
      "Iteration 1250, loss = 0.68815883\n",
      "Iteration 1251, loss = 0.68816232\n",
      "Iteration 1252, loss = 0.68815875\n",
      "Iteration 1253, loss = 0.68815532\n",
      "Iteration 1254, loss = 0.68815413\n",
      "Iteration 1255, loss = 0.68815415\n",
      "Iteration 1256, loss = 0.68814938\n",
      "Iteration 1257, loss = 0.68814893\n",
      "Iteration 1258, loss = 0.68814625\n",
      "Iteration 1259, loss = 0.68814542\n",
      "Iteration 1260, loss = 0.68814415\n",
      "Iteration 1261, loss = 0.68814238\n",
      "Iteration 1262, loss = 0.68814450\n",
      "Iteration 1263, loss = 0.68813965\n",
      "Iteration 1264, loss = 0.68813890\n",
      "Iteration 1265, loss = 0.68813583\n",
      "Iteration 1266, loss = 0.68813457\n",
      "Iteration 1267, loss = 0.68813266\n",
      "Iteration 1268, loss = 0.68813163\n",
      "Iteration 1269, loss = 0.68813039\n",
      "Iteration 1270, loss = 0.68812701\n",
      "Iteration 1271, loss = 0.68812582\n",
      "Iteration 1272, loss = 0.68812574\n",
      "Iteration 1273, loss = 0.68812222\n",
      "Iteration 1274, loss = 0.68813039\n",
      "Iteration 1275, loss = 0.68811967\n",
      "Iteration 1276, loss = 0.68811871\n",
      "Iteration 1277, loss = 0.68811569\n",
      "Iteration 1278, loss = 0.68811255\n",
      "Iteration 1279, loss = 0.68811107\n",
      "Iteration 1280, loss = 0.68811401\n",
      "Iteration 1281, loss = 0.68811387\n",
      "Iteration 1282, loss = 0.68811043\n",
      "Iteration 1283, loss = 0.68810297\n",
      "Iteration 1284, loss = 0.68810678\n",
      "Iteration 1285, loss = 0.68810061\n",
      "Iteration 1286, loss = 0.68810048\n",
      "Iteration 1287, loss = 0.68809614\n",
      "Iteration 1288, loss = 0.68810279\n",
      "Iteration 1289, loss = 0.68809721\n",
      "Iteration 1290, loss = 0.68809601\n",
      "Iteration 1291, loss = 0.68809031\n",
      "Iteration 1292, loss = 0.68808961\n",
      "Iteration 1293, loss = 0.68809077\n",
      "Iteration 1294, loss = 0.68808533\n",
      "Iteration 1295, loss = 0.68808240\n",
      "Iteration 1296, loss = 0.68807998\n",
      "Iteration 1297, loss = 0.68808944\n",
      "Iteration 1298, loss = 0.68807903\n",
      "Iteration 1299, loss = 0.68807652\n",
      "Iteration 1300, loss = 0.68807704\n",
      "Iteration 1301, loss = 0.68807198\n",
      "Iteration 1302, loss = 0.68807188\n",
      "Iteration 1303, loss = 0.68807148\n",
      "Iteration 1304, loss = 0.68807391\n",
      "Iteration 1305, loss = 0.68806751\n",
      "Iteration 1306, loss = 0.68806228\n",
      "Iteration 1307, loss = 0.68806443\n",
      "Iteration 1308, loss = 0.68805991\n",
      "Iteration 1309, loss = 0.68806053\n",
      "Iteration 1310, loss = 0.68805572\n",
      "Iteration 1311, loss = 0.68805337\n",
      "Iteration 1312, loss = 0.68805370\n",
      "Iteration 1313, loss = 0.68804878\n",
      "Iteration 1314, loss = 0.68804870\n",
      "Iteration 1315, loss = 0.68805685\n",
      "Iteration 1316, loss = 0.68804329\n",
      "Iteration 1317, loss = 0.68804080\n",
      "Iteration 1318, loss = 0.68804277\n",
      "Iteration 1319, loss = 0.68803888\n",
      "Iteration 1320, loss = 0.68803532\n",
      "Iteration 1321, loss = 0.68803195\n",
      "Iteration 1322, loss = 0.68803947\n",
      "Iteration 1323, loss = 0.68802742\n",
      "Iteration 1324, loss = 0.68802455\n",
      "Iteration 1325, loss = 0.68803610\n",
      "Iteration 1326, loss = 0.68802086\n",
      "Iteration 1327, loss = 0.68802433\n",
      "Iteration 1328, loss = 0.68801676\n",
      "Iteration 1329, loss = 0.68801402\n",
      "Iteration 1330, loss = 0.68801369\n",
      "Iteration 1331, loss = 0.68801108\n",
      "Iteration 1332, loss = 0.68800722\n",
      "Iteration 1333, loss = 0.68800337\n",
      "Iteration 1334, loss = 0.68800466\n",
      "Iteration 1335, loss = 0.68799894\n",
      "Iteration 1336, loss = 0.68799624\n",
      "Iteration 1337, loss = 0.68799442\n",
      "Iteration 1338, loss = 0.68799693\n",
      "Iteration 1339, loss = 0.68798906\n",
      "Iteration 1340, loss = 0.68799324\n",
      "Iteration 1341, loss = 0.68798427\n",
      "Iteration 1342, loss = 0.68798330\n",
      "Iteration 1343, loss = 0.68798016\n",
      "Iteration 1344, loss = 0.68797819\n",
      "Iteration 1345, loss = 0.68797666\n",
      "Iteration 1346, loss = 0.68797419\n",
      "Iteration 1347, loss = 0.68797575\n",
      "Iteration 1348, loss = 0.68797072\n",
      "Iteration 1349, loss = 0.68796616\n",
      "Iteration 1350, loss = 0.68796747\n",
      "Iteration 1351, loss = 0.68796298\n",
      "Iteration 1352, loss = 0.68796054\n",
      "Iteration 1353, loss = 0.68796136\n",
      "Iteration 1354, loss = 0.68795770\n",
      "Iteration 1355, loss = 0.68796198\n",
      "Iteration 1356, loss = 0.68795229\n",
      "Iteration 1357, loss = 0.68795276\n",
      "Iteration 1358, loss = 0.68794965\n",
      "Iteration 1359, loss = 0.68794843\n",
      "Iteration 1360, loss = 0.68794411\n",
      "Iteration 1361, loss = 0.68794787\n",
      "Iteration 1362, loss = 0.68794025\n",
      "Iteration 1363, loss = 0.68793583\n",
      "Iteration 1364, loss = 0.68793594\n",
      "Iteration 1365, loss = 0.68793150\n",
      "Iteration 1366, loss = 0.68792825\n",
      "Iteration 1367, loss = 0.68792635\n",
      "Iteration 1368, loss = 0.68792582\n",
      "Iteration 1369, loss = 0.68792084\n",
      "Iteration 1370, loss = 0.68791771\n",
      "Iteration 1371, loss = 0.68791875\n",
      "Iteration 1372, loss = 0.68791417\n",
      "Iteration 1373, loss = 0.68790997\n",
      "Iteration 1374, loss = 0.68791240\n",
      "Iteration 1375, loss = 0.68790444\n",
      "Iteration 1376, loss = 0.68790295\n",
      "Iteration 1377, loss = 0.68789867\n",
      "Iteration 1378, loss = 0.68789595\n",
      "Iteration 1379, loss = 0.68789537\n",
      "Iteration 1380, loss = 0.68789806\n",
      "Iteration 1381, loss = 0.68789014\n",
      "Iteration 1382, loss = 0.68788671\n",
      "Iteration 1383, loss = 0.68788456\n",
      "Iteration 1384, loss = 0.68789064\n",
      "Iteration 1385, loss = 0.68787787\n",
      "Iteration 1386, loss = 0.68788149\n",
      "Iteration 1387, loss = 0.68787219\n",
      "Iteration 1388, loss = 0.68786841\n",
      "Iteration 1389, loss = 0.68786995\n",
      "Iteration 1390, loss = 0.68787154\n",
      "Iteration 1391, loss = 0.68786411\n",
      "Iteration 1392, loss = 0.68785865\n",
      "Iteration 1393, loss = 0.68785433\n",
      "Iteration 1394, loss = 0.68785261\n",
      "Iteration 1395, loss = 0.68785920\n",
      "Iteration 1396, loss = 0.68785231\n",
      "Iteration 1397, loss = 0.68784221\n",
      "Iteration 1398, loss = 0.68783908\n",
      "Iteration 1399, loss = 0.68783628\n",
      "Iteration 1400, loss = 0.68783258\n",
      "Iteration 1401, loss = 0.68783328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1402, loss = 0.68783170\n",
      "Iteration 1403, loss = 0.68782712\n",
      "Iteration 1404, loss = 0.68782081\n",
      "Iteration 1405, loss = 0.68781716\n",
      "Iteration 1406, loss = 0.68781965\n",
      "Iteration 1407, loss = 0.68781533\n",
      "Iteration 1408, loss = 0.68781023\n",
      "Iteration 1409, loss = 0.68780710\n",
      "Iteration 1410, loss = 0.68780430\n",
      "Iteration 1411, loss = 0.68780155\n",
      "Iteration 1412, loss = 0.68779926\n",
      "Iteration 1413, loss = 0.68780184\n",
      "Iteration 1414, loss = 0.68779683\n",
      "Iteration 1415, loss = 0.68779054\n",
      "Iteration 1416, loss = 0.68779369\n",
      "Iteration 1417, loss = 0.68778581\n",
      "Iteration 1418, loss = 0.68778163\n",
      "Iteration 1419, loss = 0.68778160\n",
      "Iteration 1420, loss = 0.68778022\n",
      "Iteration 1421, loss = 0.68777620\n",
      "Iteration 1422, loss = 0.68777304\n",
      "Iteration 1423, loss = 0.68776815\n",
      "Iteration 1424, loss = 0.68776518\n",
      "Iteration 1425, loss = 0.68776253\n",
      "Iteration 1426, loss = 0.68776221\n",
      "Iteration 1427, loss = 0.68776221\n",
      "Iteration 1428, loss = 0.68775870\n",
      "Iteration 1429, loss = 0.68775671\n",
      "Iteration 1430, loss = 0.68775084\n",
      "Iteration 1431, loss = 0.68775267\n",
      "Iteration 1432, loss = 0.68774729\n",
      "Iteration 1433, loss = 0.68774489\n",
      "Iteration 1434, loss = 0.68774232\n",
      "Iteration 1435, loss = 0.68774007\n",
      "Iteration 1436, loss = 0.68774162\n",
      "Iteration 1437, loss = 0.68773638\n",
      "Iteration 1438, loss = 0.68773259\n",
      "Iteration 1439, loss = 0.68773395\n",
      "Iteration 1440, loss = 0.68772873\n",
      "Iteration 1441, loss = 0.68772666\n",
      "Iteration 1442, loss = 0.68772913\n",
      "Iteration 1443, loss = 0.68772318\n",
      "Iteration 1444, loss = 0.68771912\n",
      "Iteration 1445, loss = 0.68771844\n",
      "Iteration 1446, loss = 0.68772094\n",
      "Iteration 1447, loss = 0.68771308\n",
      "Iteration 1448, loss = 0.68771533\n",
      "Iteration 1449, loss = 0.68770950\n",
      "Iteration 1450, loss = 0.68770815\n",
      "Iteration 1451, loss = 0.68770584\n",
      "Iteration 1452, loss = 0.68770356\n",
      "Iteration 1453, loss = 0.68770052\n",
      "Iteration 1454, loss = 0.68769920\n",
      "Iteration 1455, loss = 0.68769575\n",
      "Iteration 1456, loss = 0.68769832\n",
      "Iteration 1457, loss = 0.68769305\n",
      "Iteration 1458, loss = 0.68768953\n",
      "Iteration 1459, loss = 0.68768841\n",
      "Iteration 1460, loss = 0.68768366\n",
      "Iteration 1461, loss = 0.68768464\n",
      "Iteration 1462, loss = 0.68768236\n",
      "Iteration 1463, loss = 0.68767971\n",
      "Iteration 1464, loss = 0.68767636\n",
      "Iteration 1465, loss = 0.68767550\n",
      "Iteration 1466, loss = 0.68767027\n",
      "Iteration 1467, loss = 0.68766964\n",
      "Iteration 1468, loss = 0.68766793\n",
      "Iteration 1469, loss = 0.68766428\n",
      "Iteration 1470, loss = 0.68766358\n",
      "Iteration 1471, loss = 0.68766309\n",
      "Iteration 1472, loss = 0.68766101\n",
      "Iteration 1473, loss = 0.68766080\n",
      "Iteration 1474, loss = 0.68765784\n",
      "Iteration 1475, loss = 0.68765424\n",
      "Iteration 1476, loss = 0.68764966\n",
      "Iteration 1477, loss = 0.68765142\n",
      "Iteration 1478, loss = 0.68764693\n",
      "Iteration 1479, loss = 0.68764355\n",
      "Iteration 1480, loss = 0.68763814\n",
      "Iteration 1481, loss = 0.68763920\n",
      "Iteration 1482, loss = 0.68763711\n",
      "Iteration 1483, loss = 0.68763206\n",
      "Iteration 1484, loss = 0.68763104\n",
      "Iteration 1485, loss = 0.68762856\n",
      "Iteration 1486, loss = 0.68762684\n",
      "Iteration 1487, loss = 0.68762370\n",
      "Iteration 1488, loss = 0.68762297\n",
      "Iteration 1489, loss = 0.68762116\n",
      "Iteration 1490, loss = 0.68761849\n",
      "Iteration 1491, loss = 0.68761465\n",
      "Iteration 1492, loss = 0.68761241\n",
      "Iteration 1493, loss = 0.68761043\n",
      "Iteration 1494, loss = 0.68761747\n",
      "Iteration 1495, loss = 0.68760678\n",
      "Iteration 1496, loss = 0.68760535\n",
      "Iteration 1497, loss = 0.68760222\n",
      "Iteration 1498, loss = 0.68759794\n",
      "Iteration 1499, loss = 0.68759604\n",
      "Iteration 1500, loss = 0.68759684\n",
      "Iteration 1501, loss = 0.68759211\n",
      "Iteration 1502, loss = 0.68759083\n",
      "Iteration 1503, loss = 0.68758736\n",
      "Iteration 1504, loss = 0.68758558\n",
      "Iteration 1505, loss = 0.68758218\n",
      "Iteration 1506, loss = 0.68758054\n",
      "Iteration 1507, loss = 0.68757697\n",
      "Iteration 1508, loss = 0.68757439\n",
      "Iteration 1509, loss = 0.68757170\n",
      "Iteration 1510, loss = 0.68756944\n",
      "Iteration 1511, loss = 0.68757766\n",
      "Iteration 1512, loss = 0.68756439\n",
      "Iteration 1513, loss = 0.68756072\n",
      "Iteration 1514, loss = 0.68756364\n",
      "Iteration 1515, loss = 0.68755810\n",
      "Iteration 1516, loss = 0.68755548\n",
      "Iteration 1517, loss = 0.68755157\n",
      "Iteration 1518, loss = 0.68755176\n",
      "Iteration 1519, loss = 0.68754781\n",
      "Iteration 1520, loss = 0.68754456\n",
      "Iteration 1521, loss = 0.68754149\n",
      "Iteration 1522, loss = 0.68754002\n",
      "Iteration 1523, loss = 0.68753893\n",
      "Iteration 1524, loss = 0.68753861\n",
      "Iteration 1525, loss = 0.68753730\n",
      "Iteration 1526, loss = 0.68753431\n",
      "Iteration 1527, loss = 0.68753162\n",
      "Iteration 1528, loss = 0.68753367\n",
      "Iteration 1529, loss = 0.68752574\n",
      "Iteration 1530, loss = 0.68752372\n",
      "Iteration 1531, loss = 0.68751845\n",
      "Iteration 1532, loss = 0.68751827\n",
      "Iteration 1533, loss = 0.68751573\n",
      "Iteration 1534, loss = 0.68751195\n",
      "Iteration 1535, loss = 0.68750923\n",
      "Iteration 1536, loss = 0.68751090\n",
      "Iteration 1537, loss = 0.68750740\n",
      "Iteration 1538, loss = 0.68750586\n",
      "Iteration 1539, loss = 0.68750451\n",
      "Iteration 1540, loss = 0.68749867\n",
      "Iteration 1541, loss = 0.68749701\n",
      "Iteration 1542, loss = 0.68749563\n",
      "Iteration 1543, loss = 0.68749226\n",
      "Iteration 1544, loss = 0.68749544\n",
      "Iteration 1545, loss = 0.68748794\n",
      "Iteration 1546, loss = 0.68748709\n",
      "Iteration 1547, loss = 0.68748137\n",
      "Iteration 1548, loss = 0.68748282\n",
      "Iteration 1549, loss = 0.68747785\n",
      "Iteration 1550, loss = 0.68747595\n",
      "Iteration 1551, loss = 0.68747206\n",
      "Iteration 1552, loss = 0.68747109\n",
      "Iteration 1553, loss = 0.68746692\n",
      "Iteration 1554, loss = 0.68746565\n",
      "Iteration 1555, loss = 0.68746301\n",
      "Iteration 1556, loss = 0.68746292\n",
      "Iteration 1557, loss = 0.68745767\n",
      "Iteration 1558, loss = 0.68745571\n",
      "Iteration 1559, loss = 0.68745438\n",
      "Iteration 1560, loss = 0.68745292\n",
      "Iteration 1561, loss = 0.68744793\n",
      "Iteration 1562, loss = 0.68744892\n",
      "Iteration 1563, loss = 0.68744346\n",
      "Iteration 1564, loss = 0.68744358\n",
      "Iteration 1565, loss = 0.68744291\n",
      "Iteration 1566, loss = 0.68743771\n",
      "Iteration 1567, loss = 0.68743954\n",
      "Iteration 1568, loss = 0.68743268\n",
      "Iteration 1569, loss = 0.68742956\n",
      "Iteration 1570, loss = 0.68742644\n",
      "Iteration 1571, loss = 0.68742652\n",
      "Iteration 1572, loss = 0.68742571\n",
      "Iteration 1573, loss = 0.68742113\n",
      "Iteration 1574, loss = 0.68741772\n",
      "Iteration 1575, loss = 0.68741574\n",
      "Iteration 1576, loss = 0.68741369\n",
      "Iteration 1577, loss = 0.68741389\n",
      "Iteration 1578, loss = 0.68742419\n",
      "Iteration 1579, loss = 0.68741015\n",
      "Iteration 1580, loss = 0.68740450\n",
      "Iteration 1581, loss = 0.68740266\n",
      "Iteration 1582, loss = 0.68740105\n",
      "Iteration 1583, loss = 0.68739671\n",
      "Iteration 1584, loss = 0.68739614\n",
      "Iteration 1585, loss = 0.68739238\n",
      "Iteration 1586, loss = 0.68739464\n",
      "Iteration 1587, loss = 0.68739214\n",
      "Iteration 1588, loss = 0.68738607\n",
      "Iteration 1589, loss = 0.68738357\n",
      "Iteration 1590, loss = 0.68738219\n",
      "Iteration 1591, loss = 0.68738207\n",
      "Iteration 1592, loss = 0.68738122\n",
      "Iteration 1593, loss = 0.68737757\n",
      "Iteration 1594, loss = 0.68737269\n",
      "Iteration 1595, loss = 0.68737052\n",
      "Iteration 1596, loss = 0.68736820\n",
      "Iteration 1597, loss = 0.68736981\n",
      "Iteration 1598, loss = 0.68736476\n",
      "Iteration 1599, loss = 0.68736594\n",
      "Iteration 1600, loss = 0.68735718\n",
      "Iteration 1601, loss = 0.68735748\n",
      "Iteration 1602, loss = 0.68736185\n",
      "Iteration 1603, loss = 0.68735095\n",
      "Iteration 1604, loss = 0.68735177\n",
      "Iteration 1605, loss = 0.68734514\n",
      "Iteration 1606, loss = 0.68734339\n",
      "Iteration 1607, loss = 0.68734454\n",
      "Iteration 1608, loss = 0.68733820\n",
      "Iteration 1609, loss = 0.68733939\n",
      "Iteration 1610, loss = 0.68733991\n",
      "Iteration 1611, loss = 0.68733125\n",
      "Iteration 1612, loss = 0.68733224\n",
      "Iteration 1613, loss = 0.68733003\n",
      "Iteration 1614, loss = 0.68732497\n",
      "Iteration 1615, loss = 0.68732289\n",
      "Iteration 1616, loss = 0.68731983\n",
      "Iteration 1617, loss = 0.68731714\n",
      "Iteration 1618, loss = 0.68731713\n",
      "Iteration 1619, loss = 0.68731150\n",
      "Iteration 1620, loss = 0.68731113\n",
      "Iteration 1621, loss = 0.68731092\n",
      "Iteration 1622, loss = 0.68730519\n",
      "Iteration 1623, loss = 0.68730325\n",
      "Iteration 1624, loss = 0.68730208\n",
      "Iteration 1625, loss = 0.68729811\n",
      "Iteration 1626, loss = 0.68729893\n",
      "Iteration 1627, loss = 0.68729502\n",
      "Iteration 1628, loss = 0.68729895\n",
      "Iteration 1629, loss = 0.68729049\n",
      "Iteration 1630, loss = 0.68728811\n",
      "Iteration 1631, loss = 0.68728440\n",
      "Iteration 1632, loss = 0.68728262\n",
      "Iteration 1633, loss = 0.68728478\n",
      "Iteration 1634, loss = 0.68728935\n",
      "Iteration 1635, loss = 0.68727617\n",
      "Iteration 1636, loss = 0.68727434\n",
      "Iteration 1637, loss = 0.68727423\n",
      "Iteration 1638, loss = 0.68727084\n",
      "Iteration 1639, loss = 0.68726673\n",
      "Iteration 1640, loss = 0.68726514\n",
      "Iteration 1641, loss = 0.68726227\n",
      "Iteration 1642, loss = 0.68726420\n",
      "Iteration 1643, loss = 0.68725849\n",
      "Iteration 1644, loss = 0.68725672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1645, loss = 0.68725540\n",
      "Iteration 1646, loss = 0.68725335\n",
      "Iteration 1647, loss = 0.68725029\n",
      "Iteration 1648, loss = 0.68724574\n",
      "Iteration 1649, loss = 0.68724644\n",
      "Iteration 1650, loss = 0.68724182\n",
      "Iteration 1651, loss = 0.68724141\n",
      "Iteration 1652, loss = 0.68723602\n",
      "Iteration 1653, loss = 0.68723537\n",
      "Iteration 1654, loss = 0.68723439\n",
      "Iteration 1655, loss = 0.68723399\n",
      "Iteration 1656, loss = 0.68723040\n",
      "Iteration 1657, loss = 0.68722741\n",
      "Iteration 1658, loss = 0.68722702\n",
      "Iteration 1659, loss = 0.68722235\n",
      "Iteration 1660, loss = 0.68722263\n",
      "Iteration 1661, loss = 0.68721879\n",
      "Iteration 1662, loss = 0.68721594\n",
      "Iteration 1663, loss = 0.68721565\n",
      "Iteration 1664, loss = 0.68721228\n",
      "Iteration 1665, loss = 0.68721556\n",
      "Iteration 1666, loss = 0.68720966\n",
      "Iteration 1667, loss = 0.68720570\n",
      "Iteration 1668, loss = 0.68720698\n",
      "Iteration 1669, loss = 0.68720174\n",
      "Iteration 1670, loss = 0.68720023\n",
      "Iteration 1671, loss = 0.68719663\n",
      "Iteration 1672, loss = 0.68719372\n",
      "Iteration 1673, loss = 0.68719485\n",
      "Iteration 1674, loss = 0.68719179\n",
      "Iteration 1675, loss = 0.68719260\n",
      "Iteration 1676, loss = 0.68718920\n",
      "Iteration 1677, loss = 0.68718462\n",
      "Iteration 1678, loss = 0.68718579\n",
      "Iteration 1679, loss = 0.68718151\n",
      "Iteration 1680, loss = 0.68717852\n",
      "Iteration 1681, loss = 0.68717383\n",
      "Iteration 1682, loss = 0.68717251\n",
      "Iteration 1683, loss = 0.68717637\n",
      "Iteration 1684, loss = 0.68717203\n",
      "Iteration 1685, loss = 0.68716554\n",
      "Iteration 1686, loss = 0.68716769\n",
      "Iteration 1687, loss = 0.68716198\n",
      "Iteration 1688, loss = 0.68716155\n",
      "Iteration 1689, loss = 0.68715737\n",
      "Iteration 1690, loss = 0.68715906\n",
      "Iteration 1691, loss = 0.68715409\n",
      "Iteration 1692, loss = 0.68715028\n",
      "Iteration 1693, loss = 0.68715023\n",
      "Iteration 1694, loss = 0.68714784\n",
      "Iteration 1695, loss = 0.68714605\n",
      "Iteration 1696, loss = 0.68714378\n",
      "Iteration 1697, loss = 0.68714228\n",
      "Iteration 1698, loss = 0.68713918\n",
      "Iteration 1699, loss = 0.68713672\n",
      "Iteration 1700, loss = 0.68713451\n",
      "Iteration 1701, loss = 0.68713248\n",
      "Iteration 1702, loss = 0.68713215\n",
      "Iteration 1703, loss = 0.68712990\n",
      "Iteration 1704, loss = 0.68712492\n",
      "Iteration 1705, loss = 0.68712557\n",
      "Iteration 1706, loss = 0.68712009\n",
      "Iteration 1707, loss = 0.68711871\n",
      "Iteration 1708, loss = 0.68711780\n",
      "Iteration 1709, loss = 0.68711382\n",
      "Iteration 1710, loss = 0.68711464\n",
      "Iteration 1711, loss = 0.68710886\n",
      "Iteration 1712, loss = 0.68711428\n",
      "Iteration 1713, loss = 0.68711056\n",
      "Iteration 1714, loss = 0.68710661\n",
      "Iteration 1715, loss = 0.68710256\n",
      "Iteration 1716, loss = 0.68709941\n",
      "Iteration 1717, loss = 0.68710130\n",
      "Iteration 1718, loss = 0.68710219\n",
      "Iteration 1719, loss = 0.68709352\n",
      "Iteration 1720, loss = 0.68709255\n",
      "Iteration 1721, loss = 0.68708822\n",
      "Iteration 1722, loss = 0.68708995\n",
      "Iteration 1723, loss = 0.68708302\n",
      "Iteration 1724, loss = 0.68708118\n",
      "Iteration 1725, loss = 0.68707964\n",
      "Iteration 1726, loss = 0.68708020\n",
      "Iteration 1727, loss = 0.68707800\n",
      "Iteration 1728, loss = 0.68707272\n",
      "Iteration 1729, loss = 0.68707056\n",
      "Iteration 1730, loss = 0.68706829\n",
      "Iteration 1731, loss = 0.68706717\n",
      "Iteration 1732, loss = 0.68706753\n",
      "Iteration 1733, loss = 0.68706121\n",
      "Iteration 1734, loss = 0.68705953\n",
      "Iteration 1735, loss = 0.68705998\n",
      "Iteration 1736, loss = 0.68705685\n",
      "Iteration 1737, loss = 0.68705380\n",
      "Iteration 1738, loss = 0.68705005\n",
      "Iteration 1739, loss = 0.68705374\n",
      "Iteration 1740, loss = 0.68705271\n",
      "Iteration 1741, loss = 0.68704593\n",
      "Iteration 1742, loss = 0.68704118\n",
      "Iteration 1743, loss = 0.68704163\n",
      "Iteration 1744, loss = 0.68703820\n",
      "Iteration 1745, loss = 0.68703625\n",
      "Iteration 1746, loss = 0.68703295\n",
      "Iteration 1747, loss = 0.68703612\n",
      "Iteration 1748, loss = 0.68703525\n",
      "Iteration 1749, loss = 0.68702622\n",
      "Iteration 1750, loss = 0.68702379\n",
      "Iteration 1751, loss = 0.68702134\n",
      "Iteration 1752, loss = 0.68702026\n",
      "Iteration 1753, loss = 0.68701766\n",
      "Iteration 1754, loss = 0.68701632\n",
      "Iteration 1755, loss = 0.68701345\n",
      "Iteration 1756, loss = 0.68701142\n",
      "Iteration 1757, loss = 0.68701019\n",
      "Iteration 1758, loss = 0.68700873\n",
      "Iteration 1759, loss = 0.68700670\n",
      "Iteration 1760, loss = 0.68700325\n",
      "Iteration 1761, loss = 0.68700064\n",
      "Iteration 1762, loss = 0.68699825\n",
      "Iteration 1763, loss = 0.68699552\n",
      "Iteration 1764, loss = 0.68699609\n",
      "Iteration 1765, loss = 0.68699339\n",
      "Iteration 1766, loss = 0.68699449\n",
      "Iteration 1767, loss = 0.68699051\n",
      "Iteration 1768, loss = 0.68698971\n",
      "Iteration 1769, loss = 0.68698271\n",
      "Iteration 1770, loss = 0.68698090\n",
      "Iteration 1771, loss = 0.68697756\n",
      "Iteration 1772, loss = 0.68697753\n",
      "Iteration 1773, loss = 0.68697539\n",
      "Iteration 1774, loss = 0.68697305\n",
      "Iteration 1775, loss = 0.68696854\n",
      "Iteration 1776, loss = 0.68696933\n",
      "Iteration 1777, loss = 0.68696635\n",
      "Iteration 1778, loss = 0.68696684\n",
      "Iteration 1779, loss = 0.68696051\n",
      "Iteration 1780, loss = 0.68695942\n",
      "Iteration 1781, loss = 0.68695494\n",
      "Iteration 1782, loss = 0.68695301\n",
      "Iteration 1783, loss = 0.68694989\n",
      "Iteration 1784, loss = 0.68694874\n",
      "Iteration 1785, loss = 0.68695719\n",
      "Iteration 1786, loss = 0.68694556\n",
      "Iteration 1787, loss = 0.68694237\n",
      "Iteration 1788, loss = 0.68694446\n",
      "Iteration 1789, loss = 0.68693713\n",
      "Iteration 1790, loss = 0.68693471\n",
      "Iteration 1791, loss = 0.68693393\n",
      "Iteration 1792, loss = 0.68692955\n",
      "Iteration 1793, loss = 0.68692897\n",
      "Iteration 1794, loss = 0.68692444\n",
      "Iteration 1795, loss = 0.68692188\n",
      "Iteration 1796, loss = 0.68692276\n",
      "Iteration 1797, loss = 0.68692703\n",
      "Iteration 1798, loss = 0.68691492\n",
      "Iteration 1799, loss = 0.68691579\n",
      "Iteration 1800, loss = 0.68691421\n",
      "Iteration 1801, loss = 0.68690846\n",
      "Iteration 1802, loss = 0.68690829\n",
      "Iteration 1803, loss = 0.68690754\n",
      "Iteration 1804, loss = 0.68690141\n",
      "Iteration 1805, loss = 0.68689973\n",
      "Iteration 1806, loss = 0.68689756\n",
      "Iteration 1807, loss = 0.68689702\n",
      "Iteration 1808, loss = 0.68689322\n",
      "Iteration 1809, loss = 0.68689303\n",
      "Iteration 1810, loss = 0.68688818\n",
      "Iteration 1811, loss = 0.68689066\n",
      "Iteration 1812, loss = 0.68689443\n",
      "Iteration 1813, loss = 0.68689005\n",
      "Iteration 1814, loss = 0.68688270\n",
      "Iteration 1815, loss = 0.68687627\n",
      "Iteration 1816, loss = 0.68687607\n",
      "Iteration 1817, loss = 0.68687346\n",
      "Iteration 1818, loss = 0.68687213\n",
      "Iteration 1819, loss = 0.68686995\n",
      "Iteration 1820, loss = 0.68686814\n",
      "Iteration 1821, loss = 0.68686613\n",
      "Iteration 1822, loss = 0.68687014\n",
      "Iteration 1823, loss = 0.68686000\n",
      "Iteration 1824, loss = 0.68685937\n",
      "Iteration 1825, loss = 0.68685556\n",
      "Iteration 1826, loss = 0.68685542\n",
      "Iteration 1827, loss = 0.68686174\n",
      "Iteration 1828, loss = 0.68685007\n",
      "Iteration 1829, loss = 0.68685034\n",
      "Iteration 1830, loss = 0.68684685\n",
      "Iteration 1831, loss = 0.68684424\n",
      "Iteration 1832, loss = 0.68684129\n",
      "Iteration 1833, loss = 0.68684389\n",
      "Iteration 1834, loss = 0.68684245\n",
      "Iteration 1835, loss = 0.68683809\n",
      "Iteration 1836, loss = 0.68683757\n",
      "Iteration 1837, loss = 0.68683286\n",
      "Iteration 1838, loss = 0.68682972\n",
      "Iteration 1839, loss = 0.68682876\n",
      "Iteration 1840, loss = 0.68682621\n",
      "Iteration 1841, loss = 0.68682403\n",
      "Iteration 1842, loss = 0.68682507\n",
      "Iteration 1843, loss = 0.68682238\n",
      "Iteration 1844, loss = 0.68682222\n",
      "Iteration 1845, loss = 0.68681870\n",
      "Iteration 1846, loss = 0.68682005\n",
      "Iteration 1847, loss = 0.68681758\n",
      "Iteration 1848, loss = 0.68681459\n",
      "Iteration 1849, loss = 0.68682385\n",
      "Iteration 1850, loss = 0.68680959\n",
      "Iteration 1851, loss = 0.68681208\n",
      "Iteration 1852, loss = 0.68680896\n",
      "Iteration 1853, loss = 0.68680916\n",
      "Iteration 1854, loss = 0.68680218\n",
      "Iteration 1855, loss = 0.68679993\n",
      "Iteration 1856, loss = 0.68680439\n",
      "Iteration 1857, loss = 0.68679486\n",
      "Iteration 1858, loss = 0.68679402\n",
      "Iteration 1859, loss = 0.68679070\n",
      "Iteration 1860, loss = 0.68678863\n",
      "Iteration 1861, loss = 0.68678900\n",
      "Iteration 1862, loss = 0.68678448\n",
      "Iteration 1863, loss = 0.68678246\n",
      "Iteration 1864, loss = 0.68677993\n",
      "Iteration 1865, loss = 0.68677967\n",
      "Iteration 1866, loss = 0.68677840\n",
      "Iteration 1867, loss = 0.68677756\n",
      "Iteration 1868, loss = 0.68677218\n",
      "Iteration 1869, loss = 0.68677142\n",
      "Iteration 1870, loss = 0.68676932\n",
      "Iteration 1871, loss = 0.68676638\n",
      "Iteration 1872, loss = 0.68676781\n",
      "Iteration 1873, loss = 0.68676265\n",
      "Iteration 1874, loss = 0.68676558\n",
      "Iteration 1875, loss = 0.68675888\n",
      "Iteration 1876, loss = 0.68676052\n",
      "Iteration 1877, loss = 0.68675607\n",
      "Iteration 1878, loss = 0.68675507\n",
      "Iteration 1879, loss = 0.68675240\n",
      "Iteration 1880, loss = 0.68674976\n",
      "Iteration 1881, loss = 0.68675735\n",
      "Iteration 1882, loss = 0.68674565\n",
      "Iteration 1883, loss = 0.68674621\n",
      "Iteration 1884, loss = 0.68674320\n",
      "Iteration 1885, loss = 0.68673965\n",
      "Iteration 1886, loss = 0.68673750\n",
      "Iteration 1887, loss = 0.68673868\n",
      "Iteration 1888, loss = 0.68673409\n",
      "Iteration 1889, loss = 0.68673264\n",
      "Iteration 1890, loss = 0.68673024\n",
      "Iteration 1891, loss = 0.68672960\n",
      "Iteration 1892, loss = 0.68673044\n",
      "Iteration 1893, loss = 0.68672340\n",
      "Iteration 1894, loss = 0.68672462\n",
      "Iteration 1895, loss = 0.68672170\n",
      "Iteration 1896, loss = 0.68671862\n",
      "Iteration 1897, loss = 0.68672322\n",
      "Iteration 1898, loss = 0.68671847\n",
      "Iteration 1899, loss = 0.68671417\n",
      "Iteration 1900, loss = 0.68670995\n",
      "Iteration 1901, loss = 0.68670908\n",
      "Iteration 1902, loss = 0.68671117\n",
      "Iteration 1903, loss = 0.68670442\n",
      "Iteration 1904, loss = 0.68670141\n",
      "Iteration 1905, loss = 0.68670237\n",
      "Iteration 1906, loss = 0.68670257\n",
      "Iteration 1907, loss = 0.68670042\n",
      "Iteration 1908, loss = 0.68669373\n",
      "Iteration 1909, loss = 0.68669132\n",
      "Iteration 1910, loss = 0.68669477\n",
      "Iteration 1911, loss = 0.68668856\n",
      "Iteration 1912, loss = 0.68668641\n",
      "Iteration 1913, loss = 0.68668490\n",
      "Iteration 1914, loss = 0.68668156\n",
      "Iteration 1915, loss = 0.68668470\n",
      "Iteration 1916, loss = 0.68668266\n",
      "Iteration 1917, loss = 0.68667743\n",
      "Iteration 1918, loss = 0.68667356\n",
      "Iteration 1919, loss = 0.68667311\n",
      "Iteration 1920, loss = 0.68666972\n",
      "Iteration 1921, loss = 0.68666861\n",
      "Iteration 1922, loss = 0.68666635\n",
      "Iteration 1923, loss = 0.68666972\n",
      "Iteration 1924, loss = 0.68666249\n",
      "Iteration 1925, loss = 0.68665864\n",
      "Iteration 1926, loss = 0.68665815\n",
      "Iteration 1927, loss = 0.68665826\n",
      "Iteration 1928, loss = 0.68665292\n",
      "Iteration 1929, loss = 0.68665626\n",
      "Iteration 1930, loss = 0.68665080\n",
      "Iteration 1931, loss = 0.68665096\n",
      "Iteration 1932, loss = 0.68666430\n",
      "Iteration 1933, loss = 0.68664499\n",
      "Iteration 1934, loss = 0.68664610\n",
      "Iteration 1935, loss = 0.68663969\n",
      "Iteration 1936, loss = 0.68664059\n",
      "Iteration 1937, loss = 0.68663960\n",
      "Iteration 1938, loss = 0.68663589\n",
      "Iteration 1939, loss = 0.68663273\n",
      "Iteration 1940, loss = 0.68663105\n",
      "Iteration 1941, loss = 0.68662804\n",
      "Iteration 1942, loss = 0.68662609\n",
      "Iteration 1943, loss = 0.68662578\n",
      "Iteration 1944, loss = 0.68662042\n",
      "Iteration 1945, loss = 0.68661857\n",
      "Iteration 1946, loss = 0.68661762\n",
      "Iteration 1947, loss = 0.68661377\n",
      "Iteration 1948, loss = 0.68661266\n",
      "Iteration 1949, loss = 0.68662168\n",
      "Iteration 1950, loss = 0.68660824\n",
      "Iteration 1951, loss = 0.68661037\n",
      "Iteration 1952, loss = 0.68660737\n",
      "Iteration 1953, loss = 0.68660172\n",
      "Iteration 1954, loss = 0.68660241\n",
      "Iteration 1955, loss = 0.68660120\n",
      "Iteration 1956, loss = 0.68659789\n",
      "Iteration 1957, loss = 0.68660113\n",
      "Iteration 1958, loss = 0.68659065\n",
      "Iteration 1959, loss = 0.68659019\n",
      "Iteration 1960, loss = 0.68658755\n",
      "Iteration 1961, loss = 0.68658570\n",
      "Iteration 1962, loss = 0.68658429\n",
      "Iteration 1963, loss = 0.68658506\n",
      "Iteration 1964, loss = 0.68658477\n",
      "Iteration 1965, loss = 0.68657816\n",
      "Iteration 1966, loss = 0.68657706\n",
      "Iteration 1967, loss = 0.68657549\n",
      "Iteration 1968, loss = 0.68657073\n",
      "Iteration 1969, loss = 0.68657395\n",
      "Iteration 1970, loss = 0.68657325\n",
      "Iteration 1971, loss = 0.68657086\n",
      "Iteration 1972, loss = 0.68656574\n",
      "Iteration 1973, loss = 0.68656466\n",
      "Iteration 1974, loss = 0.68655837\n",
      "Iteration 1975, loss = 0.68655978\n",
      "Iteration 1976, loss = 0.68655478\n",
      "Iteration 1977, loss = 0.68655552\n",
      "Iteration 1978, loss = 0.68655193\n",
      "Iteration 1979, loss = 0.68655915\n",
      "Iteration 1980, loss = 0.68654917\n",
      "Iteration 1981, loss = 0.68654782\n",
      "Iteration 1982, loss = 0.68654900\n",
      "Iteration 1983, loss = 0.68654517\n",
      "Iteration 1984, loss = 0.68653873\n",
      "Iteration 1985, loss = 0.68653775\n",
      "Iteration 1986, loss = 0.68653690\n",
      "Iteration 1987, loss = 0.68653377\n",
      "Iteration 1988, loss = 0.68653085\n",
      "Iteration 1989, loss = 0.68653153\n",
      "Iteration 1990, loss = 0.68652686\n",
      "Iteration 1991, loss = 0.68652504\n",
      "Iteration 1992, loss = 0.68652397\n",
      "Iteration 1993, loss = 0.68652276\n",
      "Iteration 1994, loss = 0.68652114\n",
      "Iteration 1995, loss = 0.68651817\n",
      "Iteration 1996, loss = 0.68651894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1997, loss = 0.68651436\n",
      "Iteration 1998, loss = 0.68651148\n",
      "Iteration 1999, loss = 0.68651057\n",
      "Iteration 2000, loss = 0.68650824\n",
      "Iteration 2001, loss = 0.68650733\n",
      "Iteration 2002, loss = 0.68650412\n",
      "Iteration 2003, loss = 0.68650169\n",
      "Iteration 2004, loss = 0.68650378\n",
      "Iteration 2005, loss = 0.68649943\n",
      "Iteration 2006, loss = 0.68650373\n",
      "Iteration 2007, loss = 0.68649413\n",
      "Iteration 2008, loss = 0.68649464\n",
      "Iteration 2009, loss = 0.68648955\n",
      "Iteration 2010, loss = 0.68648875\n",
      "Iteration 2011, loss = 0.68648502\n",
      "Iteration 2012, loss = 0.68649397\n",
      "Iteration 2013, loss = 0.68648388\n",
      "Iteration 2014, loss = 0.68647975\n",
      "Iteration 2015, loss = 0.68648092\n",
      "Iteration 2016, loss = 0.68647784\n",
      "Iteration 2017, loss = 0.68647875\n",
      "Iteration 2018, loss = 0.68647225\n",
      "Iteration 2019, loss = 0.68647204\n",
      "Iteration 2020, loss = 0.68647685\n",
      "Iteration 2021, loss = 0.68646956\n",
      "Iteration 2022, loss = 0.68646494\n",
      "Iteration 2023, loss = 0.68646321\n",
      "Iteration 2024, loss = 0.68646289\n",
      "Iteration 2025, loss = 0.68646165\n",
      "Iteration 2026, loss = 0.68646422\n",
      "Iteration 2027, loss = 0.68645552\n",
      "Iteration 2028, loss = 0.68645367\n",
      "Iteration 2029, loss = 0.68645531\n",
      "Iteration 2030, loss = 0.68645121\n",
      "Iteration 2031, loss = 0.68644986\n",
      "Iteration 2032, loss = 0.68644657\n",
      "Iteration 2033, loss = 0.68644553\n",
      "Iteration 2034, loss = 0.68644573\n",
      "Iteration 2035, loss = 0.68644062\n",
      "Iteration 2036, loss = 0.68643959\n",
      "Iteration 2037, loss = 0.68643627\n",
      "Iteration 2038, loss = 0.68643615\n",
      "Iteration 2039, loss = 0.68643424\n",
      "Iteration 2040, loss = 0.68643258\n",
      "Iteration 2041, loss = 0.68642974\n",
      "Iteration 2042, loss = 0.68642876\n",
      "Iteration 2043, loss = 0.68642713\n",
      "Iteration 2044, loss = 0.68642485\n",
      "Iteration 2045, loss = 0.68642468\n",
      "Iteration 2046, loss = 0.68642526\n",
      "Iteration 2047, loss = 0.68641878\n",
      "Iteration 2048, loss = 0.68641736\n",
      "Iteration 2049, loss = 0.68641562\n",
      "Iteration 2050, loss = 0.68641438\n",
      "Iteration 2051, loss = 0.68641362\n",
      "Iteration 2052, loss = 0.68641238\n",
      "Iteration 2053, loss = 0.68640956\n",
      "Iteration 2054, loss = 0.68641039\n",
      "Iteration 2055, loss = 0.68640457\n",
      "Iteration 2056, loss = 0.68640613\n",
      "Iteration 2057, loss = 0.68640131\n",
      "Iteration 2058, loss = 0.68639929\n",
      "Iteration 2059, loss = 0.68640663\n",
      "Iteration 2060, loss = 0.68639605\n",
      "Iteration 2061, loss = 0.68639631\n",
      "Iteration 2062, loss = 0.68639412\n",
      "Iteration 2063, loss = 0.68639342\n",
      "Iteration 2064, loss = 0.68639021\n",
      "Iteration 2065, loss = 0.68638790\n",
      "Iteration 2066, loss = 0.68638585\n",
      "Iteration 2067, loss = 0.68639453\n",
      "Iteration 2068, loss = 0.68638262\n",
      "Iteration 2069, loss = 0.68638916\n",
      "Iteration 2070, loss = 0.68637947\n",
      "Iteration 2071, loss = 0.68637761\n",
      "Iteration 2072, loss = 0.68637609\n",
      "Iteration 2073, loss = 0.68637289\n",
      "Iteration 2074, loss = 0.68637206\n",
      "Iteration 2075, loss = 0.68637425\n",
      "Iteration 2076, loss = 0.68636801\n",
      "Iteration 2077, loss = 0.68637259\n",
      "Iteration 2078, loss = 0.68636625\n",
      "Iteration 2079, loss = 0.68636236\n",
      "Iteration 2080, loss = 0.68636367\n",
      "Iteration 2081, loss = 0.68636006\n",
      "Iteration 2082, loss = 0.68635832\n",
      "Iteration 2083, loss = 0.68635581\n",
      "Iteration 2084, loss = 0.68635402\n",
      "Iteration 2085, loss = 0.68635197\n",
      "Iteration 2086, loss = 0.68635107\n",
      "Iteration 2087, loss = 0.68635527\n",
      "Iteration 2088, loss = 0.68634914\n",
      "Iteration 2089, loss = 0.68634739\n",
      "Iteration 2090, loss = 0.68634430\n",
      "Iteration 2091, loss = 0.68634352\n",
      "Iteration 2092, loss = 0.68633964\n",
      "Iteration 2093, loss = 0.68633745\n",
      "Iteration 2094, loss = 0.68634490\n",
      "Iteration 2095, loss = 0.68633411\n",
      "Iteration 2096, loss = 0.68633349\n",
      "Iteration 2097, loss = 0.68633193\n",
      "Iteration 2098, loss = 0.68633648\n",
      "Iteration 2099, loss = 0.68632809\n",
      "Iteration 2100, loss = 0.68633065\n",
      "Iteration 2101, loss = 0.68632870\n",
      "Iteration 2102, loss = 0.68632479\n",
      "Iteration 2103, loss = 0.68632222\n",
      "Iteration 2104, loss = 0.68632023\n",
      "Iteration 2105, loss = 0.68632458\n",
      "Iteration 2106, loss = 0.68631777\n",
      "Iteration 2107, loss = 0.68632014\n",
      "Iteration 2108, loss = 0.68631564\n",
      "Iteration 2109, loss = 0.68631491\n",
      "Iteration 2110, loss = 0.68631646\n",
      "Iteration 2111, loss = 0.68630911\n",
      "Iteration 2112, loss = 0.68630569\n",
      "Iteration 2113, loss = 0.68630688\n",
      "Iteration 2114, loss = 0.68630513\n",
      "Iteration 2115, loss = 0.68630348\n",
      "Iteration 2116, loss = 0.68630142\n",
      "Iteration 2117, loss = 0.68629768\n",
      "Iteration 2118, loss = 0.68629531\n",
      "Iteration 2119, loss = 0.68630228\n",
      "Iteration 2120, loss = 0.68629468\n",
      "Iteration 2121, loss = 0.68629346\n",
      "Iteration 2122, loss = 0.68628879\n",
      "Iteration 2123, loss = 0.68630559\n",
      "Iteration 2124, loss = 0.68628734\n",
      "Iteration 2125, loss = 0.68628644\n",
      "Iteration 2126, loss = 0.68628283\n",
      "Iteration 2127, loss = 0.68628033\n",
      "Iteration 2128, loss = 0.68627911\n",
      "Iteration 2129, loss = 0.68627606\n",
      "Iteration 2130, loss = 0.68627763\n",
      "Iteration 2131, loss = 0.68627375\n",
      "Iteration 2132, loss = 0.68627374\n",
      "Iteration 2133, loss = 0.68627000\n",
      "Iteration 2134, loss = 0.68627155\n",
      "Iteration 2135, loss = 0.68626871\n",
      "Iteration 2136, loss = 0.68626493\n",
      "Iteration 2137, loss = 0.68626517\n",
      "Iteration 2138, loss = 0.68626080\n",
      "Iteration 2139, loss = 0.68625962\n",
      "Iteration 2140, loss = 0.68625696\n",
      "Iteration 2141, loss = 0.68625624\n",
      "Iteration 2142, loss = 0.68625643\n",
      "Iteration 2143, loss = 0.68625378\n",
      "Iteration 2144, loss = 0.68625413\n",
      "Iteration 2145, loss = 0.68625198\n",
      "Iteration 2146, loss = 0.68624626\n",
      "Iteration 2147, loss = 0.68624902\n",
      "Iteration 2148, loss = 0.68624751\n",
      "Iteration 2149, loss = 0.68624434\n",
      "Iteration 2150, loss = 0.68623937\n",
      "Iteration 2151, loss = 0.68623939\n",
      "Iteration 2152, loss = 0.68623724\n",
      "Iteration 2153, loss = 0.68623447\n",
      "Iteration 2154, loss = 0.68623374\n",
      "Iteration 2155, loss = 0.68623560\n",
      "Iteration 2156, loss = 0.68623207\n",
      "Iteration 2157, loss = 0.68622928\n",
      "Iteration 2158, loss = 0.68622595\n",
      "Iteration 2159, loss = 0.68622411\n",
      "Iteration 2160, loss = 0.68622729\n",
      "Iteration 2161, loss = 0.68622693\n",
      "Iteration 2162, loss = 0.68621870\n",
      "Iteration 2163, loss = 0.68621810\n",
      "Iteration 2164, loss = 0.68621600\n",
      "Iteration 2165, loss = 0.68621277\n",
      "Iteration 2166, loss = 0.68621217\n",
      "Iteration 2167, loss = 0.68621078\n",
      "Iteration 2168, loss = 0.68621045\n",
      "Iteration 2169, loss = 0.68620646\n",
      "Iteration 2170, loss = 0.68620956\n",
      "Iteration 2171, loss = 0.68620518\n",
      "Iteration 2172, loss = 0.68620222\n",
      "Iteration 2173, loss = 0.68620091\n",
      "Iteration 2174, loss = 0.68620137\n",
      "Iteration 2175, loss = 0.68619878\n",
      "Iteration 2176, loss = 0.68619725\n",
      "Iteration 2177, loss = 0.68619973\n",
      "Iteration 2178, loss = 0.68619049\n",
      "Iteration 2179, loss = 0.68618978\n",
      "Iteration 2180, loss = 0.68618706\n",
      "Iteration 2181, loss = 0.68618487\n",
      "Iteration 2182, loss = 0.68618499\n",
      "Iteration 2183, loss = 0.68618199\n",
      "Iteration 2184, loss = 0.68618000\n",
      "Iteration 2185, loss = 0.68617786\n",
      "Iteration 2186, loss = 0.68617712\n",
      "Iteration 2187, loss = 0.68617526\n",
      "Iteration 2188, loss = 0.68617584\n",
      "Iteration 2189, loss = 0.68617225\n",
      "Iteration 2190, loss = 0.68617047\n",
      "Iteration 2191, loss = 0.68616662\n",
      "Iteration 2192, loss = 0.68616997\n",
      "Iteration 2193, loss = 0.68616396\n",
      "Iteration 2194, loss = 0.68616248\n",
      "Iteration 2195, loss = 0.68616152\n",
      "Iteration 2196, loss = 0.68616290\n",
      "Iteration 2197, loss = 0.68615921\n",
      "Iteration 2198, loss = 0.68615404\n",
      "Iteration 2199, loss = 0.68615515\n",
      "Iteration 2200, loss = 0.68615387\n",
      "Iteration 2201, loss = 0.68615436\n",
      "Iteration 2202, loss = 0.68614981\n",
      "Iteration 2203, loss = 0.68615091\n",
      "Iteration 2204, loss = 0.68614403\n",
      "Iteration 2205, loss = 0.68614166\n",
      "Iteration 2206, loss = 0.68613887\n",
      "Iteration 2207, loss = 0.68613719\n",
      "Iteration 2208, loss = 0.68613825\n",
      "Iteration 2209, loss = 0.68613645\n",
      "Iteration 2210, loss = 0.68613181\n",
      "Iteration 2211, loss = 0.68613188\n",
      "Iteration 2212, loss = 0.68612970\n",
      "Iteration 2213, loss = 0.68613600\n",
      "Iteration 2214, loss = 0.68612459\n",
      "Iteration 2215, loss = 0.68612486\n",
      "Iteration 2216, loss = 0.68612210\n",
      "Iteration 2217, loss = 0.68612147\n",
      "Iteration 2218, loss = 0.68611935\n",
      "Iteration 2219, loss = 0.68611607\n",
      "Iteration 2220, loss = 0.68611485\n",
      "Iteration 2221, loss = 0.68611357\n",
      "Iteration 2222, loss = 0.68611245\n",
      "Iteration 2223, loss = 0.68611203\n",
      "Iteration 2224, loss = 0.68611061\n",
      "Iteration 2225, loss = 0.68610687\n",
      "Iteration 2226, loss = 0.68610652\n",
      "Iteration 2227, loss = 0.68610573\n",
      "Iteration 2228, loss = 0.68610226\n",
      "Iteration 2229, loss = 0.68610106\n",
      "Iteration 2230, loss = 0.68609728\n",
      "Iteration 2231, loss = 0.68610077\n",
      "Iteration 2232, loss = 0.68609498\n",
      "Iteration 2233, loss = 0.68609426\n",
      "Iteration 2234, loss = 0.68609219\n",
      "Iteration 2235, loss = 0.68609103\n",
      "Iteration 2236, loss = 0.68609000\n",
      "Iteration 2237, loss = 0.68608854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2238, loss = 0.68608533\n",
      "Iteration 2239, loss = 0.68608594\n",
      "Iteration 2240, loss = 0.68608088\n",
      "Iteration 2241, loss = 0.68607968\n",
      "Iteration 2242, loss = 0.68607703\n",
      "Iteration 2243, loss = 0.68607598\n",
      "Iteration 2244, loss = 0.68607561\n",
      "Iteration 2245, loss = 0.68607618\n",
      "Iteration 2246, loss = 0.68607287\n",
      "Iteration 2247, loss = 0.68607003\n",
      "Iteration 2248, loss = 0.68607422\n",
      "Iteration 2249, loss = 0.68606605\n",
      "Iteration 2250, loss = 0.68606468\n",
      "Iteration 2251, loss = 0.68606246\n",
      "Iteration 2252, loss = 0.68606089\n",
      "Iteration 2253, loss = 0.68606281\n",
      "Iteration 2254, loss = 0.68606171\n",
      "Iteration 2255, loss = 0.68605465\n",
      "Iteration 2256, loss = 0.68605250\n",
      "Iteration 2257, loss = 0.68605117\n",
      "Iteration 2258, loss = 0.68605013\n",
      "Iteration 2259, loss = 0.68604772\n",
      "Iteration 2260, loss = 0.68605807\n",
      "Iteration 2261, loss = 0.68604699\n",
      "Iteration 2262, loss = 0.68604600\n",
      "Iteration 2263, loss = 0.68604176\n",
      "Iteration 2264, loss = 0.68604080\n",
      "Iteration 2265, loss = 0.68603834\n",
      "Iteration 2266, loss = 0.68603739\n",
      "Iteration 2267, loss = 0.68603606\n",
      "Iteration 2268, loss = 0.68604022\n",
      "Iteration 2269, loss = 0.68603109\n",
      "Iteration 2270, loss = 0.68603086\n",
      "Iteration 2271, loss = 0.68602861\n",
      "Iteration 2272, loss = 0.68602563\n",
      "Iteration 2273, loss = 0.68602867\n",
      "Iteration 2274, loss = 0.68602260\n",
      "Iteration 2275, loss = 0.68602257\n",
      "Iteration 2276, loss = 0.68602069\n",
      "Iteration 2277, loss = 0.68602209\n",
      "Iteration 2278, loss = 0.68601801\n",
      "Iteration 2279, loss = 0.68601598\n",
      "Iteration 2280, loss = 0.68601523\n",
      "Iteration 2281, loss = 0.68601029\n",
      "Iteration 2282, loss = 0.68601148\n",
      "Iteration 2283, loss = 0.68600720\n",
      "Iteration 2284, loss = 0.68600771\n",
      "Iteration 2285, loss = 0.68600574\n",
      "Iteration 2286, loss = 0.68600503\n",
      "Iteration 2287, loss = 0.68600194\n",
      "Iteration 2288, loss = 0.68600142\n",
      "Iteration 2289, loss = 0.68600247\n",
      "Iteration 2290, loss = 0.68599742\n",
      "Iteration 2291, loss = 0.68599486\n",
      "Iteration 2292, loss = 0.68599206\n",
      "Iteration 2293, loss = 0.68599050\n",
      "Iteration 2294, loss = 0.68598809\n",
      "Iteration 2295, loss = 0.68598757\n",
      "Iteration 2296, loss = 0.68598483\n",
      "Iteration 2297, loss = 0.68598902\n",
      "Iteration 2298, loss = 0.68599012\n",
      "Iteration 2299, loss = 0.68598200\n",
      "Iteration 2300, loss = 0.68598003\n",
      "Iteration 2301, loss = 0.68597637\n",
      "Iteration 2302, loss = 0.68597881\n",
      "Iteration 2303, loss = 0.68597908\n",
      "Iteration 2304, loss = 0.68597588\n",
      "Iteration 2305, loss = 0.68597966\n",
      "Iteration 2306, loss = 0.68597133\n",
      "Iteration 2307, loss = 0.68596680\n",
      "Iteration 2308, loss = 0.68597024\n",
      "Iteration 2309, loss = 0.68596390\n",
      "Iteration 2310, loss = 0.68596144\n",
      "Iteration 2311, loss = 0.68596253\n",
      "Iteration 2312, loss = 0.68596033\n",
      "Iteration 2313, loss = 0.68595668\n",
      "Iteration 2314, loss = 0.68595934\n",
      "Iteration 2315, loss = 0.68595322\n",
      "Iteration 2316, loss = 0.68595372\n",
      "Iteration 2317, loss = 0.68595603\n",
      "Iteration 2318, loss = 0.68594835\n",
      "Iteration 2319, loss = 0.68594885\n",
      "Iteration 2320, loss = 0.68594773\n",
      "Iteration 2321, loss = 0.68594302\n",
      "Iteration 2322, loss = 0.68594338\n",
      "Iteration 2323, loss = 0.68594260\n",
      "Iteration 2324, loss = 0.68593900\n",
      "Iteration 2325, loss = 0.68594349\n",
      "Iteration 2326, loss = 0.68594011\n",
      "Iteration 2327, loss = 0.68593441\n",
      "Iteration 2328, loss = 0.68593506\n",
      "Iteration 2329, loss = 0.68593325\n",
      "Iteration 2330, loss = 0.68592838\n",
      "Iteration 2331, loss = 0.68592728\n",
      "Iteration 2332, loss = 0.68592889\n",
      "Iteration 2333, loss = 0.68592536\n",
      "Iteration 2334, loss = 0.68592260\n",
      "Iteration 2335, loss = 0.68592279\n",
      "Iteration 2336, loss = 0.68592658\n",
      "Iteration 2337, loss = 0.68592234\n",
      "Iteration 2338, loss = 0.68591731\n",
      "Iteration 2339, loss = 0.68591996\n",
      "Iteration 2340, loss = 0.68592241\n",
      "Iteration 2341, loss = 0.68591400\n",
      "Iteration 2342, loss = 0.68591259\n",
      "Iteration 2343, loss = 0.68590807\n",
      "Iteration 2344, loss = 0.68590798\n",
      "Iteration 2345, loss = 0.68590804\n",
      "Iteration 2346, loss = 0.68590558\n",
      "Iteration 2347, loss = 0.68590217\n",
      "Iteration 2348, loss = 0.68589954\n",
      "Iteration 2349, loss = 0.68589786\n",
      "Iteration 2350, loss = 0.68589663\n",
      "Iteration 2351, loss = 0.68589488\n",
      "Iteration 2352, loss = 0.68589757\n",
      "Iteration 2353, loss = 0.68589519\n",
      "Iteration 2354, loss = 0.68588801\n",
      "Iteration 2355, loss = 0.68588582\n",
      "Iteration 2356, loss = 0.68588449\n",
      "Iteration 2357, loss = 0.68588464\n",
      "Iteration 2358, loss = 0.68588340\n",
      "Iteration 2359, loss = 0.68588212\n",
      "Iteration 2360, loss = 0.68587978\n",
      "Iteration 2361, loss = 0.68587830\n",
      "Iteration 2362, loss = 0.68587816\n",
      "Iteration 2363, loss = 0.68587385\n",
      "Iteration 2364, loss = 0.68587629\n",
      "Iteration 2365, loss = 0.68587140\n",
      "Iteration 2366, loss = 0.68587342\n",
      "Iteration 2367, loss = 0.68586929\n",
      "Iteration 2368, loss = 0.68586613\n",
      "Iteration 2369, loss = 0.68586768\n",
      "Iteration 2370, loss = 0.68586556\n",
      "Iteration 2371, loss = 0.68586100\n",
      "Iteration 2372, loss = 0.68585853\n",
      "Iteration 2373, loss = 0.68586009\n",
      "Iteration 2374, loss = 0.68585640\n",
      "Iteration 2375, loss = 0.68585621\n",
      "Iteration 2376, loss = 0.68585227\n",
      "Iteration 2377, loss = 0.68585676\n",
      "Iteration 2378, loss = 0.68584904\n",
      "Iteration 2379, loss = 0.68584861\n",
      "Iteration 2380, loss = 0.68584627\n",
      "Iteration 2381, loss = 0.68584456\n",
      "Iteration 2382, loss = 0.68584315\n",
      "Iteration 2383, loss = 0.68585004\n",
      "Iteration 2384, loss = 0.68583966\n",
      "Iteration 2385, loss = 0.68584114\n",
      "Iteration 2386, loss = 0.68583617\n",
      "Iteration 2387, loss = 0.68584385\n",
      "Iteration 2388, loss = 0.68583443\n",
      "Iteration 2389, loss = 0.68583119\n",
      "Iteration 2390, loss = 0.68583232\n",
      "Iteration 2391, loss = 0.68583476\n",
      "Iteration 2392, loss = 0.68582840\n",
      "Iteration 2393, loss = 0.68582612\n",
      "Iteration 2394, loss = 0.68582323\n",
      "Iteration 2395, loss = 0.68582321\n",
      "Iteration 2396, loss = 0.68582114\n",
      "Iteration 2397, loss = 0.68582552\n",
      "Iteration 2398, loss = 0.68582164\n",
      "Iteration 2399, loss = 0.68581991\n",
      "Iteration 2400, loss = 0.68582005\n",
      "Iteration 2401, loss = 0.68581415\n",
      "Iteration 2402, loss = 0.68581260\n",
      "Iteration 2403, loss = 0.68581129\n",
      "Iteration 2404, loss = 0.68580748\n",
      "Iteration 2405, loss = 0.68580779\n",
      "Iteration 2406, loss = 0.68580441\n",
      "Iteration 2407, loss = 0.68580311\n",
      "Iteration 2408, loss = 0.68580269\n",
      "Iteration 2409, loss = 0.68580250\n",
      "Iteration 2410, loss = 0.68579861\n",
      "Iteration 2411, loss = 0.68579721\n",
      "Iteration 2412, loss = 0.68580028\n",
      "Iteration 2413, loss = 0.68579304\n",
      "Iteration 2414, loss = 0.68579461\n",
      "Iteration 2415, loss = 0.68579102\n",
      "Iteration 2416, loss = 0.68579208\n",
      "Iteration 2417, loss = 0.68578782\n",
      "Iteration 2418, loss = 0.68578534\n",
      "Iteration 2419, loss = 0.68578504\n",
      "Iteration 2420, loss = 0.68579254\n",
      "Iteration 2421, loss = 0.68578161\n",
      "Iteration 2422, loss = 0.68578064\n",
      "Iteration 2423, loss = 0.68577853\n",
      "Iteration 2424, loss = 0.68577828\n",
      "Iteration 2425, loss = 0.68577563\n",
      "Iteration 2426, loss = 0.68577894\n",
      "Iteration 2427, loss = 0.68577139\n",
      "Iteration 2428, loss = 0.68577635\n",
      "Iteration 2429, loss = 0.68577087\n",
      "Iteration 2430, loss = 0.68576931\n",
      "Iteration 2431, loss = 0.68576609\n",
      "Iteration 2432, loss = 0.68577282\n",
      "Iteration 2433, loss = 0.68576468\n",
      "Iteration 2434, loss = 0.68576085\n",
      "Iteration 2435, loss = 0.68576007\n",
      "Iteration 2436, loss = 0.68575903\n",
      "Iteration 2437, loss = 0.68575916\n",
      "Iteration 2438, loss = 0.68575839\n",
      "Iteration 2439, loss = 0.68575449\n",
      "Iteration 2440, loss = 0.68575728\n",
      "Iteration 2441, loss = 0.68575445\n",
      "Iteration 2442, loss = 0.68574917\n",
      "Iteration 2443, loss = 0.68574929\n",
      "Iteration 2444, loss = 0.68575263\n",
      "Iteration 2445, loss = 0.68574945\n",
      "Iteration 2446, loss = 0.68574210\n",
      "Iteration 2447, loss = 0.68574399\n",
      "Iteration 2448, loss = 0.68574407\n",
      "Iteration 2449, loss = 0.68573938\n",
      "Iteration 2450, loss = 0.68573545\n",
      "Iteration 2451, loss = 0.68573677\n",
      "Iteration 2452, loss = 0.68573239\n",
      "Iteration 2453, loss = 0.68573097\n",
      "Iteration 2454, loss = 0.68573047\n",
      "Iteration 2455, loss = 0.68573393\n",
      "Iteration 2456, loss = 0.68573163\n",
      "Iteration 2457, loss = 0.68572625\n",
      "Iteration 2458, loss = 0.68572253\n",
      "Iteration 2459, loss = 0.68572121\n",
      "Iteration 2460, loss = 0.68572054\n",
      "Iteration 2461, loss = 0.68571859\n",
      "Iteration 2462, loss = 0.68572032\n",
      "Iteration 2463, loss = 0.68571576\n",
      "Iteration 2464, loss = 0.68571516\n",
      "Iteration 2465, loss = 0.68571345\n",
      "Iteration 2466, loss = 0.68571067\n",
      "Iteration 2467, loss = 0.68571114\n",
      "Iteration 2468, loss = 0.68570837\n",
      "Iteration 2469, loss = 0.68570610\n",
      "Iteration 2470, loss = 0.68570696\n",
      "Iteration 2471, loss = 0.68570597\n",
      "Iteration 2472, loss = 0.68570144\n",
      "Iteration 2473, loss = 0.68570605\n",
      "Iteration 2474, loss = 0.68570061\n",
      "Iteration 2475, loss = 0.68569760\n",
      "Iteration 2476, loss = 0.68570121\n",
      "Iteration 2477, loss = 0.68569629\n",
      "Iteration 2478, loss = 0.68569875\n",
      "Iteration 2479, loss = 0.68569136\n",
      "Iteration 2480, loss = 0.68569121\n",
      "Iteration 2481, loss = 0.68568936\n",
      "Iteration 2482, loss = 0.68568629\n",
      "Iteration 2483, loss = 0.68568655\n",
      "Iteration 2484, loss = 0.68569722\n",
      "Iteration 2485, loss = 0.68568542\n",
      "Iteration 2486, loss = 0.68568115\n",
      "Iteration 2487, loss = 0.68567856\n",
      "Iteration 2488, loss = 0.68568018\n",
      "Iteration 2489, loss = 0.68567666\n",
      "Iteration 2490, loss = 0.68567569\n",
      "Iteration 2491, loss = 0.68567293\n",
      "Iteration 2492, loss = 0.68567532\n",
      "Iteration 2493, loss = 0.68567546\n",
      "Iteration 2494, loss = 0.68567259\n",
      "Iteration 2495, loss = 0.68566905\n",
      "Iteration 2496, loss = 0.68567243\n",
      "Iteration 2497, loss = 0.68566631\n",
      "Iteration 2498, loss = 0.68566377\n",
      "Iteration 2499, loss = 0.68566696\n",
      "Iteration 2500, loss = 0.68566541\n",
      "Iteration 2501, loss = 0.68566032\n",
      "Iteration 2502, loss = 0.68566080\n",
      "Iteration 2503, loss = 0.68565628\n",
      "Iteration 2504, loss = 0.68565566\n",
      "Iteration 2505, loss = 0.68565313\n",
      "Iteration 2506, loss = 0.68565247\n",
      "Iteration 2507, loss = 0.68565358\n",
      "Iteration 2508, loss = 0.68564766\n",
      "Iteration 2509, loss = 0.68564927\n",
      "Iteration 2510, loss = 0.68565156\n",
      "Iteration 2511, loss = 0.68564799\n",
      "Iteration 2512, loss = 0.68564565\n",
      "Iteration 2513, loss = 0.68564060\n",
      "Iteration 2514, loss = 0.68564351\n",
      "Iteration 2515, loss = 0.68564258\n",
      "Iteration 2516, loss = 0.68563930\n",
      "Iteration 2517, loss = 0.68563522\n",
      "Iteration 2518, loss = 0.68563346\n",
      "Iteration 2519, loss = 0.68563413\n",
      "Iteration 2520, loss = 0.68563237\n",
      "Iteration 2521, loss = 0.68563119\n",
      "Iteration 2522, loss = 0.68562899\n",
      "Iteration 2523, loss = 0.68562626\n",
      "Iteration 2524, loss = 0.68562430\n",
      "Iteration 2525, loss = 0.68562274\n",
      "Iteration 2526, loss = 0.68562396\n",
      "Iteration 2527, loss = 0.68562311\n",
      "Iteration 2528, loss = 0.68561887\n",
      "Iteration 2529, loss = 0.68562101\n",
      "Iteration 2530, loss = 0.68561786\n",
      "Iteration 2531, loss = 0.68562555\n",
      "Iteration 2532, loss = 0.68561382\n",
      "Iteration 2533, loss = 0.68561207\n",
      "Iteration 2534, loss = 0.68561251\n",
      "Iteration 2535, loss = 0.68560750\n",
      "Iteration 2536, loss = 0.68561038\n",
      "Iteration 2537, loss = 0.68560486\n",
      "Iteration 2538, loss = 0.68560410\n",
      "Iteration 2539, loss = 0.68560069\n",
      "Iteration 2540, loss = 0.68560321\n",
      "Iteration 2541, loss = 0.68560881\n",
      "Iteration 2542, loss = 0.68559819\n",
      "Iteration 2543, loss = 0.68559546\n",
      "Iteration 2544, loss = 0.68559799\n",
      "Iteration 2545, loss = 0.68559835\n",
      "Iteration 2546, loss = 0.68559195\n",
      "Iteration 2547, loss = 0.68559099\n",
      "Iteration 2548, loss = 0.68559558\n",
      "Iteration 2549, loss = 0.68559470\n",
      "Iteration 2550, loss = 0.68558610\n",
      "Iteration 2551, loss = 0.68558484\n",
      "Iteration 2552, loss = 0.68558375\n",
      "Iteration 2553, loss = 0.68558050\n",
      "Iteration 2554, loss = 0.68558025\n",
      "Iteration 2555, loss = 0.68557690\n",
      "Iteration 2556, loss = 0.68557653\n",
      "Iteration 2557, loss = 0.68557710\n",
      "Iteration 2558, loss = 0.68557480\n",
      "Iteration 2559, loss = 0.68557377\n",
      "Iteration 2560, loss = 0.68557124\n",
      "Iteration 2561, loss = 0.68557502\n",
      "Iteration 2562, loss = 0.68556805\n",
      "Iteration 2563, loss = 0.68556647\n",
      "Iteration 2564, loss = 0.68556487\n",
      "Iteration 2565, loss = 0.68557127\n",
      "Iteration 2566, loss = 0.68556376\n",
      "Iteration 2567, loss = 0.68556540\n",
      "Iteration 2568, loss = 0.68555770\n",
      "Iteration 2569, loss = 0.68555974\n",
      "Iteration 2570, loss = 0.68555634\n",
      "Iteration 2571, loss = 0.68555295\n",
      "Iteration 2572, loss = 0.68555315\n",
      "Iteration 2573, loss = 0.68555156\n",
      "Iteration 2574, loss = 0.68554975\n",
      "Iteration 2575, loss = 0.68554997\n",
      "Iteration 2576, loss = 0.68554814\n",
      "Iteration 2577, loss = 0.68554651\n",
      "Iteration 2578, loss = 0.68554633\n",
      "Iteration 2579, loss = 0.68554355\n",
      "Iteration 2580, loss = 0.68554269\n",
      "Iteration 2581, loss = 0.68553930\n",
      "Iteration 2582, loss = 0.68554455\n",
      "Iteration 2583, loss = 0.68553967\n",
      "Iteration 2584, loss = 0.68553497\n",
      "Iteration 2585, loss = 0.68553397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2586, loss = 0.68553350\n",
      "Iteration 2587, loss = 0.68553477\n",
      "Iteration 2588, loss = 0.68553674\n",
      "Iteration 2589, loss = 0.68552894\n",
      "Iteration 2590, loss = 0.68552879\n",
      "Iteration 2591, loss = 0.68552416\n",
      "Iteration 2592, loss = 0.68552617\n",
      "Iteration 2593, loss = 0.68552751\n",
      "Iteration 2594, loss = 0.68552172\n",
      "Iteration 2595, loss = 0.68552001\n",
      "Iteration 2596, loss = 0.68551912\n",
      "Iteration 2597, loss = 0.68552056\n",
      "Iteration 2598, loss = 0.68551647\n",
      "Iteration 2599, loss = 0.68551565\n",
      "Iteration 2600, loss = 0.68551229\n",
      "Iteration 2601, loss = 0.68551393\n",
      "Iteration 2602, loss = 0.68551264\n",
      "Iteration 2603, loss = 0.68550918\n",
      "Iteration 2604, loss = 0.68550885\n",
      "Iteration 2605, loss = 0.68550516\n",
      "Iteration 2606, loss = 0.68551141\n",
      "Iteration 2607, loss = 0.68550308\n",
      "Iteration 2608, loss = 0.68550083\n",
      "Iteration 2609, loss = 0.68550155\n",
      "Iteration 2610, loss = 0.68549842\n",
      "Iteration 2611, loss = 0.68549847\n",
      "Iteration 2612, loss = 0.68549724\n",
      "Iteration 2613, loss = 0.68549547\n",
      "Iteration 2614, loss = 0.68549335\n",
      "Iteration 2615, loss = 0.68549261\n",
      "Iteration 2616, loss = 0.68549026\n",
      "Iteration 2617, loss = 0.68549008\n",
      "Iteration 2618, loss = 0.68549100\n",
      "Iteration 2619, loss = 0.68548650\n",
      "Iteration 2620, loss = 0.68549082\n",
      "Iteration 2621, loss = 0.68548382\n",
      "Iteration 2622, loss = 0.68548383\n",
      "Iteration 2623, loss = 0.68548454\n",
      "Iteration 2624, loss = 0.68547906\n",
      "Iteration 2625, loss = 0.68548303\n",
      "Iteration 2626, loss = 0.68547476\n",
      "Iteration 2627, loss = 0.68547481\n",
      "Iteration 2628, loss = 0.68547413\n",
      "Iteration 2629, loss = 0.68547287\n",
      "Iteration 2630, loss = 0.68547216\n",
      "Iteration 2631, loss = 0.68546915\n",
      "Iteration 2632, loss = 0.68546789\n",
      "Iteration 2633, loss = 0.68546847\n",
      "Iteration 2634, loss = 0.68546971\n",
      "Iteration 2635, loss = 0.68547146\n",
      "Iteration 2636, loss = 0.68546580\n",
      "Iteration 2637, loss = 0.68546695\n",
      "Iteration 2638, loss = 0.68546749\n",
      "Iteration 2639, loss = 0.68545724\n",
      "Iteration 2640, loss = 0.68546088\n",
      "Iteration 2641, loss = 0.68545756\n",
      "Iteration 2642, loss = 0.68545650\n",
      "Iteration 2643, loss = 0.68545343\n",
      "Iteration 2644, loss = 0.68545310\n",
      "Iteration 2645, loss = 0.68545020\n",
      "Iteration 2646, loss = 0.68546058\n",
      "Iteration 2647, loss = 0.68544764\n",
      "Iteration 2648, loss = 0.68545126\n",
      "Iteration 2649, loss = 0.68544885\n",
      "Iteration 2650, loss = 0.68545075\n",
      "Iteration 2651, loss = 0.68544887\n",
      "Iteration 2652, loss = 0.68544131\n",
      "Iteration 2653, loss = 0.68543900\n",
      "Iteration 2654, loss = 0.68543910\n",
      "Iteration 2655, loss = 0.68544242\n",
      "Iteration 2656, loss = 0.68543687\n",
      "Iteration 2657, loss = 0.68543787\n",
      "Iteration 2658, loss = 0.68543400\n",
      "Iteration 2659, loss = 0.68543118\n",
      "Iteration 2660, loss = 0.68542906\n",
      "Iteration 2661, loss = 0.68542892\n",
      "Iteration 2662, loss = 0.68542875\n",
      "Iteration 2663, loss = 0.68542849\n",
      "Iteration 2664, loss = 0.68542625\n",
      "Iteration 2665, loss = 0.68542354\n",
      "Iteration 2666, loss = 0.68542451\n",
      "Iteration 2667, loss = 0.68541932\n",
      "Iteration 2668, loss = 0.68541991\n",
      "Iteration 2669, loss = 0.68542153\n",
      "Iteration 2670, loss = 0.68541557\n",
      "Iteration 2671, loss = 0.68541725\n",
      "Iteration 2672, loss = 0.68541526\n",
      "Iteration 2673, loss = 0.68541288\n",
      "Iteration 2674, loss = 0.68541672\n",
      "Iteration 2675, loss = 0.68541804\n",
      "Iteration 2676, loss = 0.68541291\n",
      "Iteration 2677, loss = 0.68540885\n",
      "Iteration 2678, loss = 0.68541164\n",
      "Iteration 2679, loss = 0.68540604\n",
      "Iteration 2680, loss = 0.68540352\n",
      "Iteration 2681, loss = 0.68540340\n",
      "Iteration 2682, loss = 0.68539969\n",
      "Iteration 2683, loss = 0.68539781\n",
      "Iteration 2684, loss = 0.68539931\n",
      "Iteration 2685, loss = 0.68539679\n",
      "Iteration 2686, loss = 0.68539525\n",
      "Iteration 2687, loss = 0.68539423\n",
      "Iteration 2688, loss = 0.68539615\n",
      "Iteration 2689, loss = 0.68539250\n",
      "Iteration 2690, loss = 0.68539157\n",
      "Iteration 2691, loss = 0.68539776\n",
      "Iteration 2692, loss = 0.68538661\n",
      "Iteration 2693, loss = 0.68538496\n",
      "Iteration 2694, loss = 0.68538329\n",
      "Iteration 2695, loss = 0.68538306\n",
      "Iteration 2696, loss = 0.68538026\n",
      "Iteration 2697, loss = 0.68538130\n",
      "Iteration 2698, loss = 0.68538376\n",
      "Iteration 2699, loss = 0.68537691\n",
      "Iteration 2700, loss = 0.68537594\n",
      "Iteration 2701, loss = 0.68537512\n",
      "Iteration 2702, loss = 0.68537462\n",
      "Iteration 2703, loss = 0.68537104\n",
      "Iteration 2704, loss = 0.68536925\n",
      "Iteration 2705, loss = 0.68536948\n",
      "Iteration 2706, loss = 0.68536825\n",
      "Iteration 2707, loss = 0.68536917\n",
      "Iteration 2708, loss = 0.68536550\n",
      "Iteration 2709, loss = 0.68536312\n",
      "Iteration 2710, loss = 0.68536895\n",
      "Iteration 2711, loss = 0.68536244\n",
      "Iteration 2712, loss = 0.68536054\n",
      "Iteration 2713, loss = 0.68535928\n",
      "Iteration 2714, loss = 0.68535927\n",
      "Iteration 2715, loss = 0.68535628\n",
      "Iteration 2716, loss = 0.68535648\n",
      "Iteration 2717, loss = 0.68535229\n",
      "Iteration 2718, loss = 0.68535102\n",
      "Iteration 2719, loss = 0.68534971\n",
      "Iteration 2720, loss = 0.68534796\n",
      "Iteration 2721, loss = 0.68534892\n",
      "Iteration 2722, loss = 0.68534649\n",
      "Iteration 2723, loss = 0.68534537\n",
      "Iteration 2724, loss = 0.68534555\n",
      "Iteration 2725, loss = 0.68534083\n",
      "Iteration 2726, loss = 0.68534284\n",
      "Iteration 2727, loss = 0.68534405\n",
      "Iteration 2728, loss = 0.68533590\n",
      "Iteration 2729, loss = 0.68533772\n",
      "Iteration 2730, loss = 0.68533343\n",
      "Iteration 2731, loss = 0.68533211\n",
      "Iteration 2732, loss = 0.68533146\n",
      "Iteration 2733, loss = 0.68532969\n",
      "Iteration 2734, loss = 0.68533055\n",
      "Iteration 2735, loss = 0.68532792\n",
      "Iteration 2736, loss = 0.68532755\n",
      "Iteration 2737, loss = 0.68532432\n",
      "Iteration 2738, loss = 0.68532425\n",
      "Iteration 2739, loss = 0.68532169\n",
      "Iteration 2740, loss = 0.68532214\n",
      "Iteration 2741, loss = 0.68532054\n",
      "Iteration 2742, loss = 0.68531625\n",
      "Iteration 2743, loss = 0.68532610\n",
      "Iteration 2744, loss = 0.68531436\n",
      "Iteration 2745, loss = 0.68531606\n",
      "Iteration 2746, loss = 0.68531067\n",
      "Iteration 2747, loss = 0.68531364\n",
      "Iteration 2748, loss = 0.68530795\n",
      "Iteration 2749, loss = 0.68531053\n",
      "Iteration 2750, loss = 0.68530884\n",
      "Iteration 2751, loss = 0.68530952\n",
      "Iteration 2752, loss = 0.68530238\n",
      "Iteration 2753, loss = 0.68530344\n",
      "Iteration 2754, loss = 0.68530864\n",
      "Iteration 2755, loss = 0.68529970\n",
      "Iteration 2756, loss = 0.68529727\n",
      "Iteration 2757, loss = 0.68529530\n",
      "Iteration 2758, loss = 0.68529754\n",
      "Iteration 2759, loss = 0.68529199\n",
      "Iteration 2760, loss = 0.68530005\n",
      "Iteration 2761, loss = 0.68529071\n",
      "Iteration 2762, loss = 0.68528910\n",
      "Iteration 2763, loss = 0.68528646\n",
      "Iteration 2764, loss = 0.68528503\n",
      "Iteration 2765, loss = 0.68528317\n",
      "Iteration 2766, loss = 0.68528684\n",
      "Iteration 2767, loss = 0.68528010\n",
      "Iteration 2768, loss = 0.68528100\n",
      "Iteration 2769, loss = 0.68527752\n",
      "Iteration 2770, loss = 0.68527866\n",
      "Iteration 2771, loss = 0.68527838\n",
      "Iteration 2772, loss = 0.68527360\n",
      "Iteration 2773, loss = 0.68527298\n",
      "Iteration 2774, loss = 0.68527155\n",
      "Iteration 2775, loss = 0.68527425\n",
      "Iteration 2776, loss = 0.68526979\n",
      "Iteration 2777, loss = 0.68526518\n",
      "Iteration 2778, loss = 0.68526652\n",
      "Iteration 2779, loss = 0.68526453\n",
      "Iteration 2780, loss = 0.68526114\n",
      "Iteration 2781, loss = 0.68526209\n",
      "Iteration 2782, loss = 0.68525968\n",
      "Iteration 2783, loss = 0.68525715\n",
      "Iteration 2784, loss = 0.68525515\n",
      "Iteration 2785, loss = 0.68525431\n",
      "Iteration 2786, loss = 0.68525565\n",
      "Iteration 2787, loss = 0.68525116\n",
      "Iteration 2788, loss = 0.68525097\n",
      "Iteration 2789, loss = 0.68525073\n",
      "Iteration 2790, loss = 0.68524936\n",
      "Iteration 2791, loss = 0.68525242\n",
      "Iteration 2792, loss = 0.68524450\n",
      "Iteration 2793, loss = 0.68524371\n",
      "Iteration 2794, loss = 0.68524219\n",
      "Iteration 2795, loss = 0.68524248\n",
      "Iteration 2796, loss = 0.68524062\n",
      "Iteration 2797, loss = 0.68523626\n",
      "Iteration 2798, loss = 0.68523617\n",
      "Iteration 2799, loss = 0.68523484\n",
      "Iteration 2800, loss = 0.68523207\n",
      "Iteration 2801, loss = 0.68523449\n",
      "Iteration 2802, loss = 0.68523013\n",
      "Iteration 2803, loss = 0.68522971\n",
      "Iteration 2804, loss = 0.68522627\n",
      "Iteration 2805, loss = 0.68522757\n",
      "Iteration 2806, loss = 0.68522489\n",
      "Iteration 2807, loss = 0.68522358\n",
      "Iteration 2808, loss = 0.68521999\n",
      "Iteration 2809, loss = 0.68522748\n",
      "Iteration 2810, loss = 0.68522263\n",
      "Iteration 2811, loss = 0.68522124\n",
      "Iteration 2812, loss = 0.68521721\n",
      "Iteration 2813, loss = 0.68521722\n",
      "Iteration 2814, loss = 0.68521505\n",
      "Iteration 2815, loss = 0.68521214\n",
      "Iteration 2816, loss = 0.68521005\n",
      "Iteration 2817, loss = 0.68521071\n",
      "Iteration 2818, loss = 0.68520726\n",
      "Iteration 2819, loss = 0.68520461\n",
      "Iteration 2820, loss = 0.68520493\n",
      "Iteration 2821, loss = 0.68520202\n",
      "Iteration 2822, loss = 0.68520597\n",
      "Iteration 2823, loss = 0.68519851\n",
      "Iteration 2824, loss = 0.68520218\n",
      "Iteration 2825, loss = 0.68519524\n",
      "Iteration 2826, loss = 0.68519520\n",
      "Iteration 2827, loss = 0.68519348\n",
      "Iteration 2828, loss = 0.68519300\n",
      "Iteration 2829, loss = 0.68519070\n",
      "Iteration 2830, loss = 0.68519002\n",
      "Iteration 2831, loss = 0.68518794\n",
      "Iteration 2832, loss = 0.68518812\n",
      "Iteration 2833, loss = 0.68518589\n",
      "Iteration 2834, loss = 0.68518246\n",
      "Iteration 2835, loss = 0.68518192\n",
      "Iteration 2836, loss = 0.68518243\n",
      "Iteration 2837, loss = 0.68517923\n",
      "Iteration 2838, loss = 0.68517644\n",
      "Iteration 2839, loss = 0.68518170\n",
      "Iteration 2840, loss = 0.68517710\n",
      "Iteration 2841, loss = 0.68517533\n",
      "Iteration 2842, loss = 0.68517413\n",
      "Iteration 2843, loss = 0.68517112\n",
      "Iteration 2844, loss = 0.68516904\n",
      "Iteration 2845, loss = 0.68517105\n",
      "Iteration 2846, loss = 0.68516710\n",
      "Iteration 2847, loss = 0.68516385\n",
      "Iteration 2848, loss = 0.68516378\n",
      "Iteration 2849, loss = 0.68516609\n",
      "Iteration 2850, loss = 0.68515961\n",
      "Iteration 2851, loss = 0.68516372\n",
      "Iteration 2852, loss = 0.68515535\n",
      "Iteration 2853, loss = 0.68515537\n",
      "Iteration 2854, loss = 0.68515225\n",
      "Iteration 2855, loss = 0.68515034\n",
      "Iteration 2856, loss = 0.68515168\n",
      "Iteration 2857, loss = 0.68514916\n",
      "Iteration 2858, loss = 0.68514469\n",
      "Iteration 2859, loss = 0.68514442\n",
      "Iteration 2860, loss = 0.68514075\n",
      "Iteration 2861, loss = 0.68514063\n",
      "Iteration 2862, loss = 0.68513956\n",
      "Iteration 2863, loss = 0.68513909\n",
      "Iteration 2864, loss = 0.68513485\n",
      "Iteration 2865, loss = 0.68513331\n",
      "Iteration 2866, loss = 0.68513398\n",
      "Iteration 2867, loss = 0.68513172\n",
      "Iteration 2868, loss = 0.68513040\n",
      "Iteration 2869, loss = 0.68513055\n",
      "Iteration 2870, loss = 0.68512721\n",
      "Iteration 2871, loss = 0.68512761\n",
      "Iteration 2872, loss = 0.68512725\n",
      "Iteration 2873, loss = 0.68511842\n",
      "Iteration 2874, loss = 0.68511820\n",
      "Iteration 2875, loss = 0.68511596\n",
      "Iteration 2876, loss = 0.68512442\n",
      "Iteration 2877, loss = 0.68511594\n",
      "Iteration 2878, loss = 0.68511044\n",
      "Iteration 2879, loss = 0.68510995\n",
      "Iteration 2880, loss = 0.68511150\n",
      "Iteration 2881, loss = 0.68510738\n",
      "Iteration 2882, loss = 0.68510795\n",
      "Iteration 2883, loss = 0.68510486\n",
      "Iteration 2884, loss = 0.68510296\n",
      "Iteration 2885, loss = 0.68511080\n",
      "Iteration 2886, loss = 0.68510156\n",
      "Iteration 2887, loss = 0.68509967\n",
      "Iteration 2888, loss = 0.68509805\n",
      "Iteration 2889, loss = 0.68509939\n",
      "Iteration 2890, loss = 0.68509496\n",
      "Iteration 2891, loss = 0.68509470\n",
      "Iteration 2892, loss = 0.68509500\n",
      "Iteration 2893, loss = 0.68508779\n",
      "Iteration 2894, loss = 0.68508781\n",
      "Iteration 2895, loss = 0.68509308\n",
      "Iteration 2896, loss = 0.68508614\n",
      "Iteration 2897, loss = 0.68508526\n",
      "Iteration 2898, loss = 0.68507882\n",
      "Iteration 2899, loss = 0.68507700\n",
      "Iteration 2900, loss = 0.68507738\n",
      "Iteration 2901, loss = 0.68507674\n",
      "Iteration 2902, loss = 0.68507730\n",
      "Iteration 2903, loss = 0.68507106\n",
      "Iteration 2904, loss = 0.68507420\n",
      "Iteration 2905, loss = 0.68507201\n",
      "Iteration 2906, loss = 0.68506791\n",
      "Iteration 2907, loss = 0.68506533\n",
      "Iteration 2908, loss = 0.68506585\n",
      "Iteration 2909, loss = 0.68506171\n",
      "Iteration 2910, loss = 0.68505859\n",
      "Iteration 2911, loss = 0.68506256\n",
      "Iteration 2912, loss = 0.68505712\n",
      "Iteration 2913, loss = 0.68505452\n",
      "Iteration 2914, loss = 0.68505371\n",
      "Iteration 2915, loss = 0.68505394\n",
      "Iteration 2916, loss = 0.68505109\n",
      "Iteration 2917, loss = 0.68504749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2918, loss = 0.68505224\n",
      "Iteration 2919, loss = 0.68504724\n",
      "Iteration 2920, loss = 0.68504424\n",
      "Iteration 2921, loss = 0.68504078\n",
      "Iteration 2922, loss = 0.68504056\n",
      "Iteration 2923, loss = 0.68504554\n",
      "Iteration 2924, loss = 0.68503647\n",
      "Iteration 2925, loss = 0.68503667\n",
      "Iteration 2926, loss = 0.68503722\n",
      "Iteration 2927, loss = 0.68503297\n",
      "Iteration 2928, loss = 0.68503377\n",
      "Iteration 2929, loss = 0.68503665\n",
      "Iteration 2930, loss = 0.68502634\n",
      "Iteration 2931, loss = 0.68502535\n",
      "Iteration 2932, loss = 0.68502312\n",
      "Iteration 2933, loss = 0.68502206\n",
      "Iteration 2934, loss = 0.68502078\n",
      "Iteration 2935, loss = 0.68502018\n",
      "Iteration 2936, loss = 0.68501723\n",
      "Iteration 2937, loss = 0.68501568\n",
      "Iteration 2938, loss = 0.68501604\n",
      "Iteration 2939, loss = 0.68501428\n",
      "Iteration 2940, loss = 0.68501190\n",
      "Iteration 2941, loss = 0.68500953\n",
      "Iteration 2942, loss = 0.68500809\n",
      "Iteration 2943, loss = 0.68500609\n",
      "Iteration 2944, loss = 0.68500552\n",
      "Iteration 2945, loss = 0.68501002\n",
      "Iteration 2946, loss = 0.68500395\n",
      "Iteration 2947, loss = 0.68500108\n",
      "Iteration 2948, loss = 0.68499681\n",
      "Iteration 2949, loss = 0.68500253\n",
      "Iteration 2950, loss = 0.68499364\n",
      "Iteration 2951, loss = 0.68499097\n",
      "Iteration 2952, loss = 0.68498922\n",
      "Iteration 2953, loss = 0.68499045\n",
      "Iteration 2954, loss = 0.68498810\n",
      "Iteration 2955, loss = 0.68498568\n",
      "Iteration 2956, loss = 0.68498605\n",
      "Iteration 2957, loss = 0.68498143\n",
      "Iteration 2958, loss = 0.68498836\n",
      "Iteration 2959, loss = 0.68498910\n",
      "Iteration 2960, loss = 0.68498352\n",
      "Iteration 2961, loss = 0.68498008\n",
      "Iteration 2962, loss = 0.68497684\n",
      "Iteration 2963, loss = 0.68497147\n",
      "Iteration 2964, loss = 0.68497076\n",
      "Iteration 2965, loss = 0.68496941\n",
      "Iteration 2966, loss = 0.68496733\n",
      "Iteration 2967, loss = 0.68496614\n",
      "Iteration 2968, loss = 0.68496269\n",
      "Iteration 2969, loss = 0.68496277\n",
      "Iteration 2970, loss = 0.68496144\n",
      "Iteration 2971, loss = 0.68495828\n",
      "Iteration 2972, loss = 0.68495995\n",
      "Iteration 2973, loss = 0.68496216\n",
      "Iteration 2974, loss = 0.68495348\n",
      "Iteration 2975, loss = 0.68494963\n",
      "Iteration 2976, loss = 0.68495298\n",
      "Iteration 2977, loss = 0.68494847\n",
      "Iteration 2978, loss = 0.68494552\n",
      "Iteration 2979, loss = 0.68494290\n",
      "Iteration 2980, loss = 0.68494220\n",
      "Iteration 2981, loss = 0.68494029\n",
      "Iteration 2982, loss = 0.68493802\n",
      "Iteration 2983, loss = 0.68493783\n",
      "Iteration 2984, loss = 0.68493454\n",
      "Iteration 2985, loss = 0.68493198\n",
      "Iteration 2986, loss = 0.68493054\n",
      "Iteration 2987, loss = 0.68492990\n",
      "Iteration 2988, loss = 0.68493112\n",
      "Iteration 2989, loss = 0.68492505\n",
      "Iteration 2990, loss = 0.68492662\n",
      "Iteration 2991, loss = 0.68492444\n",
      "Iteration 2992, loss = 0.68491878\n",
      "Iteration 2993, loss = 0.68492172\n",
      "Iteration 2994, loss = 0.68491869\n",
      "Iteration 2995, loss = 0.68491385\n",
      "Iteration 2996, loss = 0.68491434\n",
      "Iteration 2997, loss = 0.68491224\n",
      "Iteration 2998, loss = 0.68491217\n",
      "Iteration 2999, loss = 0.68490697\n",
      "Iteration 3000, loss = 0.68490848\n",
      "Iteration 3001, loss = 0.68490280\n",
      "Iteration 3002, loss = 0.68490564\n",
      "Iteration 3003, loss = 0.68490017\n",
      "Iteration 3004, loss = 0.68489685\n",
      "Iteration 3005, loss = 0.68490033\n",
      "Iteration 3006, loss = 0.68489203\n",
      "Iteration 3007, loss = 0.68489787\n",
      "Iteration 3008, loss = 0.68488971\n",
      "Iteration 3009, loss = 0.68488818\n",
      "Iteration 3010, loss = 0.68488816\n",
      "Iteration 3011, loss = 0.68488401\n",
      "Iteration 3012, loss = 0.68488342\n",
      "Iteration 3013, loss = 0.68489024\n",
      "Iteration 3014, loss = 0.68488777\n",
      "Iteration 3015, loss = 0.68488180\n",
      "Iteration 3016, loss = 0.68487894\n",
      "Iteration 3017, loss = 0.68487260\n",
      "Iteration 3018, loss = 0.68487394\n",
      "Iteration 3019, loss = 0.68487205\n",
      "Iteration 3020, loss = 0.68487124\n",
      "Iteration 3021, loss = 0.68486550\n",
      "Iteration 3022, loss = 0.68486683\n",
      "Iteration 3023, loss = 0.68486178\n",
      "Iteration 3024, loss = 0.68485938\n",
      "Iteration 3025, loss = 0.68486193\n",
      "Iteration 3026, loss = 0.68485592\n",
      "Iteration 3027, loss = 0.68485403\n",
      "Iteration 3028, loss = 0.68485382\n",
      "Iteration 3029, loss = 0.68485526\n",
      "Iteration 3030, loss = 0.68484938\n",
      "Iteration 3031, loss = 0.68484875\n",
      "Iteration 3032, loss = 0.68484443\n",
      "Iteration 3033, loss = 0.68484882\n",
      "Iteration 3034, loss = 0.68484033\n",
      "Iteration 3035, loss = 0.68484015\n",
      "Iteration 3036, loss = 0.68483678\n",
      "Iteration 3037, loss = 0.68483614\n",
      "Iteration 3038, loss = 0.68483754\n",
      "Iteration 3039, loss = 0.68483103\n",
      "Iteration 3040, loss = 0.68482925\n",
      "Iteration 3041, loss = 0.68482748\n",
      "Iteration 3042, loss = 0.68482629\n",
      "Iteration 3043, loss = 0.68482512\n",
      "Iteration 3044, loss = 0.68482529\n",
      "Iteration 3045, loss = 0.68482302\n",
      "Iteration 3046, loss = 0.68482094\n",
      "Iteration 3047, loss = 0.68481763\n",
      "Iteration 3048, loss = 0.68481956\n",
      "Iteration 3049, loss = 0.68482046\n",
      "Iteration 3050, loss = 0.68481054\n",
      "Iteration 3051, loss = 0.68480839\n",
      "Iteration 3052, loss = 0.68480664\n",
      "Iteration 3053, loss = 0.68480673\n",
      "Iteration 3054, loss = 0.68480414\n",
      "Iteration 3055, loss = 0.68480125\n",
      "Iteration 3056, loss = 0.68479944\n",
      "Iteration 3057, loss = 0.68479891\n",
      "Iteration 3058, loss = 0.68480647\n",
      "Iteration 3059, loss = 0.68479547\n",
      "Iteration 3060, loss = 0.68479505\n",
      "Iteration 3061, loss = 0.68479223\n",
      "Iteration 3062, loss = 0.68479081\n",
      "Iteration 3063, loss = 0.68478625\n",
      "Iteration 3064, loss = 0.68478851\n",
      "Iteration 3065, loss = 0.68478476\n",
      "Iteration 3066, loss = 0.68478288\n",
      "Iteration 3067, loss = 0.68478128\n",
      "Iteration 3068, loss = 0.68477739\n",
      "Iteration 3069, loss = 0.68477584\n",
      "Iteration 3070, loss = 0.68477853\n",
      "Iteration 3071, loss = 0.68477777\n",
      "Iteration 3072, loss = 0.68477891\n",
      "Iteration 3073, loss = 0.68476824\n",
      "Iteration 3074, loss = 0.68477006\n",
      "Iteration 3075, loss = 0.68476377\n",
      "Iteration 3076, loss = 0.68476573\n",
      "Iteration 3077, loss = 0.68476057\n",
      "Iteration 3078, loss = 0.68476009\n",
      "Iteration 3079, loss = 0.68476007\n",
      "Iteration 3080, loss = 0.68475666\n",
      "Iteration 3081, loss = 0.68475473\n",
      "Iteration 3082, loss = 0.68475127\n",
      "Iteration 3083, loss = 0.68475190\n",
      "Iteration 3084, loss = 0.68474769\n",
      "Iteration 3085, loss = 0.68474506\n",
      "Iteration 3086, loss = 0.68474610\n",
      "Iteration 3087, loss = 0.68474177\n",
      "Iteration 3088, loss = 0.68474086\n",
      "Iteration 3089, loss = 0.68473814\n",
      "Iteration 3090, loss = 0.68474046\n",
      "Iteration 3091, loss = 0.68473358\n",
      "Iteration 3092, loss = 0.68473028\n",
      "Iteration 3093, loss = 0.68473999\n",
      "Iteration 3094, loss = 0.68473101\n",
      "Iteration 3095, loss = 0.68472527\n",
      "Iteration 3096, loss = 0.68472213\n",
      "Iteration 3097, loss = 0.68472286\n",
      "Iteration 3098, loss = 0.68471854\n",
      "Iteration 3099, loss = 0.68471691\n",
      "Iteration 3100, loss = 0.68471384\n",
      "Iteration 3101, loss = 0.68471290\n",
      "Iteration 3102, loss = 0.68470965\n",
      "Iteration 3103, loss = 0.68470781\n",
      "Iteration 3104, loss = 0.68470807\n",
      "Iteration 3105, loss = 0.68470360\n",
      "Iteration 3106, loss = 0.68470193\n",
      "Iteration 3107, loss = 0.68469987\n",
      "Iteration 3108, loss = 0.68470338\n",
      "Iteration 3109, loss = 0.68469553\n",
      "Iteration 3110, loss = 0.68469745\n",
      "Iteration 3111, loss = 0.68469022\n",
      "Iteration 3112, loss = 0.68468876\n",
      "Iteration 3113, loss = 0.68468656\n",
      "Iteration 3114, loss = 0.68468577\n",
      "Iteration 3115, loss = 0.68468796\n",
      "Iteration 3116, loss = 0.68468542\n",
      "Iteration 3117, loss = 0.68469325\n",
      "Iteration 3118, loss = 0.68467851\n",
      "Iteration 3119, loss = 0.68467769\n",
      "Iteration 3120, loss = 0.68467332\n",
      "Iteration 3121, loss = 0.68467889\n",
      "Iteration 3122, loss = 0.68467198\n",
      "Iteration 3123, loss = 0.68466899\n",
      "Iteration 3124, loss = 0.68466358\n",
      "Iteration 3125, loss = 0.68466251\n",
      "Iteration 3126, loss = 0.68466199\n",
      "Iteration 3127, loss = 0.68465873\n",
      "Iteration 3128, loss = 0.68465558\n",
      "Iteration 3129, loss = 0.68465464\n",
      "Iteration 3130, loss = 0.68465465\n",
      "Iteration 3131, loss = 0.68465374\n",
      "Iteration 3132, loss = 0.68465074\n",
      "Iteration 3133, loss = 0.68464566\n",
      "Iteration 3134, loss = 0.68464804\n",
      "Iteration 3135, loss = 0.68464577\n",
      "Iteration 3136, loss = 0.68464133\n",
      "Iteration 3137, loss = 0.68463955\n",
      "Iteration 3138, loss = 0.68463875\n",
      "Iteration 3139, loss = 0.68463560\n",
      "Iteration 3140, loss = 0.68463282\n",
      "Iteration 3141, loss = 0.68463148\n",
      "Iteration 3142, loss = 0.68463190\n",
      "Iteration 3143, loss = 0.68462692\n",
      "Iteration 3144, loss = 0.68462419\n",
      "Iteration 3145, loss = 0.68462655\n",
      "Iteration 3146, loss = 0.68462574\n",
      "Iteration 3147, loss = 0.68461759\n",
      "Iteration 3148, loss = 0.68461660\n",
      "Iteration 3149, loss = 0.68461410\n",
      "Iteration 3150, loss = 0.68461209\n",
      "Iteration 3151, loss = 0.68460947\n",
      "Iteration 3152, loss = 0.68460693\n",
      "Iteration 3153, loss = 0.68460729\n",
      "Iteration 3154, loss = 0.68460724\n",
      "Iteration 3155, loss = 0.68460037\n",
      "Iteration 3156, loss = 0.68460031\n",
      "Iteration 3157, loss = 0.68459562\n",
      "Iteration 3158, loss = 0.68459761\n",
      "Iteration 3159, loss = 0.68459260\n",
      "Iteration 3160, loss = 0.68459073\n",
      "Iteration 3161, loss = 0.68458889\n",
      "Iteration 3162, loss = 0.68458726\n",
      "Iteration 3163, loss = 0.68458401\n",
      "Iteration 3164, loss = 0.68458202\n",
      "Iteration 3165, loss = 0.68458094\n",
      "Iteration 3166, loss = 0.68457982\n",
      "Iteration 3167, loss = 0.68457576\n",
      "Iteration 3168, loss = 0.68457399\n",
      "Iteration 3169, loss = 0.68457425\n",
      "Iteration 3170, loss = 0.68457378\n",
      "Iteration 3171, loss = 0.68456765\n",
      "Iteration 3172, loss = 0.68456570\n",
      "Iteration 3173, loss = 0.68456350\n",
      "Iteration 3174, loss = 0.68456169\n",
      "Iteration 3175, loss = 0.68456010\n",
      "Iteration 3176, loss = 0.68455758\n",
      "Iteration 3177, loss = 0.68455486\n",
      "Iteration 3178, loss = 0.68455480\n",
      "Iteration 3179, loss = 0.68456306\n",
      "Iteration 3180, loss = 0.68455066\n",
      "Iteration 3181, loss = 0.68454970\n",
      "Iteration 3182, loss = 0.68454870\n",
      "Iteration 3183, loss = 0.68454392\n",
      "Iteration 3184, loss = 0.68455163\n",
      "Iteration 3185, loss = 0.68453750\n",
      "Iteration 3186, loss = 0.68454151\n",
      "Iteration 3187, loss = 0.68453446\n",
      "Iteration 3188, loss = 0.68453382\n",
      "Iteration 3189, loss = 0.68452965\n",
      "Iteration 3190, loss = 0.68452863\n",
      "Iteration 3191, loss = 0.68453087\n",
      "Iteration 3192, loss = 0.68452573\n",
      "Iteration 3193, loss = 0.68452352\n",
      "Iteration 3194, loss = 0.68451962\n",
      "Iteration 3195, loss = 0.68451638\n",
      "Iteration 3196, loss = 0.68451750\n",
      "Iteration 3197, loss = 0.68451646\n",
      "Iteration 3198, loss = 0.68451022\n",
      "Iteration 3199, loss = 0.68450953\n",
      "Iteration 3200, loss = 0.68450877\n",
      "Iteration 3201, loss = 0.68451380\n",
      "Iteration 3202, loss = 0.68450286\n",
      "Iteration 3203, loss = 0.68449989\n",
      "Iteration 3204, loss = 0.68449702\n",
      "Iteration 3205, loss = 0.68449389\n",
      "Iteration 3206, loss = 0.68449339\n",
      "Iteration 3207, loss = 0.68448911\n",
      "Iteration 3208, loss = 0.68448854\n",
      "Iteration 3209, loss = 0.68448633\n",
      "Iteration 3210, loss = 0.68448485\n",
      "Iteration 3211, loss = 0.68448880\n",
      "Iteration 3212, loss = 0.68447906\n",
      "Iteration 3213, loss = 0.68447728\n",
      "Iteration 3214, loss = 0.68447433\n",
      "Iteration 3215, loss = 0.68447540\n",
      "Iteration 3216, loss = 0.68447180\n",
      "Iteration 3217, loss = 0.68447186\n",
      "Iteration 3218, loss = 0.68446754\n",
      "Iteration 3219, loss = 0.68446486\n",
      "Iteration 3220, loss = 0.68446388\n",
      "Iteration 3221, loss = 0.68446026\n",
      "Iteration 3222, loss = 0.68445865\n",
      "Iteration 3223, loss = 0.68445679\n",
      "Iteration 3224, loss = 0.68445388\n",
      "Iteration 3225, loss = 0.68445076\n",
      "Iteration 3226, loss = 0.68444960\n",
      "Iteration 3227, loss = 0.68444767\n",
      "Iteration 3228, loss = 0.68444720\n",
      "Iteration 3229, loss = 0.68444478\n",
      "Iteration 3230, loss = 0.68443910\n",
      "Iteration 3231, loss = 0.68443482\n",
      "Iteration 3232, loss = 0.68444011\n",
      "Iteration 3233, loss = 0.68443628\n",
      "Iteration 3234, loss = 0.68443153\n",
      "Iteration 3235, loss = 0.68442871\n",
      "Iteration 3236, loss = 0.68442740\n",
      "Iteration 3237, loss = 0.68442415\n",
      "Iteration 3238, loss = 0.68442044\n",
      "Iteration 3239, loss = 0.68443328\n",
      "Iteration 3240, loss = 0.68441547\n",
      "Iteration 3241, loss = 0.68441248\n",
      "Iteration 3242, loss = 0.68440962\n",
      "Iteration 3243, loss = 0.68440716\n",
      "Iteration 3244, loss = 0.68440619\n",
      "Iteration 3245, loss = 0.68440188\n",
      "Iteration 3246, loss = 0.68440301\n",
      "Iteration 3247, loss = 0.68439974\n",
      "Iteration 3248, loss = 0.68439855\n",
      "Iteration 3249, loss = 0.68439292\n",
      "Iteration 3250, loss = 0.68439199\n",
      "Iteration 3251, loss = 0.68438910\n",
      "Iteration 3252, loss = 0.68438948\n",
      "Iteration 3253, loss = 0.68438423\n",
      "Iteration 3254, loss = 0.68438105\n",
      "Iteration 3255, loss = 0.68437945\n",
      "Iteration 3256, loss = 0.68437820\n",
      "Iteration 3257, loss = 0.68437402\n",
      "Iteration 3258, loss = 0.68437741\n",
      "Iteration 3259, loss = 0.68437131\n",
      "Iteration 3260, loss = 0.68436922\n",
      "Iteration 3261, loss = 0.68436498\n",
      "Iteration 3262, loss = 0.68436455\n",
      "Iteration 3263, loss = 0.68436188\n",
      "Iteration 3264, loss = 0.68436055\n",
      "Iteration 3265, loss = 0.68436749\n",
      "Iteration 3266, loss = 0.68435763\n",
      "Iteration 3267, loss = 0.68435386\n",
      "Iteration 3268, loss = 0.68435330\n",
      "Iteration 3269, loss = 0.68434957\n",
      "Iteration 3270, loss = 0.68434686\n",
      "Iteration 3271, loss = 0.68434349\n",
      "Iteration 3272, loss = 0.68434160\n",
      "Iteration 3273, loss = 0.68433980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3274, loss = 0.68433705\n",
      "Iteration 3275, loss = 0.68434253\n",
      "Iteration 3276, loss = 0.68433421\n",
      "Iteration 3277, loss = 0.68433342\n",
      "Iteration 3278, loss = 0.68433211\n",
      "Iteration 3279, loss = 0.68432642\n",
      "Iteration 3280, loss = 0.68432566\n",
      "Iteration 3281, loss = 0.68432320\n",
      "Iteration 3282, loss = 0.68432152\n",
      "Iteration 3283, loss = 0.68431720\n",
      "Iteration 3284, loss = 0.68431628\n",
      "Iteration 3285, loss = 0.68431287\n",
      "Iteration 3286, loss = 0.68431065\n",
      "Iteration 3287, loss = 0.68430783\n",
      "Iteration 3288, loss = 0.68430666\n",
      "Iteration 3289, loss = 0.68430395\n",
      "Iteration 3290, loss = 0.68430323\n",
      "Iteration 3291, loss = 0.68429864\n",
      "Iteration 3292, loss = 0.68429953\n",
      "Iteration 3293, loss = 0.68429787\n",
      "Iteration 3294, loss = 0.68429220\n",
      "Iteration 3295, loss = 0.68429100\n",
      "Iteration 3296, loss = 0.68428920\n",
      "Iteration 3297, loss = 0.68428706\n",
      "Iteration 3298, loss = 0.68428417\n",
      "Iteration 3299, loss = 0.68428278\n",
      "Iteration 3300, loss = 0.68428141\n",
      "Iteration 3301, loss = 0.68427713\n",
      "Iteration 3302, loss = 0.68427847\n",
      "Iteration 3303, loss = 0.68427238\n",
      "Iteration 3304, loss = 0.68427064\n",
      "Iteration 3305, loss = 0.68426862\n",
      "Iteration 3306, loss = 0.68426702\n",
      "Iteration 3307, loss = 0.68426511\n",
      "Iteration 3308, loss = 0.68426205\n",
      "Iteration 3309, loss = 0.68426077\n",
      "Iteration 3310, loss = 0.68425870\n",
      "Iteration 3311, loss = 0.68425728\n",
      "Iteration 3312, loss = 0.68425476\n",
      "Iteration 3313, loss = 0.68425026\n",
      "Iteration 3314, loss = 0.68425137\n",
      "Iteration 3315, loss = 0.68424707\n",
      "Iteration 3316, loss = 0.68424538\n",
      "Iteration 3317, loss = 0.68424295\n",
      "Iteration 3318, loss = 0.68424240\n",
      "Iteration 3319, loss = 0.68424267\n",
      "Iteration 3320, loss = 0.68424205\n",
      "Iteration 3321, loss = 0.68423594\n",
      "Iteration 3322, loss = 0.68423197\n",
      "Iteration 3323, loss = 0.68422992\n",
      "Iteration 3324, loss = 0.68422807\n",
      "Iteration 3325, loss = 0.68422976\n",
      "Iteration 3326, loss = 0.68422605\n",
      "Iteration 3327, loss = 0.68422170\n",
      "Iteration 3328, loss = 0.68421984\n",
      "Iteration 3329, loss = 0.68421533\n",
      "Iteration 3330, loss = 0.68421609\n",
      "Iteration 3331, loss = 0.68421418\n",
      "Iteration 3332, loss = 0.68421415\n",
      "Iteration 3333, loss = 0.68420923\n",
      "Iteration 3334, loss = 0.68420821\n",
      "Iteration 3335, loss = 0.68420432\n",
      "Iteration 3336, loss = 0.68420742\n",
      "Iteration 3337, loss = 0.68419934\n",
      "Iteration 3338, loss = 0.68419762\n",
      "Iteration 3339, loss = 0.68419440\n",
      "Iteration 3340, loss = 0.68419407\n",
      "Iteration 3341, loss = 0.68419107\n",
      "Iteration 3342, loss = 0.68418804\n",
      "Iteration 3343, loss = 0.68418833\n",
      "Iteration 3344, loss = 0.68418703\n",
      "Iteration 3345, loss = 0.68418665\n",
      "Iteration 3346, loss = 0.68418505\n",
      "Iteration 3347, loss = 0.68417843\n",
      "Iteration 3348, loss = 0.68417515\n",
      "Iteration 3349, loss = 0.68417284\n",
      "Iteration 3350, loss = 0.68417230\n",
      "Iteration 3351, loss = 0.68417007\n",
      "Iteration 3352, loss = 0.68416887\n",
      "Iteration 3353, loss = 0.68416947\n",
      "Iteration 3354, loss = 0.68416306\n",
      "Iteration 3355, loss = 0.68416123\n",
      "Iteration 3356, loss = 0.68415955\n",
      "Iteration 3357, loss = 0.68416303\n",
      "Iteration 3358, loss = 0.68415442\n",
      "Iteration 3359, loss = 0.68415309\n",
      "Iteration 3360, loss = 0.68414871\n",
      "Iteration 3361, loss = 0.68415085\n",
      "Iteration 3362, loss = 0.68414556\n",
      "Iteration 3363, loss = 0.68414600\n",
      "Iteration 3364, loss = 0.68414283\n",
      "Iteration 3365, loss = 0.68414049\n",
      "Iteration 3366, loss = 0.68413947\n",
      "Iteration 3367, loss = 0.68413617\n",
      "Iteration 3368, loss = 0.68413373\n",
      "Iteration 3369, loss = 0.68414053\n",
      "Iteration 3370, loss = 0.68412886\n",
      "Iteration 3371, loss = 0.68413442\n",
      "Iteration 3372, loss = 0.68412566\n",
      "Iteration 3373, loss = 0.68412631\n",
      "Iteration 3374, loss = 0.68412024\n",
      "Iteration 3375, loss = 0.68411659\n",
      "Iteration 3376, loss = 0.68411561\n",
      "Iteration 3377, loss = 0.68411499\n",
      "Iteration 3378, loss = 0.68411315\n",
      "Iteration 3379, loss = 0.68410721\n",
      "Iteration 3380, loss = 0.68411065\n",
      "Iteration 3381, loss = 0.68410506\n",
      "Iteration 3382, loss = 0.68410440\n",
      "Iteration 3383, loss = 0.68409933\n",
      "Iteration 3384, loss = 0.68409661\n",
      "Iteration 3385, loss = 0.68409402\n",
      "Iteration 3386, loss = 0.68409441\n",
      "Iteration 3387, loss = 0.68409273\n",
      "Iteration 3388, loss = 0.68408886\n",
      "Iteration 3389, loss = 0.68409716\n",
      "Iteration 3390, loss = 0.68408345\n",
      "Iteration 3391, loss = 0.68409190\n",
      "Iteration 3392, loss = 0.68407955\n",
      "Iteration 3393, loss = 0.68407700\n",
      "Iteration 3394, loss = 0.68407519\n",
      "Iteration 3395, loss = 0.68407658\n",
      "Iteration 3396, loss = 0.68407531\n",
      "Iteration 3397, loss = 0.68406776\n",
      "Iteration 3398, loss = 0.68406618\n",
      "Iteration 3399, loss = 0.68406464\n",
      "Iteration 3400, loss = 0.68406090\n",
      "Iteration 3401, loss = 0.68406133\n",
      "Iteration 3402, loss = 0.68405892\n",
      "Iteration 3403, loss = 0.68405425\n",
      "Iteration 3404, loss = 0.68405793\n",
      "Iteration 3405, loss = 0.68405376\n",
      "Iteration 3406, loss = 0.68405025\n",
      "Iteration 3407, loss = 0.68404609\n",
      "Iteration 3408, loss = 0.68404512\n",
      "Iteration 3409, loss = 0.68404131\n",
      "Iteration 3410, loss = 0.68404053\n",
      "Iteration 3411, loss = 0.68403621\n",
      "Iteration 3412, loss = 0.68403618\n",
      "Iteration 3413, loss = 0.68403067\n",
      "Iteration 3414, loss = 0.68403291\n",
      "Iteration 3415, loss = 0.68402821\n",
      "Iteration 3416, loss = 0.68402440\n",
      "Iteration 3417, loss = 0.68402292\n",
      "Iteration 3418, loss = 0.68402544\n",
      "Iteration 3419, loss = 0.68401827\n",
      "Iteration 3420, loss = 0.68401876\n",
      "Iteration 3421, loss = 0.68401966\n",
      "Iteration 3422, loss = 0.68401445\n",
      "Iteration 3423, loss = 0.68401278\n",
      "Iteration 3424, loss = 0.68400891\n",
      "Iteration 3425, loss = 0.68400646\n",
      "Iteration 3426, loss = 0.68400470\n",
      "Iteration 3427, loss = 0.68400085\n",
      "Iteration 3428, loss = 0.68399851\n",
      "Iteration 3429, loss = 0.68399768\n",
      "Iteration 3430, loss = 0.68399467\n",
      "Iteration 3431, loss = 0.68399164\n",
      "Iteration 3432, loss = 0.68398866\n",
      "Iteration 3433, loss = 0.68398606\n",
      "Iteration 3434, loss = 0.68398921\n",
      "Iteration 3435, loss = 0.68398602\n",
      "Iteration 3436, loss = 0.68398890\n",
      "Iteration 3437, loss = 0.68398014\n",
      "Iteration 3438, loss = 0.68397607\n",
      "Iteration 3439, loss = 0.68397362\n",
      "Iteration 3440, loss = 0.68398552\n",
      "Iteration 3441, loss = 0.68397089\n",
      "Iteration 3442, loss = 0.68396700\n",
      "Iteration 3443, loss = 0.68396865\n",
      "Iteration 3444, loss = 0.68396393\n",
      "Iteration 3445, loss = 0.68395927\n",
      "Iteration 3446, loss = 0.68395947\n",
      "Iteration 3447, loss = 0.68396983\n",
      "Iteration 3448, loss = 0.68395220\n",
      "Iteration 3449, loss = 0.68395689\n",
      "Iteration 3450, loss = 0.68395025\n",
      "Iteration 3451, loss = 0.68394930\n",
      "Iteration 3452, loss = 0.68394615\n",
      "Iteration 3453, loss = 0.68394358\n",
      "Iteration 3454, loss = 0.68394151\n",
      "Iteration 3455, loss = 0.68395009\n",
      "Iteration 3456, loss = 0.68393475\n",
      "Iteration 3457, loss = 0.68393734\n",
      "Iteration 3458, loss = 0.68393090\n",
      "Iteration 3459, loss = 0.68393238\n",
      "Iteration 3460, loss = 0.68394054\n",
      "Iteration 3461, loss = 0.68392887\n",
      "Iteration 3462, loss = 0.68392083\n",
      "Iteration 3463, loss = 0.68392279\n",
      "Iteration 3464, loss = 0.68391950\n",
      "Iteration 3465, loss = 0.68391938\n",
      "Iteration 3466, loss = 0.68391068\n",
      "Iteration 3467, loss = 0.68391063\n",
      "Iteration 3468, loss = 0.68390692\n",
      "Iteration 3469, loss = 0.68390968\n",
      "Iteration 3470, loss = 0.68390446\n",
      "Iteration 3471, loss = 0.68390240\n",
      "Iteration 3472, loss = 0.68390236\n",
      "Iteration 3473, loss = 0.68389735\n",
      "Iteration 3474, loss = 0.68389701\n",
      "Iteration 3475, loss = 0.68389630\n",
      "Iteration 3476, loss = 0.68389118\n",
      "Iteration 3477, loss = 0.68389228\n",
      "Iteration 3478, loss = 0.68389084\n",
      "Iteration 3479, loss = 0.68388388\n",
      "Iteration 3480, loss = 0.68388227\n",
      "Iteration 3481, loss = 0.68388255\n",
      "Iteration 3482, loss = 0.68387720\n",
      "Iteration 3483, loss = 0.68387798\n",
      "Iteration 3484, loss = 0.68387625\n",
      "Iteration 3485, loss = 0.68387303\n",
      "Iteration 3486, loss = 0.68387534\n",
      "Iteration 3487, loss = 0.68387011\n",
      "Iteration 3488, loss = 0.68386820\n",
      "Iteration 3489, loss = 0.68386375\n",
      "Iteration 3490, loss = 0.68386272\n",
      "Iteration 3491, loss = 0.68385881\n",
      "Iteration 3492, loss = 0.68385602\n",
      "Iteration 3493, loss = 0.68385565\n",
      "Iteration 3494, loss = 0.68385415\n",
      "Iteration 3495, loss = 0.68385078\n",
      "Iteration 3496, loss = 0.68385238\n",
      "Iteration 3497, loss = 0.68384766\n",
      "Iteration 3498, loss = 0.68384565\n",
      "Iteration 3499, loss = 0.68384473\n",
      "Iteration 3500, loss = 0.68384135\n",
      "Iteration 3501, loss = 0.68383940\n",
      "Iteration 3502, loss = 0.68383892\n",
      "Iteration 3503, loss = 0.68383306\n",
      "Iteration 3504, loss = 0.68383078\n",
      "Iteration 3505, loss = 0.68383001\n",
      "Iteration 3506, loss = 0.68383213\n",
      "Iteration 3507, loss = 0.68382813\n",
      "Iteration 3508, loss = 0.68382372\n",
      "Iteration 3509, loss = 0.68382569\n",
      "Iteration 3510, loss = 0.68381937\n",
      "Iteration 3511, loss = 0.68381832\n",
      "Iteration 3512, loss = 0.68381551\n",
      "Iteration 3513, loss = 0.68381656\n",
      "Iteration 3514, loss = 0.68381279\n",
      "Iteration 3515, loss = 0.68380985\n",
      "Iteration 3516, loss = 0.68380854\n",
      "Iteration 3517, loss = 0.68380496\n",
      "Iteration 3518, loss = 0.68380565\n",
      "Iteration 3519, loss = 0.68380369\n",
      "Iteration 3520, loss = 0.68380193\n",
      "Iteration 3521, loss = 0.68379875\n",
      "Iteration 3522, loss = 0.68379782\n",
      "Iteration 3523, loss = 0.68379200\n",
      "Iteration 3524, loss = 0.68379286\n",
      "Iteration 3525, loss = 0.68378985\n",
      "Iteration 3526, loss = 0.68378978\n",
      "Iteration 3527, loss = 0.68378917\n",
      "Iteration 3528, loss = 0.68379092\n",
      "Iteration 3529, loss = 0.68378134\n",
      "Iteration 3530, loss = 0.68378031\n",
      "Iteration 3531, loss = 0.68377728\n",
      "Iteration 3532, loss = 0.68377770\n",
      "Iteration 3533, loss = 0.68377169\n",
      "Iteration 3534, loss = 0.68377191\n",
      "Iteration 3535, loss = 0.68376877\n",
      "Iteration 3536, loss = 0.68376953\n",
      "Iteration 3537, loss = 0.68376493\n",
      "Iteration 3538, loss = 0.68376483\n",
      "Iteration 3539, loss = 0.68376143\n",
      "Iteration 3540, loss = 0.68375808\n",
      "Iteration 3541, loss = 0.68376204\n",
      "Iteration 3542, loss = 0.68375521\n",
      "Iteration 3543, loss = 0.68375211\n",
      "Iteration 3544, loss = 0.68375182\n",
      "Iteration 3545, loss = 0.68374640\n",
      "Iteration 3546, loss = 0.68374442\n",
      "Iteration 3547, loss = 0.68374192\n",
      "Iteration 3548, loss = 0.68373975\n",
      "Iteration 3549, loss = 0.68373744\n",
      "Iteration 3550, loss = 0.68373461\n",
      "Iteration 3551, loss = 0.68373981\n",
      "Iteration 3552, loss = 0.68373110\n",
      "Iteration 3553, loss = 0.68373206\n",
      "Iteration 3554, loss = 0.68372894\n",
      "Iteration 3555, loss = 0.68372876\n",
      "Iteration 3556, loss = 0.68372206\n",
      "Iteration 3557, loss = 0.68372108\n",
      "Iteration 3558, loss = 0.68372507\n",
      "Iteration 3559, loss = 0.68371431\n",
      "Iteration 3560, loss = 0.68371530\n",
      "Iteration 3561, loss = 0.68371075\n",
      "Iteration 3562, loss = 0.68370999\n",
      "Iteration 3563, loss = 0.68370623\n",
      "Iteration 3564, loss = 0.68370414\n",
      "Iteration 3565, loss = 0.68370356\n",
      "Iteration 3566, loss = 0.68370060\n",
      "Iteration 3567, loss = 0.68369901\n",
      "Iteration 3568, loss = 0.68369490\n",
      "Iteration 3569, loss = 0.68369370\n",
      "Iteration 3570, loss = 0.68370686\n",
      "Iteration 3571, loss = 0.68370035\n",
      "Iteration 3572, loss = 0.68368894\n",
      "Iteration 3573, loss = 0.68368812\n",
      "Iteration 3574, loss = 0.68368282\n",
      "Iteration 3575, loss = 0.68368700\n",
      "Iteration 3576, loss = 0.68368109\n",
      "Iteration 3577, loss = 0.68367799\n",
      "Iteration 3578, loss = 0.68367585\n",
      "Iteration 3579, loss = 0.68367234\n",
      "Iteration 3580, loss = 0.68367309\n",
      "Iteration 3581, loss = 0.68366641\n",
      "Iteration 3582, loss = 0.68367083\n",
      "Iteration 3583, loss = 0.68366154\n",
      "Iteration 3584, loss = 0.68366043\n",
      "Iteration 3585, loss = 0.68366639\n",
      "Iteration 3586, loss = 0.68365576\n",
      "Iteration 3587, loss = 0.68365616\n",
      "Iteration 3588, loss = 0.68365115\n",
      "Iteration 3589, loss = 0.68365873\n",
      "Iteration 3590, loss = 0.68364889\n",
      "Iteration 3591, loss = 0.68364586\n",
      "Iteration 3592, loss = 0.68364306\n",
      "Iteration 3593, loss = 0.68364496\n",
      "Iteration 3594, loss = 0.68363892\n",
      "Iteration 3595, loss = 0.68363704\n",
      "Iteration 3596, loss = 0.68363705\n",
      "Iteration 3597, loss = 0.68363605\n",
      "Iteration 3598, loss = 0.68362925\n",
      "Iteration 3599, loss = 0.68362864\n",
      "Iteration 3600, loss = 0.68362447\n",
      "Iteration 3601, loss = 0.68362520\n",
      "Iteration 3602, loss = 0.68362254\n",
      "Iteration 3603, loss = 0.68361884\n",
      "Iteration 3604, loss = 0.68361553\n",
      "Iteration 3605, loss = 0.68361442\n",
      "Iteration 3606, loss = 0.68361332\n",
      "Iteration 3607, loss = 0.68361089\n",
      "Iteration 3608, loss = 0.68360783\n",
      "Iteration 3609, loss = 0.68360369\n",
      "Iteration 3610, loss = 0.68360127\n",
      "Iteration 3611, loss = 0.68359917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3612, loss = 0.68359841\n",
      "Iteration 3613, loss = 0.68359434\n",
      "Iteration 3614, loss = 0.68360130\n",
      "Iteration 3615, loss = 0.68359023\n",
      "Iteration 3616, loss = 0.68359006\n",
      "Iteration 3617, loss = 0.68358728\n",
      "Iteration 3618, loss = 0.68358423\n",
      "Iteration 3619, loss = 0.68358013\n",
      "Iteration 3620, loss = 0.68358075\n",
      "Iteration 3621, loss = 0.68357663\n",
      "Iteration 3622, loss = 0.68357725\n",
      "Iteration 3623, loss = 0.68357194\n",
      "Iteration 3624, loss = 0.68357184\n",
      "Iteration 3625, loss = 0.68356689\n",
      "Iteration 3626, loss = 0.68356347\n",
      "Iteration 3627, loss = 0.68356119\n",
      "Iteration 3628, loss = 0.68356040\n",
      "Iteration 3629, loss = 0.68356078\n",
      "Iteration 3630, loss = 0.68355350\n",
      "Iteration 3631, loss = 0.68355091\n",
      "Iteration 3632, loss = 0.68354921\n",
      "Iteration 3633, loss = 0.68354713\n",
      "Iteration 3634, loss = 0.68354333\n",
      "Iteration 3635, loss = 0.68354385\n",
      "Iteration 3636, loss = 0.68353966\n",
      "Iteration 3637, loss = 0.68353707\n",
      "Iteration 3638, loss = 0.68353490\n",
      "Iteration 3639, loss = 0.68353232\n",
      "Iteration 3640, loss = 0.68353361\n",
      "Iteration 3641, loss = 0.68352920\n",
      "Iteration 3642, loss = 0.68352751\n",
      "Iteration 3643, loss = 0.68352322\n",
      "Iteration 3644, loss = 0.68352143\n",
      "Iteration 3645, loss = 0.68352652\n",
      "Iteration 3646, loss = 0.68351456\n",
      "Iteration 3647, loss = 0.68351233\n",
      "Iteration 3648, loss = 0.68351022\n",
      "Iteration 3649, loss = 0.68350643\n",
      "Iteration 3650, loss = 0.68351071\n",
      "Iteration 3651, loss = 0.68350142\n",
      "Iteration 3652, loss = 0.68349767\n",
      "Iteration 3653, loss = 0.68349685\n",
      "Iteration 3654, loss = 0.68349463\n",
      "Iteration 3655, loss = 0.68349244\n",
      "Iteration 3656, loss = 0.68348822\n",
      "Iteration 3657, loss = 0.68348899\n",
      "Iteration 3658, loss = 0.68348573\n",
      "Iteration 3659, loss = 0.68348102\n",
      "Iteration 3660, loss = 0.68347900\n",
      "Iteration 3661, loss = 0.68348102\n",
      "Iteration 3662, loss = 0.68347846\n",
      "Iteration 3663, loss = 0.68347231\n",
      "Iteration 3664, loss = 0.68347301\n",
      "Iteration 3665, loss = 0.68346654\n",
      "Iteration 3666, loss = 0.68346485\n",
      "Iteration 3667, loss = 0.68346468\n",
      "Iteration 3668, loss = 0.68346013\n",
      "Iteration 3669, loss = 0.68346041\n",
      "Iteration 3670, loss = 0.68345570\n",
      "Iteration 3671, loss = 0.68345159\n",
      "Iteration 3672, loss = 0.68344818\n",
      "Iteration 3673, loss = 0.68344750\n",
      "Iteration 3674, loss = 0.68344322\n",
      "Iteration 3675, loss = 0.68344479\n",
      "Iteration 3676, loss = 0.68343918\n",
      "Iteration 3677, loss = 0.68343616\n",
      "Iteration 3678, loss = 0.68343305\n",
      "Iteration 3679, loss = 0.68343489\n",
      "Iteration 3680, loss = 0.68343001\n",
      "Iteration 3681, loss = 0.68342676\n",
      "Iteration 3682, loss = 0.68342790\n",
      "Iteration 3683, loss = 0.68342057\n",
      "Iteration 3684, loss = 0.68341944\n",
      "Iteration 3685, loss = 0.68341898\n",
      "Iteration 3686, loss = 0.68341631\n",
      "Iteration 3687, loss = 0.68340943\n",
      "Iteration 3688, loss = 0.68340960\n",
      "Iteration 3689, loss = 0.68340590\n",
      "Iteration 3690, loss = 0.68340376\n",
      "Iteration 3691, loss = 0.68340261\n",
      "Iteration 3692, loss = 0.68339781\n",
      "Iteration 3693, loss = 0.68339573\n",
      "Iteration 3694, loss = 0.68339872\n",
      "Iteration 3695, loss = 0.68339128\n",
      "Iteration 3696, loss = 0.68338629\n",
      "Iteration 3697, loss = 0.68338339\n",
      "Iteration 3698, loss = 0.68338231\n",
      "Iteration 3699, loss = 0.68337965\n",
      "Iteration 3700, loss = 0.68338271\n",
      "Iteration 3701, loss = 0.68337670\n",
      "Iteration 3702, loss = 0.68337528\n",
      "Iteration 3703, loss = 0.68336971\n",
      "Iteration 3704, loss = 0.68336850\n",
      "Iteration 3705, loss = 0.68336858\n",
      "Iteration 3706, loss = 0.68336337\n",
      "Iteration 3707, loss = 0.68336251\n",
      "Iteration 3708, loss = 0.68335588\n",
      "Iteration 3709, loss = 0.68335545\n",
      "Iteration 3710, loss = 0.68335084\n",
      "Iteration 3711, loss = 0.68334820\n",
      "Iteration 3712, loss = 0.68334693\n",
      "Iteration 3713, loss = 0.68334240\n",
      "Iteration 3714, loss = 0.68335402\n",
      "Iteration 3715, loss = 0.68333652\n",
      "Iteration 3716, loss = 0.68333493\n",
      "Iteration 3717, loss = 0.68334215\n",
      "Iteration 3718, loss = 0.68332908\n",
      "Iteration 3719, loss = 0.68332699\n",
      "Iteration 3720, loss = 0.68333086\n",
      "Iteration 3721, loss = 0.68332439\n",
      "Iteration 3722, loss = 0.68332028\n",
      "Iteration 3723, loss = 0.68331621\n",
      "Iteration 3724, loss = 0.68331455\n",
      "Iteration 3725, loss = 0.68331090\n",
      "Iteration 3726, loss = 0.68331382\n",
      "Iteration 3727, loss = 0.68330590\n",
      "Iteration 3728, loss = 0.68330460\n",
      "Iteration 3729, loss = 0.68330440\n",
      "Iteration 3730, loss = 0.68330075\n",
      "Iteration 3731, loss = 0.68329573\n",
      "Iteration 3732, loss = 0.68329606\n",
      "Iteration 3733, loss = 0.68329330\n",
      "Iteration 3734, loss = 0.68329196\n",
      "Iteration 3735, loss = 0.68328751\n",
      "Iteration 3736, loss = 0.68328875\n",
      "Iteration 3737, loss = 0.68328065\n",
      "Iteration 3738, loss = 0.68327936\n",
      "Iteration 3739, loss = 0.68327966\n",
      "Iteration 3740, loss = 0.68327053\n",
      "Iteration 3741, loss = 0.68327006\n",
      "Iteration 3742, loss = 0.68326684\n",
      "Iteration 3743, loss = 0.68327309\n",
      "Iteration 3744, loss = 0.68326107\n",
      "Iteration 3745, loss = 0.68326335\n",
      "Iteration 3746, loss = 0.68326877\n",
      "Iteration 3747, loss = 0.68325837\n",
      "Iteration 3748, loss = 0.68325106\n",
      "Iteration 3749, loss = 0.68324933\n",
      "Iteration 3750, loss = 0.68324836\n",
      "Iteration 3751, loss = 0.68324452\n",
      "Iteration 3752, loss = 0.68324282\n",
      "Iteration 3753, loss = 0.68324297\n",
      "Iteration 3754, loss = 0.68323604\n",
      "Iteration 3755, loss = 0.68323284\n",
      "Iteration 3756, loss = 0.68322753\n",
      "Iteration 3757, loss = 0.68322648\n",
      "Iteration 3758, loss = 0.68322322\n",
      "Iteration 3759, loss = 0.68322184\n",
      "Iteration 3760, loss = 0.68321626\n",
      "Iteration 3761, loss = 0.68321450\n",
      "Iteration 3762, loss = 0.68321159\n",
      "Iteration 3763, loss = 0.68320742\n",
      "Iteration 3764, loss = 0.68321026\n",
      "Iteration 3765, loss = 0.68320346\n",
      "Iteration 3766, loss = 0.68319917\n",
      "Iteration 3767, loss = 0.68319701\n",
      "Iteration 3768, loss = 0.68319367\n",
      "Iteration 3769, loss = 0.68319156\n",
      "Iteration 3770, loss = 0.68319030\n",
      "Iteration 3771, loss = 0.68318592\n",
      "Iteration 3772, loss = 0.68318021\n",
      "Iteration 3773, loss = 0.68318022\n",
      "Iteration 3774, loss = 0.68317380\n",
      "Iteration 3775, loss = 0.68317067\n",
      "Iteration 3776, loss = 0.68316809\n",
      "Iteration 3777, loss = 0.68316854\n",
      "Iteration 3778, loss = 0.68316962\n",
      "Iteration 3779, loss = 0.68315883\n",
      "Iteration 3780, loss = 0.68315609\n",
      "Iteration 3781, loss = 0.68315604\n",
      "Iteration 3782, loss = 0.68315458\n",
      "Iteration 3783, loss = 0.68314998\n",
      "Iteration 3784, loss = 0.68315026\n",
      "Iteration 3785, loss = 0.68314189\n",
      "Iteration 3786, loss = 0.68314111\n",
      "Iteration 3787, loss = 0.68313352\n",
      "Iteration 3788, loss = 0.68313650\n",
      "Iteration 3789, loss = 0.68312596\n",
      "Iteration 3790, loss = 0.68313464\n",
      "Iteration 3791, loss = 0.68312141\n",
      "Iteration 3792, loss = 0.68312036\n",
      "Iteration 3793, loss = 0.68311850\n",
      "Iteration 3794, loss = 0.68311568\n",
      "Iteration 3795, loss = 0.68310735\n",
      "Iteration 3796, loss = 0.68311107\n",
      "Iteration 3797, loss = 0.68310273\n",
      "Iteration 3798, loss = 0.68309763\n",
      "Iteration 3799, loss = 0.68309562\n",
      "Iteration 3800, loss = 0.68309210\n",
      "Iteration 3801, loss = 0.68308813\n",
      "Iteration 3802, loss = 0.68308477\n",
      "Iteration 3803, loss = 0.68308234\n",
      "Iteration 3804, loss = 0.68307762\n",
      "Iteration 3805, loss = 0.68307482\n",
      "Iteration 3806, loss = 0.68307677\n",
      "Iteration 3807, loss = 0.68307193\n",
      "Iteration 3808, loss = 0.68306612\n",
      "Iteration 3809, loss = 0.68306105\n",
      "Iteration 3810, loss = 0.68306026\n",
      "Iteration 3811, loss = 0.68305221\n",
      "Iteration 3812, loss = 0.68304790\n",
      "Iteration 3813, loss = 0.68304686\n",
      "Iteration 3814, loss = 0.68304409\n",
      "Iteration 3815, loss = 0.68303718\n",
      "Iteration 3816, loss = 0.68303476\n",
      "Iteration 3817, loss = 0.68304321\n",
      "Iteration 3818, loss = 0.68302923\n",
      "Iteration 3819, loss = 0.68302725\n",
      "Iteration 3820, loss = 0.68302372\n",
      "Iteration 3821, loss = 0.68301689\n",
      "Iteration 3822, loss = 0.68301001\n",
      "Iteration 3823, loss = 0.68300657\n",
      "Iteration 3824, loss = 0.68300091\n",
      "Iteration 3825, loss = 0.68299780\n",
      "Iteration 3826, loss = 0.68299427\n",
      "Iteration 3827, loss = 0.68298925\n",
      "Iteration 3828, loss = 0.68298479\n",
      "Iteration 3829, loss = 0.68298594\n",
      "Iteration 3830, loss = 0.68297760\n",
      "Iteration 3831, loss = 0.68297937\n",
      "Iteration 3832, loss = 0.68297096\n",
      "Iteration 3833, loss = 0.68296671\n",
      "Iteration 3834, loss = 0.68296216\n",
      "Iteration 3835, loss = 0.68295821\n",
      "Iteration 3836, loss = 0.68295285\n",
      "Iteration 3837, loss = 0.68294777\n",
      "Iteration 3838, loss = 0.68294355\n",
      "Iteration 3839, loss = 0.68293873\n",
      "Iteration 3840, loss = 0.68294508\n",
      "Iteration 3841, loss = 0.68293162\n",
      "Iteration 3842, loss = 0.68293407\n",
      "Iteration 3843, loss = 0.68292536\n",
      "Iteration 3844, loss = 0.68291850\n",
      "Iteration 3845, loss = 0.68291077\n",
      "Iteration 3846, loss = 0.68290951\n",
      "Iteration 3847, loss = 0.68290482\n",
      "Iteration 3848, loss = 0.68289593\n",
      "Iteration 3849, loss = 0.68290014\n",
      "Iteration 3850, loss = 0.68288809\n",
      "Iteration 3851, loss = 0.68288242\n",
      "Iteration 3852, loss = 0.68288249\n",
      "Iteration 3853, loss = 0.68287596\n",
      "Iteration 3854, loss = 0.68287136\n",
      "Iteration 3855, loss = 0.68286553\n",
      "Iteration 3856, loss = 0.68286289\n",
      "Iteration 3857, loss = 0.68285490\n",
      "Iteration 3858, loss = 0.68285275\n",
      "Iteration 3859, loss = 0.68284633\n",
      "Iteration 3860, loss = 0.68284984\n",
      "Iteration 3861, loss = 0.68284189\n",
      "Iteration 3862, loss = 0.68283555\n",
      "Iteration 3863, loss = 0.68282925\n",
      "Iteration 3864, loss = 0.68282749\n",
      "Iteration 3865, loss = 0.68282289\n",
      "Iteration 3866, loss = 0.68281384\n",
      "Iteration 3867, loss = 0.68281111\n",
      "Iteration 3868, loss = 0.68280560\n",
      "Iteration 3869, loss = 0.68280272\n",
      "Iteration 3870, loss = 0.68280122\n",
      "Iteration 3871, loss = 0.68279457\n",
      "Iteration 3872, loss = 0.68279344\n",
      "Iteration 3873, loss = 0.68278894\n",
      "Iteration 3874, loss = 0.68279774\n",
      "Iteration 3875, loss = 0.68277874\n",
      "Iteration 3876, loss = 0.68277867\n",
      "Iteration 3877, loss = 0.68276883\n",
      "Iteration 3878, loss = 0.68276450\n",
      "Iteration 3879, loss = 0.68276005\n",
      "Iteration 3880, loss = 0.68275657\n",
      "Iteration 3881, loss = 0.68275528\n",
      "Iteration 3882, loss = 0.68275227\n",
      "Iteration 3883, loss = 0.68275054\n",
      "Iteration 3884, loss = 0.68274067\n",
      "Iteration 3885, loss = 0.68273657\n",
      "Iteration 3886, loss = 0.68273545\n",
      "Iteration 3887, loss = 0.68272827\n",
      "Iteration 3888, loss = 0.68272538\n",
      "Iteration 3889, loss = 0.68272225\n",
      "Iteration 3890, loss = 0.68271824\n",
      "Iteration 3891, loss = 0.68271854\n",
      "Iteration 3892, loss = 0.68270995\n",
      "Iteration 3893, loss = 0.68270482\n",
      "Iteration 3894, loss = 0.68270326\n",
      "Iteration 3895, loss = 0.68269945\n",
      "Iteration 3896, loss = 0.68269383\n",
      "Iteration 3897, loss = 0.68268932\n",
      "Iteration 3898, loss = 0.68268714\n",
      "Iteration 3899, loss = 0.68268076\n",
      "Iteration 3900, loss = 0.68268829\n",
      "Iteration 3901, loss = 0.68267886\n",
      "Iteration 3902, loss = 0.68266784\n",
      "Iteration 3903, loss = 0.68267173\n",
      "Iteration 3904, loss = 0.68265820\n",
      "Iteration 3905, loss = 0.68266806\n",
      "Iteration 3906, loss = 0.68265754\n",
      "Iteration 3907, loss = 0.68264798\n",
      "Iteration 3908, loss = 0.68264189\n",
      "Iteration 3909, loss = 0.68264266\n",
      "Iteration 3910, loss = 0.68263957\n",
      "Iteration 3911, loss = 0.68262934\n",
      "Iteration 3912, loss = 0.68263834\n",
      "Iteration 3913, loss = 0.68262328\n",
      "Iteration 3914, loss = 0.68261791\n",
      "Iteration 3915, loss = 0.68261450\n",
      "Iteration 3916, loss = 0.68261595\n",
      "Iteration 3917, loss = 0.68260275\n",
      "Iteration 3918, loss = 0.68259843\n",
      "Iteration 3919, loss = 0.68259706\n",
      "Iteration 3920, loss = 0.68259077\n",
      "Iteration 3921, loss = 0.68259103\n",
      "Iteration 3922, loss = 0.68258251\n",
      "Iteration 3923, loss = 0.68258106\n",
      "Iteration 3924, loss = 0.68257212\n",
      "Iteration 3925, loss = 0.68256856\n",
      "Iteration 3926, loss = 0.68256556\n",
      "Iteration 3927, loss = 0.68255906\n",
      "Iteration 3928, loss = 0.68255674\n",
      "Iteration 3929, loss = 0.68254890\n",
      "Iteration 3930, loss = 0.68254614\n",
      "Iteration 3931, loss = 0.68255224\n",
      "Iteration 3932, loss = 0.68254041\n",
      "Iteration 3933, loss = 0.68253566\n",
      "Iteration 3934, loss = 0.68252685\n",
      "Iteration 3935, loss = 0.68252549\n",
      "Iteration 3936, loss = 0.68252207\n",
      "Iteration 3937, loss = 0.68251272\n",
      "Iteration 3938, loss = 0.68250839\n",
      "Iteration 3939, loss = 0.68250381\n",
      "Iteration 3940, loss = 0.68249817\n",
      "Iteration 3941, loss = 0.68249544\n",
      "Iteration 3942, loss = 0.68249483\n",
      "Iteration 3943, loss = 0.68248677\n",
      "Iteration 3944, loss = 0.68247902\n",
      "Iteration 3945, loss = 0.68247622\n",
      "Iteration 3946, loss = 0.68247805\n",
      "Iteration 3947, loss = 0.68246555\n",
      "Iteration 3948, loss = 0.68246446\n",
      "Iteration 3949, loss = 0.68246811\n",
      "Iteration 3950, loss = 0.68245839\n",
      "Iteration 3951, loss = 0.68245065\n",
      "Iteration 3952, loss = 0.68244675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3953, loss = 0.68244023\n",
      "Iteration 3954, loss = 0.68243705\n",
      "Iteration 3955, loss = 0.68243497\n",
      "Iteration 3956, loss = 0.68242923\n",
      "Iteration 3957, loss = 0.68242549\n",
      "Iteration 3958, loss = 0.68242013\n",
      "Iteration 3959, loss = 0.68242309\n",
      "Iteration 3960, loss = 0.68241664\n",
      "Iteration 3961, loss = 0.68241263\n",
      "Iteration 3962, loss = 0.68240495\n",
      "Iteration 3963, loss = 0.68240082\n",
      "Iteration 3964, loss = 0.68239870\n",
      "Iteration 3965, loss = 0.68239203\n",
      "Iteration 3966, loss = 0.68238679\n",
      "Iteration 3967, loss = 0.68238635\n",
      "Iteration 3968, loss = 0.68238090\n",
      "Iteration 3969, loss = 0.68237528\n",
      "Iteration 3970, loss = 0.68237232\n",
      "Iteration 3971, loss = 0.68236726\n",
      "Iteration 3972, loss = 0.68236289\n",
      "Iteration 3973, loss = 0.68235718\n",
      "Iteration 3974, loss = 0.68235624\n",
      "Iteration 3975, loss = 0.68235507\n",
      "Iteration 3976, loss = 0.68234355\n",
      "Iteration 3977, loss = 0.68233814\n",
      "Iteration 3978, loss = 0.68233389\n",
      "Iteration 3979, loss = 0.68233175\n",
      "Iteration 3980, loss = 0.68232807\n",
      "Iteration 3981, loss = 0.68232312\n",
      "Iteration 3982, loss = 0.68232241\n",
      "Iteration 3983, loss = 0.68231942\n",
      "Iteration 3984, loss = 0.68230913\n",
      "Iteration 3985, loss = 0.68230483\n",
      "Iteration 3986, loss = 0.68229958\n",
      "Iteration 3987, loss = 0.68229320\n",
      "Iteration 3988, loss = 0.68229012\n",
      "Iteration 3989, loss = 0.68228684\n",
      "Iteration 3990, loss = 0.68228568\n",
      "Iteration 3991, loss = 0.68227605\n",
      "Iteration 3992, loss = 0.68227483\n",
      "Iteration 3993, loss = 0.68226920\n",
      "Iteration 3994, loss = 0.68226230\n",
      "Iteration 3995, loss = 0.68225845\n",
      "Iteration 3996, loss = 0.68225801\n",
      "Iteration 3997, loss = 0.68225176\n",
      "Iteration 3998, loss = 0.68225051\n",
      "Iteration 3999, loss = 0.68223952\n",
      "Iteration 4000, loss = 0.68223889\n",
      "Iteration 4001, loss = 0.68223187\n",
      "Iteration 4002, loss = 0.68223058\n",
      "Iteration 4003, loss = 0.68222246\n",
      "Iteration 4004, loss = 0.68221764\n",
      "Iteration 4005, loss = 0.68221689\n",
      "Iteration 4006, loss = 0.68221246\n",
      "Iteration 4007, loss = 0.68220873\n",
      "Iteration 4008, loss = 0.68220157\n",
      "Iteration 4009, loss = 0.68219963\n",
      "Iteration 4010, loss = 0.68219086\n",
      "Iteration 4011, loss = 0.68219649\n",
      "Iteration 4012, loss = 0.68218427\n",
      "Iteration 4013, loss = 0.68217662\n",
      "Iteration 4014, loss = 0.68217666\n",
      "Iteration 4015, loss = 0.68216997\n",
      "Iteration 4016, loss = 0.68216519\n",
      "Iteration 4017, loss = 0.68215987\n",
      "Iteration 4018, loss = 0.68215657\n",
      "Iteration 4019, loss = 0.68215371\n",
      "Iteration 4020, loss = 0.68214618\n",
      "Iteration 4021, loss = 0.68214152\n",
      "Iteration 4022, loss = 0.68213704\n",
      "Iteration 4023, loss = 0.68214310\n",
      "Iteration 4024, loss = 0.68212762\n",
      "Iteration 4025, loss = 0.68212506\n",
      "Iteration 4026, loss = 0.68212187\n",
      "Iteration 4027, loss = 0.68211606\n",
      "Iteration 4028, loss = 0.68211932\n",
      "Iteration 4029, loss = 0.68210659\n",
      "Iteration 4030, loss = 0.68210814\n",
      "Iteration 4031, loss = 0.68210295\n",
      "Iteration 4032, loss = 0.68209355\n",
      "Iteration 4033, loss = 0.68208822\n",
      "Iteration 4034, loss = 0.68208448\n",
      "Iteration 4035, loss = 0.68208490\n",
      "Iteration 4036, loss = 0.68207423\n",
      "Iteration 4037, loss = 0.68207774\n",
      "Iteration 4038, loss = 0.68206961\n",
      "Iteration 4039, loss = 0.68206181\n",
      "Iteration 4040, loss = 0.68206454\n",
      "Iteration 4041, loss = 0.68205192\n",
      "Iteration 4042, loss = 0.68204776\n",
      "Iteration 4043, loss = 0.68204571\n",
      "Iteration 4044, loss = 0.68204378\n",
      "Iteration 4045, loss = 0.68203753\n",
      "Iteration 4046, loss = 0.68203182\n",
      "Iteration 4047, loss = 0.68202762\n",
      "Iteration 4048, loss = 0.68202279\n",
      "Iteration 4049, loss = 0.68201711\n",
      "Iteration 4050, loss = 0.68201263\n",
      "Iteration 4051, loss = 0.68201327\n",
      "Iteration 4052, loss = 0.68200496\n",
      "Iteration 4053, loss = 0.68200873\n",
      "Iteration 4054, loss = 0.68199542\n",
      "Iteration 4055, loss = 0.68199874\n",
      "Iteration 4056, loss = 0.68198641\n",
      "Iteration 4057, loss = 0.68198643\n",
      "Iteration 4058, loss = 0.68197793\n",
      "Iteration 4059, loss = 0.68197888\n",
      "Iteration 4060, loss = 0.68197299\n",
      "Iteration 4061, loss = 0.68196564\n",
      "Iteration 4062, loss = 0.68196613\n",
      "Iteration 4063, loss = 0.68195649\n",
      "Iteration 4064, loss = 0.68195215\n",
      "Iteration 4065, loss = 0.68194985\n",
      "Iteration 4066, loss = 0.68194357\n",
      "Iteration 4067, loss = 0.68193995\n",
      "Iteration 4068, loss = 0.68193299\n",
      "Iteration 4069, loss = 0.68193204\n",
      "Iteration 4070, loss = 0.68193015\n",
      "Iteration 4071, loss = 0.68192043\n",
      "Iteration 4072, loss = 0.68192234\n",
      "Iteration 4073, loss = 0.68191900\n",
      "Iteration 4074, loss = 0.68191360\n",
      "Iteration 4075, loss = 0.68190441\n",
      "Iteration 4076, loss = 0.68189795\n",
      "Iteration 4077, loss = 0.68189900\n",
      "Iteration 4078, loss = 0.68189135\n",
      "Iteration 4079, loss = 0.68188491\n",
      "Iteration 4080, loss = 0.68188084\n",
      "Iteration 4081, loss = 0.68188209\n",
      "Iteration 4082, loss = 0.68187597\n",
      "Iteration 4083, loss = 0.68186733\n",
      "Iteration 4084, loss = 0.68187075\n",
      "Iteration 4085, loss = 0.68185713\n",
      "Iteration 4086, loss = 0.68185532\n",
      "Iteration 4087, loss = 0.68184995\n",
      "Iteration 4088, loss = 0.68184584\n",
      "Iteration 4089, loss = 0.68184416\n",
      "Iteration 4090, loss = 0.68184089\n",
      "Iteration 4091, loss = 0.68183159\n",
      "Iteration 4092, loss = 0.68182547\n",
      "Iteration 4093, loss = 0.68182686\n",
      "Iteration 4094, loss = 0.68182426\n",
      "Iteration 4095, loss = 0.68181575\n",
      "Iteration 4096, loss = 0.68181214\n",
      "Iteration 4097, loss = 0.68180517\n",
      "Iteration 4098, loss = 0.68180289\n",
      "Iteration 4099, loss = 0.68179626\n",
      "Iteration 4100, loss = 0.68179969\n",
      "Iteration 4101, loss = 0.68179102\n",
      "Iteration 4102, loss = 0.68178410\n",
      "Iteration 4103, loss = 0.68178421\n",
      "Iteration 4104, loss = 0.68177710\n",
      "Iteration 4105, loss = 0.68177028\n",
      "Iteration 4106, loss = 0.68176735\n",
      "Iteration 4107, loss = 0.68176273\n",
      "Iteration 4108, loss = 0.68175692\n",
      "Iteration 4109, loss = 0.68175446\n",
      "Iteration 4110, loss = 0.68174899\n",
      "Iteration 4111, loss = 0.68174428\n",
      "Iteration 4112, loss = 0.68173937\n",
      "Iteration 4113, loss = 0.68173574\n",
      "Iteration 4114, loss = 0.68172976\n",
      "Iteration 4115, loss = 0.68173090\n",
      "Iteration 4116, loss = 0.68172644\n",
      "Iteration 4117, loss = 0.68171711\n",
      "Iteration 4118, loss = 0.68171221\n",
      "Iteration 4119, loss = 0.68171394\n",
      "Iteration 4120, loss = 0.68170579\n",
      "Iteration 4121, loss = 0.68169977\n",
      "Iteration 4122, loss = 0.68169946\n",
      "Iteration 4123, loss = 0.68169259\n",
      "Iteration 4124, loss = 0.68168414\n",
      "Iteration 4125, loss = 0.68168167\n",
      "Iteration 4126, loss = 0.68168033\n",
      "Iteration 4127, loss = 0.68167654\n",
      "Iteration 4128, loss = 0.68166868\n",
      "Iteration 4129, loss = 0.68166304\n",
      "Iteration 4130, loss = 0.68165995\n",
      "Iteration 4131, loss = 0.68165389\n",
      "Iteration 4132, loss = 0.68165018\n",
      "Iteration 4133, loss = 0.68164321\n",
      "Iteration 4134, loss = 0.68164212\n",
      "Iteration 4135, loss = 0.68163751\n",
      "Iteration 4136, loss = 0.68163003\n",
      "Iteration 4137, loss = 0.68162756\n",
      "Iteration 4138, loss = 0.68162221\n",
      "Iteration 4139, loss = 0.68161715\n",
      "Iteration 4140, loss = 0.68161434\n",
      "Iteration 4141, loss = 0.68161119\n",
      "Iteration 4142, loss = 0.68160352\n",
      "Iteration 4143, loss = 0.68160025\n",
      "Iteration 4144, loss = 0.68159684\n",
      "Iteration 4145, loss = 0.68158920\n",
      "Iteration 4146, loss = 0.68158413\n",
      "Iteration 4147, loss = 0.68158864\n",
      "Iteration 4148, loss = 0.68157924\n",
      "Iteration 4149, loss = 0.68157017\n",
      "Iteration 4150, loss = 0.68156652\n",
      "Iteration 4151, loss = 0.68156303\n",
      "Iteration 4152, loss = 0.68155838\n",
      "Iteration 4153, loss = 0.68155199\n",
      "Iteration 4154, loss = 0.68154983\n",
      "Iteration 4155, loss = 0.68154284\n",
      "Iteration 4156, loss = 0.68154063\n",
      "Iteration 4157, loss = 0.68153364\n",
      "Iteration 4158, loss = 0.68153169\n",
      "Iteration 4159, loss = 0.68152522\n",
      "Iteration 4160, loss = 0.68151979\n",
      "Iteration 4161, loss = 0.68151891\n",
      "Iteration 4162, loss = 0.68151675\n",
      "Iteration 4163, loss = 0.68150677\n",
      "Iteration 4164, loss = 0.68150854\n",
      "Iteration 4165, loss = 0.68150083\n",
      "Iteration 4166, loss = 0.68149158\n",
      "Iteration 4167, loss = 0.68148818\n",
      "Iteration 4168, loss = 0.68149058\n",
      "Iteration 4169, loss = 0.68148011\n",
      "Iteration 4170, loss = 0.68147452\n",
      "Iteration 4171, loss = 0.68147236\n",
      "Iteration 4172, loss = 0.68146695\n",
      "Iteration 4173, loss = 0.68146699\n",
      "Iteration 4174, loss = 0.68145689\n",
      "Iteration 4175, loss = 0.68145327\n",
      "Iteration 4176, loss = 0.68144786\n",
      "Iteration 4177, loss = 0.68144760\n",
      "Iteration 4178, loss = 0.68144132\n",
      "Iteration 4179, loss = 0.68143527\n",
      "Iteration 4180, loss = 0.68144033\n",
      "Iteration 4181, loss = 0.68143817\n",
      "Iteration 4182, loss = 0.68142235\n",
      "Iteration 4183, loss = 0.68141952\n",
      "Iteration 4184, loss = 0.68141272\n",
      "Iteration 4185, loss = 0.68140570\n",
      "Iteration 4186, loss = 0.68140575\n",
      "Iteration 4187, loss = 0.68139625\n",
      "Iteration 4188, loss = 0.68139209\n",
      "Iteration 4189, loss = 0.68139494\n",
      "Iteration 4190, loss = 0.68138425\n",
      "Iteration 4191, loss = 0.68137992\n",
      "Iteration 4192, loss = 0.68138163\n",
      "Iteration 4193, loss = 0.68138040\n",
      "Iteration 4194, loss = 0.68137271\n",
      "Iteration 4195, loss = 0.68137164\n",
      "Iteration 4196, loss = 0.68135806\n",
      "Iteration 4197, loss = 0.68135716\n",
      "Iteration 4198, loss = 0.68134883\n",
      "Iteration 4199, loss = 0.68134939\n",
      "Iteration 4200, loss = 0.68133982\n",
      "Iteration 4201, loss = 0.68133865\n",
      "Iteration 4202, loss = 0.68133103\n",
      "Iteration 4203, loss = 0.68133057\n",
      "Iteration 4204, loss = 0.68132073\n",
      "Iteration 4205, loss = 0.68131838\n",
      "Iteration 4206, loss = 0.68131278\n",
      "Iteration 4207, loss = 0.68130882\n",
      "Iteration 4208, loss = 0.68130402\n",
      "Iteration 4209, loss = 0.68129993\n",
      "Iteration 4210, loss = 0.68129490\n",
      "Iteration 4211, loss = 0.68129435\n",
      "Iteration 4212, loss = 0.68128806\n",
      "Iteration 4213, loss = 0.68128281\n",
      "Iteration 4214, loss = 0.68129028\n",
      "Iteration 4215, loss = 0.68127498\n",
      "Iteration 4216, loss = 0.68127113\n",
      "Iteration 4217, loss = 0.68127116\n",
      "Iteration 4218, loss = 0.68126936\n",
      "Iteration 4219, loss = 0.68125789\n",
      "Iteration 4220, loss = 0.68125361\n",
      "Iteration 4221, loss = 0.68124841\n",
      "Iteration 4222, loss = 0.68124254\n",
      "Iteration 4223, loss = 0.68123852\n",
      "Iteration 4224, loss = 0.68123793\n",
      "Iteration 4225, loss = 0.68123688\n",
      "Iteration 4226, loss = 0.68122957\n",
      "Iteration 4227, loss = 0.68122605\n",
      "Iteration 4228, loss = 0.68121748\n",
      "Iteration 4229, loss = 0.68121838\n",
      "Iteration 4230, loss = 0.68120842\n",
      "Iteration 4231, loss = 0.68120423\n",
      "Iteration 4232, loss = 0.68119975\n",
      "Iteration 4233, loss = 0.68119632\n",
      "Iteration 4234, loss = 0.68119389\n",
      "Iteration 4235, loss = 0.68118678\n",
      "Iteration 4236, loss = 0.68118656\n",
      "Iteration 4237, loss = 0.68118128\n",
      "Iteration 4238, loss = 0.68117085\n",
      "Iteration 4239, loss = 0.68116807\n",
      "Iteration 4240, loss = 0.68116429\n",
      "Iteration 4241, loss = 0.68115862\n",
      "Iteration 4242, loss = 0.68115967\n",
      "Iteration 4243, loss = 0.68115012\n",
      "Iteration 4244, loss = 0.68114978\n",
      "Iteration 4245, loss = 0.68113861\n",
      "Iteration 4246, loss = 0.68114011\n",
      "Iteration 4247, loss = 0.68112606\n",
      "Iteration 4248, loss = 0.68112306\n",
      "Iteration 4249, loss = 0.68112337\n",
      "Iteration 4250, loss = 0.68112292\n",
      "Iteration 4251, loss = 0.68110965\n",
      "Iteration 4252, loss = 0.68110826\n",
      "Iteration 4253, loss = 0.68110197\n",
      "Iteration 4254, loss = 0.68109521\n",
      "Iteration 4255, loss = 0.68109604\n",
      "Iteration 4256, loss = 0.68108380\n",
      "Iteration 4257, loss = 0.68108148\n",
      "Iteration 4258, loss = 0.68107553\n",
      "Iteration 4259, loss = 0.68108431\n",
      "Iteration 4260, loss = 0.68106449\n",
      "Iteration 4261, loss = 0.68106436\n",
      "Iteration 4262, loss = 0.68105848\n",
      "Iteration 4263, loss = 0.68105730\n",
      "Iteration 4264, loss = 0.68104508\n",
      "Iteration 4265, loss = 0.68104254\n",
      "Iteration 4266, loss = 0.68104287\n",
      "Iteration 4267, loss = 0.68103642\n",
      "Iteration 4268, loss = 0.68102708\n",
      "Iteration 4269, loss = 0.68102324\n",
      "Iteration 4270, loss = 0.68101758\n",
      "Iteration 4271, loss = 0.68101071\n",
      "Iteration 4272, loss = 0.68100712\n",
      "Iteration 4273, loss = 0.68100479\n",
      "Iteration 4274, loss = 0.68099681\n",
      "Iteration 4275, loss = 0.68099319\n",
      "Iteration 4276, loss = 0.68099365\n",
      "Iteration 4277, loss = 0.68098441\n",
      "Iteration 4278, loss = 0.68097661\n",
      "Iteration 4279, loss = 0.68097255\n",
      "Iteration 4280, loss = 0.68096927\n",
      "Iteration 4281, loss = 0.68096306\n",
      "Iteration 4282, loss = 0.68095768\n",
      "Iteration 4283, loss = 0.68095993\n",
      "Iteration 4284, loss = 0.68094909\n",
      "Iteration 4285, loss = 0.68094732\n",
      "Iteration 4286, loss = 0.68094095\n",
      "Iteration 4287, loss = 0.68093587\n",
      "Iteration 4288, loss = 0.68093191\n",
      "Iteration 4289, loss = 0.68093080\n",
      "Iteration 4290, loss = 0.68092254\n",
      "Iteration 4291, loss = 0.68092139\n",
      "Iteration 4292, loss = 0.68091667\n",
      "Iteration 4293, loss = 0.68091203\n",
      "Iteration 4294, loss = 0.68090726\n",
      "Iteration 4295, loss = 0.68091055\n",
      "Iteration 4296, loss = 0.68089846\n",
      "Iteration 4297, loss = 0.68089498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4298, loss = 0.68088880\n",
      "Iteration 4299, loss = 0.68088073\n",
      "Iteration 4300, loss = 0.68087611\n",
      "Iteration 4301, loss = 0.68087275\n",
      "Iteration 4302, loss = 0.68086876\n",
      "Iteration 4303, loss = 0.68086229\n",
      "Iteration 4304, loss = 0.68087495\n",
      "Iteration 4305, loss = 0.68085223\n",
      "Iteration 4306, loss = 0.68084655\n",
      "Iteration 4307, loss = 0.68085578\n",
      "Iteration 4308, loss = 0.68085223\n",
      "Iteration 4309, loss = 0.68083315\n",
      "Iteration 4310, loss = 0.68082858\n",
      "Iteration 4311, loss = 0.68082326\n",
      "Iteration 4312, loss = 0.68081842\n",
      "Iteration 4313, loss = 0.68081043\n",
      "Iteration 4314, loss = 0.68080976\n",
      "Iteration 4315, loss = 0.68080442\n",
      "Iteration 4316, loss = 0.68080175\n",
      "Iteration 4317, loss = 0.68079213\n",
      "Iteration 4318, loss = 0.68078690\n",
      "Iteration 4319, loss = 0.68078158\n",
      "Iteration 4320, loss = 0.68077774\n",
      "Iteration 4321, loss = 0.68077799\n",
      "Iteration 4322, loss = 0.68076743\n",
      "Iteration 4323, loss = 0.68076898\n",
      "Iteration 4324, loss = 0.68075895\n",
      "Iteration 4325, loss = 0.68075541\n",
      "Iteration 4326, loss = 0.68076030\n",
      "Iteration 4327, loss = 0.68074601\n",
      "Iteration 4328, loss = 0.68074096\n",
      "Iteration 4329, loss = 0.68074000\n",
      "Iteration 4330, loss = 0.68073415\n",
      "Iteration 4331, loss = 0.68072540\n",
      "Iteration 4332, loss = 0.68072304\n",
      "Iteration 4333, loss = 0.68071594\n",
      "Iteration 4334, loss = 0.68071349\n",
      "Iteration 4335, loss = 0.68071503\n",
      "Iteration 4336, loss = 0.68070322\n",
      "Iteration 4337, loss = 0.68069758\n",
      "Iteration 4338, loss = 0.68070305\n",
      "Iteration 4339, loss = 0.68068915\n",
      "Iteration 4340, loss = 0.68068459\n",
      "Iteration 4341, loss = 0.68068263\n",
      "Iteration 4342, loss = 0.68067576\n",
      "Iteration 4343, loss = 0.68067176\n",
      "Iteration 4344, loss = 0.68066907\n",
      "Iteration 4345, loss = 0.68067552\n",
      "Iteration 4346, loss = 0.68065946\n",
      "Iteration 4347, loss = 0.68065326\n",
      "Iteration 4348, loss = 0.68065299\n",
      "Iteration 4349, loss = 0.68064990\n",
      "Iteration 4350, loss = 0.68064045\n",
      "Iteration 4351, loss = 0.68063650\n",
      "Iteration 4352, loss = 0.68063391\n",
      "Iteration 4353, loss = 0.68062493\n",
      "Iteration 4354, loss = 0.68062107\n",
      "Iteration 4355, loss = 0.68061654\n",
      "Iteration 4356, loss = 0.68061380\n",
      "Iteration 4357, loss = 0.68060699\n",
      "Iteration 4358, loss = 0.68060392\n",
      "Iteration 4359, loss = 0.68059700\n",
      "Iteration 4360, loss = 0.68059807\n",
      "Iteration 4361, loss = 0.68059023\n",
      "Iteration 4362, loss = 0.68058625\n",
      "Iteration 4363, loss = 0.68058075\n",
      "Iteration 4364, loss = 0.68057659\n",
      "Iteration 4365, loss = 0.68057375\n",
      "Iteration 4366, loss = 0.68056568\n",
      "Iteration 4367, loss = 0.68056183\n",
      "Iteration 4368, loss = 0.68055542\n",
      "Iteration 4369, loss = 0.68056303\n",
      "Iteration 4370, loss = 0.68054471\n",
      "Iteration 4371, loss = 0.68054414\n",
      "Iteration 4372, loss = 0.68053675\n",
      "Iteration 4373, loss = 0.68053246\n",
      "Iteration 4374, loss = 0.68052428\n",
      "Iteration 4375, loss = 0.68052100\n",
      "Iteration 4376, loss = 0.68051530\n",
      "Iteration 4377, loss = 0.68050964\n",
      "Iteration 4378, loss = 0.68050618\n",
      "Iteration 4379, loss = 0.68050128\n",
      "Iteration 4380, loss = 0.68049896\n",
      "Iteration 4381, loss = 0.68048936\n",
      "Iteration 4382, loss = 0.68048462\n",
      "Iteration 4383, loss = 0.68047736\n",
      "Iteration 4384, loss = 0.68047358\n",
      "Iteration 4385, loss = 0.68046549\n",
      "Iteration 4386, loss = 0.68046019\n",
      "Iteration 4387, loss = 0.68045116\n",
      "Iteration 4388, loss = 0.68044738\n",
      "Iteration 4389, loss = 0.68045691\n",
      "Iteration 4390, loss = 0.68043355\n",
      "Iteration 4391, loss = 0.68043047\n",
      "Iteration 4392, loss = 0.68042682\n",
      "Iteration 4393, loss = 0.68041980\n",
      "Iteration 4394, loss = 0.68041463\n",
      "Iteration 4395, loss = 0.68040509\n",
      "Iteration 4396, loss = 0.68040320\n",
      "Iteration 4397, loss = 0.68039570\n",
      "Iteration 4398, loss = 0.68038992\n",
      "Iteration 4399, loss = 0.68038662\n",
      "Iteration 4400, loss = 0.68037735\n",
      "Iteration 4401, loss = 0.68037480\n",
      "Iteration 4402, loss = 0.68036847\n",
      "Iteration 4403, loss = 0.68036541\n",
      "Iteration 4404, loss = 0.68035788\n",
      "Iteration 4405, loss = 0.68035267\n",
      "Iteration 4406, loss = 0.68034540\n",
      "Iteration 4407, loss = 0.68033962\n",
      "Iteration 4408, loss = 0.68033659\n",
      "Iteration 4409, loss = 0.68032963\n",
      "Iteration 4410, loss = 0.68032167\n",
      "Iteration 4411, loss = 0.68032131\n",
      "Iteration 4412, loss = 0.68030993\n",
      "Iteration 4413, loss = 0.68031076\n",
      "Iteration 4414, loss = 0.68029759\n",
      "Iteration 4415, loss = 0.68028870\n",
      "Iteration 4416, loss = 0.68028298\n",
      "Iteration 4417, loss = 0.68027922\n",
      "Iteration 4418, loss = 0.68027372\n",
      "Iteration 4419, loss = 0.68026775\n",
      "Iteration 4420, loss = 0.68025741\n",
      "Iteration 4421, loss = 0.68025252\n",
      "Iteration 4422, loss = 0.68024492\n",
      "Iteration 4423, loss = 0.68025190\n",
      "Iteration 4424, loss = 0.68023447\n",
      "Iteration 4425, loss = 0.68022938\n",
      "Iteration 4426, loss = 0.68022336\n",
      "Iteration 4427, loss = 0.68021780\n",
      "Iteration 4428, loss = 0.68021017\n",
      "Iteration 4429, loss = 0.68020286\n",
      "Iteration 4430, loss = 0.68020337\n",
      "Iteration 4431, loss = 0.68019147\n",
      "Iteration 4432, loss = 0.68018670\n",
      "Iteration 4433, loss = 0.68018096\n",
      "Iteration 4434, loss = 0.68017620\n",
      "Iteration 4435, loss = 0.68017115\n",
      "Iteration 4436, loss = 0.68016258\n",
      "Iteration 4437, loss = 0.68016041\n",
      "Iteration 4438, loss = 0.68015274\n",
      "Iteration 4439, loss = 0.68014730\n",
      "Iteration 4440, loss = 0.68014331\n",
      "Iteration 4441, loss = 0.68013536\n",
      "Iteration 4442, loss = 0.68012984\n",
      "Iteration 4443, loss = 0.68012406\n",
      "Iteration 4444, loss = 0.68011926\n",
      "Iteration 4445, loss = 0.68013154\n",
      "Iteration 4446, loss = 0.68010756\n",
      "Iteration 4447, loss = 0.68010402\n",
      "Iteration 4448, loss = 0.68009745\n",
      "Iteration 4449, loss = 0.68009183\n",
      "Iteration 4450, loss = 0.68008607\n",
      "Iteration 4451, loss = 0.68007854\n",
      "Iteration 4452, loss = 0.68007741\n",
      "Iteration 4453, loss = 0.68007547\n",
      "Iteration 4454, loss = 0.68006603\n",
      "Iteration 4455, loss = 0.68006049\n",
      "Iteration 4456, loss = 0.68005096\n",
      "Iteration 4457, loss = 0.68005345\n",
      "Iteration 4458, loss = 0.68004859\n",
      "Iteration 4459, loss = 0.68003729\n",
      "Iteration 4460, loss = 0.68003744\n",
      "Iteration 4461, loss = 0.68004160\n",
      "Iteration 4462, loss = 0.68002463\n",
      "Iteration 4463, loss = 0.68002358\n",
      "Iteration 4464, loss = 0.68002162\n",
      "Iteration 4465, loss = 0.68001806\n",
      "Iteration 4466, loss = 0.68000072\n",
      "Iteration 4467, loss = 0.67999588\n",
      "Iteration 4468, loss = 0.67999021\n",
      "Iteration 4469, loss = 0.67998481\n",
      "Iteration 4470, loss = 0.67998160\n",
      "Iteration 4471, loss = 0.67997461\n",
      "Iteration 4472, loss = 0.67997185\n",
      "Iteration 4473, loss = 0.67996478\n",
      "Iteration 4474, loss = 0.67995802\n",
      "Iteration 4475, loss = 0.67995330\n",
      "Iteration 4476, loss = 0.67995235\n",
      "Iteration 4477, loss = 0.67994557\n",
      "Iteration 4478, loss = 0.67994495\n",
      "Iteration 4479, loss = 0.67993356\n",
      "Iteration 4480, loss = 0.67992977\n",
      "Iteration 4481, loss = 0.67992276\n",
      "Iteration 4482, loss = 0.67992444\n",
      "Iteration 4483, loss = 0.67991270\n",
      "Iteration 4484, loss = 0.67990623\n",
      "Iteration 4485, loss = 0.67990312\n",
      "Iteration 4486, loss = 0.67989599\n",
      "Iteration 4487, loss = 0.67989544\n",
      "Iteration 4488, loss = 0.67988628\n",
      "Iteration 4489, loss = 0.67988571\n",
      "Iteration 4490, loss = 0.67988192\n",
      "Iteration 4491, loss = 0.67987365\n",
      "Iteration 4492, loss = 0.67986359\n",
      "Iteration 4493, loss = 0.67986446\n",
      "Iteration 4494, loss = 0.67985417\n",
      "Iteration 4495, loss = 0.67985162\n",
      "Iteration 4496, loss = 0.67984431\n",
      "Iteration 4497, loss = 0.67983767\n",
      "Iteration 4498, loss = 0.67983462\n",
      "Iteration 4499, loss = 0.67982617\n",
      "Iteration 4500, loss = 0.67982482\n",
      "Iteration 4501, loss = 0.67981761\n",
      "Iteration 4502, loss = 0.67981506\n",
      "Iteration 4503, loss = 0.67980425\n",
      "Iteration 4504, loss = 0.67980146\n",
      "Iteration 4505, loss = 0.67979738\n",
      "Iteration 4506, loss = 0.67979943\n",
      "Iteration 4507, loss = 0.67978417\n",
      "Iteration 4508, loss = 0.67978045\n",
      "Iteration 4509, loss = 0.67977525\n",
      "Iteration 4510, loss = 0.67976614\n",
      "Iteration 4511, loss = 0.67976071\n",
      "Iteration 4512, loss = 0.67975389\n",
      "Iteration 4513, loss = 0.67975293\n",
      "Iteration 4514, loss = 0.67974354\n",
      "Iteration 4515, loss = 0.67973836\n",
      "Iteration 4516, loss = 0.67973409\n",
      "Iteration 4517, loss = 0.67973881\n",
      "Iteration 4518, loss = 0.67974562\n",
      "Iteration 4519, loss = 0.67971413\n",
      "Iteration 4520, loss = 0.67971185\n",
      "Iteration 4521, loss = 0.67970440\n",
      "Iteration 4522, loss = 0.67970113\n",
      "Iteration 4523, loss = 0.67969259\n",
      "Iteration 4524, loss = 0.67968773\n",
      "Iteration 4525, loss = 0.67968643\n",
      "Iteration 4526, loss = 0.67968055\n",
      "Iteration 4527, loss = 0.67967191\n",
      "Iteration 4528, loss = 0.67966516\n",
      "Iteration 4529, loss = 0.67966365\n",
      "Iteration 4530, loss = 0.67965476\n",
      "Iteration 4531, loss = 0.67964946\n",
      "Iteration 4532, loss = 0.67964505\n",
      "Iteration 4533, loss = 0.67963842\n",
      "Iteration 4534, loss = 0.67963177\n",
      "Iteration 4535, loss = 0.67962664\n",
      "Iteration 4536, loss = 0.67962554\n",
      "Iteration 4537, loss = 0.67962088\n",
      "Iteration 4538, loss = 0.67961431\n",
      "Iteration 4539, loss = 0.67960715\n",
      "Iteration 4540, loss = 0.67960064\n",
      "Iteration 4541, loss = 0.67959372\n",
      "Iteration 4542, loss = 0.67959078\n",
      "Iteration 4543, loss = 0.67958176\n",
      "Iteration 4544, loss = 0.67957737\n",
      "Iteration 4545, loss = 0.67957580\n",
      "Iteration 4546, loss = 0.67957206\n",
      "Iteration 4547, loss = 0.67956113\n",
      "Iteration 4548, loss = 0.67956318\n",
      "Iteration 4549, loss = 0.67954801\n",
      "Iteration 4550, loss = 0.67954818\n",
      "Iteration 4551, loss = 0.67953886\n",
      "Iteration 4552, loss = 0.67953743\n",
      "Iteration 4553, loss = 0.67953024\n",
      "Iteration 4554, loss = 0.67952823\n",
      "Iteration 4555, loss = 0.67951862\n",
      "Iteration 4556, loss = 0.67951315\n",
      "Iteration 4557, loss = 0.67950569\n",
      "Iteration 4558, loss = 0.67950373\n",
      "Iteration 4559, loss = 0.67949311\n",
      "Iteration 4560, loss = 0.67948606\n",
      "Iteration 4561, loss = 0.67948398\n",
      "Iteration 4562, loss = 0.67947796\n",
      "Iteration 4563, loss = 0.67946858\n",
      "Iteration 4564, loss = 0.67946568\n",
      "Iteration 4565, loss = 0.67945738\n",
      "Iteration 4566, loss = 0.67944970\n",
      "Iteration 4567, loss = 0.67944562\n",
      "Iteration 4568, loss = 0.67944132\n",
      "Iteration 4569, loss = 0.67943218\n",
      "Iteration 4570, loss = 0.67942795\n",
      "Iteration 4571, loss = 0.67942610\n",
      "Iteration 4572, loss = 0.67941625\n",
      "Iteration 4573, loss = 0.67941342\n",
      "Iteration 4574, loss = 0.67940388\n",
      "Iteration 4575, loss = 0.67939957\n",
      "Iteration 4576, loss = 0.67939435\n",
      "Iteration 4577, loss = 0.67939089\n",
      "Iteration 4578, loss = 0.67939526\n",
      "Iteration 4579, loss = 0.67937620\n",
      "Iteration 4580, loss = 0.67937235\n",
      "Iteration 4581, loss = 0.67936488\n",
      "Iteration 4582, loss = 0.67935813\n",
      "Iteration 4583, loss = 0.67935433\n",
      "Iteration 4584, loss = 0.67936275\n",
      "Iteration 4585, loss = 0.67934243\n",
      "Iteration 4586, loss = 0.67933595\n",
      "Iteration 4587, loss = 0.67932972\n",
      "Iteration 4588, loss = 0.67933101\n",
      "Iteration 4589, loss = 0.67931807\n",
      "Iteration 4590, loss = 0.67931373\n",
      "Iteration 4591, loss = 0.67930693\n",
      "Iteration 4592, loss = 0.67930533\n",
      "Iteration 4593, loss = 0.67929692\n",
      "Iteration 4594, loss = 0.67928841\n",
      "Iteration 4595, loss = 0.67928625\n",
      "Iteration 4596, loss = 0.67927853\n",
      "Iteration 4597, loss = 0.67927295\n",
      "Iteration 4598, loss = 0.67926933\n",
      "Iteration 4599, loss = 0.67927161\n",
      "Iteration 4600, loss = 0.67925908\n",
      "Iteration 4601, loss = 0.67925558\n",
      "Iteration 4602, loss = 0.67924885\n",
      "Iteration 4603, loss = 0.67923896\n",
      "Iteration 4604, loss = 0.67923558\n",
      "Iteration 4605, loss = 0.67923279\n",
      "Iteration 4606, loss = 0.67922185\n",
      "Iteration 4607, loss = 0.67922002\n",
      "Iteration 4608, loss = 0.67921280\n",
      "Iteration 4609, loss = 0.67921216\n",
      "Iteration 4610, loss = 0.67920084\n",
      "Iteration 4611, loss = 0.67919420\n",
      "Iteration 4612, loss = 0.67919401\n",
      "Iteration 4613, loss = 0.67918260\n",
      "Iteration 4614, loss = 0.67918400\n",
      "Iteration 4615, loss = 0.67917348\n",
      "Iteration 4616, loss = 0.67916636\n",
      "Iteration 4617, loss = 0.67916014\n",
      "Iteration 4618, loss = 0.67915725\n",
      "Iteration 4619, loss = 0.67915132\n",
      "Iteration 4620, loss = 0.67914584\n",
      "Iteration 4621, loss = 0.67914408\n",
      "Iteration 4622, loss = 0.67913647\n",
      "Iteration 4623, loss = 0.67912656\n",
      "Iteration 4624, loss = 0.67912195\n",
      "Iteration 4625, loss = 0.67911688\n",
      "Iteration 4626, loss = 0.67911030\n",
      "Iteration 4627, loss = 0.67910624\n",
      "Iteration 4628, loss = 0.67911010\n",
      "Iteration 4629, loss = 0.67909309\n",
      "Iteration 4630, loss = 0.67909560\n",
      "Iteration 4631, loss = 0.67908706\n",
      "Iteration 4632, loss = 0.67907953\n",
      "Iteration 4633, loss = 0.67908148\n",
      "Iteration 4634, loss = 0.67906966\n",
      "Iteration 4635, loss = 0.67907353\n",
      "Iteration 4636, loss = 0.67905863\n",
      "Iteration 4637, loss = 0.67905336\n",
      "Iteration 4638, loss = 0.67904784\n",
      "Iteration 4639, loss = 0.67904626\n",
      "Iteration 4640, loss = 0.67904137\n",
      "Iteration 4641, loss = 0.67903139\n",
      "Iteration 4642, loss = 0.67903304\n",
      "Iteration 4643, loss = 0.67902451\n",
      "Iteration 4644, loss = 0.67901762\n",
      "Iteration 4645, loss = 0.67901213\n",
      "Iteration 4646, loss = 0.67901045\n",
      "Iteration 4647, loss = 0.67900610\n",
      "Iteration 4648, loss = 0.67899660\n",
      "Iteration 4649, loss = 0.67899247\n",
      "Iteration 4650, loss = 0.67898785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4651, loss = 0.67898094\n",
      "Iteration 4652, loss = 0.67898121\n",
      "Iteration 4653, loss = 0.67897637\n",
      "Iteration 4654, loss = 0.67896455\n",
      "Iteration 4655, loss = 0.67896060\n",
      "Iteration 4656, loss = 0.67895722\n",
      "Iteration 4657, loss = 0.67896646\n",
      "Iteration 4658, loss = 0.67895098\n",
      "Iteration 4659, loss = 0.67894101\n",
      "Iteration 4660, loss = 0.67894451\n",
      "Iteration 4661, loss = 0.67893005\n",
      "Iteration 4662, loss = 0.67892467\n",
      "Iteration 4663, loss = 0.67892406\n",
      "Iteration 4664, loss = 0.67891432\n",
      "Iteration 4665, loss = 0.67890566\n",
      "Iteration 4666, loss = 0.67890230\n",
      "Iteration 4667, loss = 0.67891181\n",
      "Iteration 4668, loss = 0.67889076\n",
      "Iteration 4669, loss = 0.67889480\n",
      "Iteration 4670, loss = 0.67888278\n",
      "Iteration 4671, loss = 0.67887609\n",
      "Iteration 4672, loss = 0.67887345\n",
      "Iteration 4673, loss = 0.67886565\n",
      "Iteration 4674, loss = 0.67886547\n",
      "Iteration 4675, loss = 0.67886030\n",
      "Iteration 4676, loss = 0.67885340\n",
      "Iteration 4677, loss = 0.67885106\n",
      "Iteration 4678, loss = 0.67883890\n",
      "Iteration 4679, loss = 0.67883564\n",
      "Iteration 4680, loss = 0.67883124\n",
      "Iteration 4681, loss = 0.67882186\n",
      "Iteration 4682, loss = 0.67881685\n",
      "Iteration 4683, loss = 0.67881299\n",
      "Iteration 4684, loss = 0.67880775\n",
      "Iteration 4685, loss = 0.67880499\n",
      "Iteration 4686, loss = 0.67879874\n",
      "Iteration 4687, loss = 0.67878923\n",
      "Iteration 4688, loss = 0.67878749\n",
      "Iteration 4689, loss = 0.67878333\n",
      "Iteration 4690, loss = 0.67877650\n",
      "Iteration 4691, loss = 0.67876870\n",
      "Iteration 4692, loss = 0.67876177\n",
      "Iteration 4693, loss = 0.67876436\n",
      "Iteration 4694, loss = 0.67874976\n",
      "Iteration 4695, loss = 0.67874688\n",
      "Iteration 4696, loss = 0.67874648\n",
      "Iteration 4697, loss = 0.67873476\n",
      "Iteration 4698, loss = 0.67872862\n",
      "Iteration 4699, loss = 0.67872723\n",
      "Iteration 4700, loss = 0.67871785\n",
      "Iteration 4701, loss = 0.67871372\n",
      "Iteration 4702, loss = 0.67871642\n",
      "Iteration 4703, loss = 0.67870213\n",
      "Iteration 4704, loss = 0.67869779\n",
      "Iteration 4705, loss = 0.67869029\n",
      "Iteration 4706, loss = 0.67868755\n",
      "Iteration 4707, loss = 0.67868653\n",
      "Iteration 4708, loss = 0.67868394\n",
      "Iteration 4709, loss = 0.67867435\n",
      "Iteration 4710, loss = 0.67866861\n",
      "Iteration 4711, loss = 0.67866324\n",
      "Iteration 4712, loss = 0.67866033\n",
      "Iteration 4713, loss = 0.67865818\n",
      "Iteration 4714, loss = 0.67864873\n",
      "Iteration 4715, loss = 0.67864393\n",
      "Iteration 4716, loss = 0.67863936\n",
      "Iteration 4717, loss = 0.67863055\n",
      "Iteration 4718, loss = 0.67862920\n",
      "Iteration 4719, loss = 0.67862482\n",
      "Iteration 4720, loss = 0.67861825\n",
      "Iteration 4721, loss = 0.67861201\n",
      "Iteration 4722, loss = 0.67860485\n",
      "Iteration 4723, loss = 0.67860465\n",
      "Iteration 4724, loss = 0.67859333\n",
      "Iteration 4725, loss = 0.67859108\n",
      "Iteration 4726, loss = 0.67858305\n",
      "Iteration 4727, loss = 0.67858068\n",
      "Iteration 4728, loss = 0.67859236\n",
      "Iteration 4729, loss = 0.67857063\n",
      "Iteration 4730, loss = 0.67856392\n",
      "Iteration 4731, loss = 0.67855648\n",
      "Iteration 4732, loss = 0.67855180\n",
      "Iteration 4733, loss = 0.67855192\n",
      "Iteration 4734, loss = 0.67854071\n",
      "Iteration 4735, loss = 0.67853799\n",
      "Iteration 4736, loss = 0.67853148\n",
      "Iteration 4737, loss = 0.67852897\n",
      "Iteration 4738, loss = 0.67852051\n",
      "Iteration 4739, loss = 0.67851410\n",
      "Iteration 4740, loss = 0.67850910\n",
      "Iteration 4741, loss = 0.67850626\n",
      "Iteration 4742, loss = 0.67849669\n",
      "Iteration 4743, loss = 0.67849079\n",
      "Iteration 4744, loss = 0.67848756\n",
      "Iteration 4745, loss = 0.67848288\n",
      "Iteration 4746, loss = 0.67847850\n",
      "Iteration 4747, loss = 0.67847218\n",
      "Iteration 4748, loss = 0.67846409\n",
      "Iteration 4749, loss = 0.67845930\n",
      "Iteration 4750, loss = 0.67845516\n",
      "Iteration 4751, loss = 0.67846826\n",
      "Iteration 4752, loss = 0.67844762\n",
      "Iteration 4753, loss = 0.67844243\n",
      "Iteration 4754, loss = 0.67843068\n",
      "Iteration 4755, loss = 0.67843155\n",
      "Iteration 4756, loss = 0.67842249\n",
      "Iteration 4757, loss = 0.67842001\n",
      "Iteration 4758, loss = 0.67841320\n",
      "Iteration 4759, loss = 0.67840690\n",
      "Iteration 4760, loss = 0.67840839\n",
      "Iteration 4761, loss = 0.67839578\n",
      "Iteration 4762, loss = 0.67838834\n",
      "Iteration 4763, loss = 0.67838784\n",
      "Iteration 4764, loss = 0.67838011\n",
      "Iteration 4765, loss = 0.67837131\n",
      "Iteration 4766, loss = 0.67836648\n",
      "Iteration 4767, loss = 0.67836230\n",
      "Iteration 4768, loss = 0.67835328\n",
      "Iteration 4769, loss = 0.67834755\n",
      "Iteration 4770, loss = 0.67834512\n",
      "Iteration 4771, loss = 0.67833525\n",
      "Iteration 4772, loss = 0.67833376\n",
      "Iteration 4773, loss = 0.67832645\n",
      "Iteration 4774, loss = 0.67831795\n",
      "Iteration 4775, loss = 0.67830884\n",
      "Iteration 4776, loss = 0.67830741\n",
      "Iteration 4777, loss = 0.67829974\n",
      "Iteration 4778, loss = 0.67829736\n",
      "Iteration 4779, loss = 0.67828405\n",
      "Iteration 4780, loss = 0.67827924\n",
      "Iteration 4781, loss = 0.67827492\n",
      "Iteration 4782, loss = 0.67827403\n",
      "Iteration 4783, loss = 0.67826505\n",
      "Iteration 4784, loss = 0.67825737\n",
      "Iteration 4785, loss = 0.67825107\n",
      "Iteration 4786, loss = 0.67825039\n",
      "Iteration 4787, loss = 0.67823652\n",
      "Iteration 4788, loss = 0.67823120\n",
      "Iteration 4789, loss = 0.67822732\n",
      "Iteration 4790, loss = 0.67822023\n",
      "Iteration 4791, loss = 0.67821428\n",
      "Iteration 4792, loss = 0.67820673\n",
      "Iteration 4793, loss = 0.67820129\n",
      "Iteration 4794, loss = 0.67820050\n",
      "Iteration 4795, loss = 0.67818981\n",
      "Iteration 4796, loss = 0.67818514\n",
      "Iteration 4797, loss = 0.67818132\n",
      "Iteration 4798, loss = 0.67817699\n",
      "Iteration 4799, loss = 0.67816729\n",
      "Iteration 4800, loss = 0.67816685\n",
      "Iteration 4801, loss = 0.67816223\n",
      "Iteration 4802, loss = 0.67815044\n",
      "Iteration 4803, loss = 0.67814202\n",
      "Iteration 4804, loss = 0.67813728\n",
      "Iteration 4805, loss = 0.67812835\n",
      "Iteration 4806, loss = 0.67812396\n",
      "Iteration 4807, loss = 0.67812835\n",
      "Iteration 4808, loss = 0.67811578\n",
      "Iteration 4809, loss = 0.67810744\n",
      "Iteration 4810, loss = 0.67809937\n",
      "Iteration 4811, loss = 0.67809848\n",
      "Iteration 4812, loss = 0.67808765\n",
      "Iteration 4813, loss = 0.67808248\n",
      "Iteration 4814, loss = 0.67807863\n",
      "Iteration 4815, loss = 0.67807318\n",
      "Iteration 4816, loss = 0.67807210\n",
      "Iteration 4817, loss = 0.67805843\n",
      "Iteration 4818, loss = 0.67805349\n",
      "Iteration 4819, loss = 0.67804781\n",
      "Iteration 4820, loss = 0.67804550\n",
      "Iteration 4821, loss = 0.67803989\n",
      "Iteration 4822, loss = 0.67803483\n",
      "Iteration 4823, loss = 0.67802558\n",
      "Iteration 4824, loss = 0.67801988\n",
      "Iteration 4825, loss = 0.67801380\n",
      "Iteration 4826, loss = 0.67800894\n",
      "Iteration 4827, loss = 0.67800514\n",
      "Iteration 4828, loss = 0.67800075\n",
      "Iteration 4829, loss = 0.67799071\n",
      "Iteration 4830, loss = 0.67798820\n",
      "Iteration 4831, loss = 0.67797972\n",
      "Iteration 4832, loss = 0.67798166\n",
      "Iteration 4833, loss = 0.67796862\n",
      "Iteration 4834, loss = 0.67797046\n",
      "Iteration 4835, loss = 0.67798093\n",
      "Iteration 4836, loss = 0.67795557\n",
      "Iteration 4837, loss = 0.67794532\n",
      "Iteration 4838, loss = 0.67794000\n",
      "Iteration 4839, loss = 0.67793263\n",
      "Iteration 4840, loss = 0.67792761\n",
      "Iteration 4841, loss = 0.67792403\n",
      "Iteration 4842, loss = 0.67792286\n",
      "Iteration 4843, loss = 0.67791434\n",
      "Iteration 4844, loss = 0.67791076\n",
      "Iteration 4845, loss = 0.67789889\n",
      "Iteration 4846, loss = 0.67789769\n",
      "Iteration 4847, loss = 0.67789457\n",
      "Iteration 4848, loss = 0.67788222\n",
      "Iteration 4849, loss = 0.67787683\n",
      "Iteration 4850, loss = 0.67787248\n",
      "Iteration 4851, loss = 0.67786695\n",
      "Iteration 4852, loss = 0.67786373\n",
      "Iteration 4853, loss = 0.67785900\n",
      "Iteration 4854, loss = 0.67785427\n",
      "Iteration 4855, loss = 0.67785066\n",
      "Iteration 4856, loss = 0.67784256\n",
      "Iteration 4857, loss = 0.67783624\n",
      "Iteration 4858, loss = 0.67783292\n",
      "Iteration 4859, loss = 0.67783271\n",
      "Iteration 4860, loss = 0.67782922\n",
      "Iteration 4861, loss = 0.67781571\n",
      "Iteration 4862, loss = 0.67781015\n",
      "Iteration 4863, loss = 0.67780594\n",
      "Iteration 4864, loss = 0.67782112\n",
      "Iteration 4865, loss = 0.67779702\n",
      "Iteration 4866, loss = 0.67780156\n",
      "Iteration 4867, loss = 0.67778506\n",
      "Iteration 4868, loss = 0.67778015\n",
      "Iteration 4869, loss = 0.67777372\n",
      "Iteration 4870, loss = 0.67777599\n",
      "Iteration 4871, loss = 0.67776399\n",
      "Iteration 4872, loss = 0.67776550\n",
      "Iteration 4873, loss = 0.67775527\n",
      "Iteration 4874, loss = 0.67775449\n",
      "Iteration 4875, loss = 0.67774564\n",
      "Iteration 4876, loss = 0.67774005\n",
      "Iteration 4877, loss = 0.67773329\n",
      "Iteration 4878, loss = 0.67773283\n",
      "Iteration 4879, loss = 0.67772413\n",
      "Iteration 4880, loss = 0.67772426\n",
      "Iteration 4881, loss = 0.67772132\n",
      "Iteration 4882, loss = 0.67771096\n",
      "Iteration 4883, loss = 0.67770562\n",
      "Iteration 4884, loss = 0.67771138\n",
      "Iteration 4885, loss = 0.67769782\n",
      "Iteration 4886, loss = 0.67768818\n",
      "Iteration 4887, loss = 0.67769095\n",
      "Iteration 4888, loss = 0.67767829\n",
      "Iteration 4889, loss = 0.67767665\n",
      "Iteration 4890, loss = 0.67766615\n",
      "Iteration 4891, loss = 0.67766567\n",
      "Iteration 4892, loss = 0.67765588\n",
      "Iteration 4893, loss = 0.67765091\n",
      "Iteration 4894, loss = 0.67764613\n",
      "Iteration 4895, loss = 0.67765120\n",
      "Iteration 4896, loss = 0.67763372\n",
      "Iteration 4897, loss = 0.67762933\n",
      "Iteration 4898, loss = 0.67762722\n",
      "Iteration 4899, loss = 0.67761829\n",
      "Iteration 4900, loss = 0.67761947\n",
      "Iteration 4901, loss = 0.67761068\n",
      "Iteration 4902, loss = 0.67760469\n",
      "Iteration 4903, loss = 0.67760376\n",
      "Iteration 4904, loss = 0.67759255\n",
      "Iteration 4905, loss = 0.67758591\n",
      "Iteration 4906, loss = 0.67758380\n",
      "Iteration 4907, loss = 0.67757401\n",
      "Iteration 4908, loss = 0.67757261\n",
      "Iteration 4909, loss = 0.67756023\n",
      "Iteration 4910, loss = 0.67755415\n",
      "Iteration 4911, loss = 0.67755065\n",
      "Iteration 4912, loss = 0.67755907\n",
      "Iteration 4913, loss = 0.67754194\n",
      "Iteration 4914, loss = 0.67753455\n",
      "Iteration 4915, loss = 0.67752706\n",
      "Iteration 4916, loss = 0.67753828\n",
      "Iteration 4917, loss = 0.67751591\n",
      "Iteration 4918, loss = 0.67751093\n",
      "Iteration 4919, loss = 0.67750610\n",
      "Iteration 4920, loss = 0.67750202\n",
      "Iteration 4921, loss = 0.67749122\n",
      "Iteration 4922, loss = 0.67748389\n",
      "Iteration 4923, loss = 0.67747815\n",
      "Iteration 4924, loss = 0.67747359\n",
      "Iteration 4925, loss = 0.67748248\n",
      "Iteration 4926, loss = 0.67746381\n",
      "Iteration 4927, loss = 0.67746153\n",
      "Iteration 4928, loss = 0.67745035\n",
      "Iteration 4929, loss = 0.67744661\n",
      "Iteration 4930, loss = 0.67744016\n",
      "Iteration 4931, loss = 0.67744232\n",
      "Iteration 4932, loss = 0.67743131\n",
      "Iteration 4933, loss = 0.67743122\n",
      "Iteration 4934, loss = 0.67742058\n",
      "Iteration 4935, loss = 0.67742073\n",
      "Iteration 4936, loss = 0.67740829\n",
      "Iteration 4937, loss = 0.67740209\n",
      "Iteration 4938, loss = 0.67740310\n",
      "Iteration 4939, loss = 0.67739325\n",
      "Iteration 4940, loss = 0.67738491\n",
      "Iteration 4941, loss = 0.67737901\n",
      "Iteration 4942, loss = 0.67737989\n",
      "Iteration 4943, loss = 0.67737354\n",
      "Iteration 4944, loss = 0.67736275\n",
      "Iteration 4945, loss = 0.67735749\n",
      "Iteration 4946, loss = 0.67735932\n",
      "Iteration 4947, loss = 0.67735489\n",
      "Iteration 4948, loss = 0.67734313\n",
      "Iteration 4949, loss = 0.67733881\n",
      "Iteration 4950, loss = 0.67733383\n",
      "Iteration 4951, loss = 0.67732931\n",
      "Iteration 4952, loss = 0.67732436\n",
      "Iteration 4953, loss = 0.67731497\n",
      "Iteration 4954, loss = 0.67731081\n",
      "Iteration 4955, loss = 0.67730602\n",
      "Iteration 4956, loss = 0.67729998\n",
      "Iteration 4957, loss = 0.67729696\n",
      "Iteration 4958, loss = 0.67729196\n",
      "Iteration 4959, loss = 0.67728689\n",
      "Iteration 4960, loss = 0.67728073\n",
      "Iteration 4961, loss = 0.67727846\n",
      "Iteration 4962, loss = 0.67727688\n",
      "Iteration 4963, loss = 0.67727017\n",
      "Iteration 4964, loss = 0.67726200\n",
      "Iteration 4965, loss = 0.67725651\n",
      "Iteration 4966, loss = 0.67725340\n",
      "Iteration 4967, loss = 0.67724817\n",
      "Iteration 4968, loss = 0.67724324\n",
      "Iteration 4969, loss = 0.67723865\n",
      "Iteration 4970, loss = 0.67722996\n",
      "Iteration 4971, loss = 0.67722943\n",
      "Iteration 4972, loss = 0.67722009\n",
      "Iteration 4973, loss = 0.67721371\n",
      "Iteration 4974, loss = 0.67721353\n",
      "Iteration 4975, loss = 0.67720697\n",
      "Iteration 4976, loss = 0.67719835\n",
      "Iteration 4977, loss = 0.67719254\n",
      "Iteration 4978, loss = 0.67719135\n",
      "Iteration 4979, loss = 0.67718597\n",
      "Iteration 4980, loss = 0.67718594\n",
      "Iteration 4981, loss = 0.67718692\n",
      "Iteration 4982, loss = 0.67716962\n",
      "Iteration 4983, loss = 0.67716939\n",
      "Iteration 4984, loss = 0.67716047\n",
      "Iteration 4985, loss = 0.67715645\n",
      "Iteration 4986, loss = 0.67714909\n",
      "Iteration 4987, loss = 0.67714344\n",
      "Iteration 4988, loss = 0.67714108\n",
      "Iteration 4989, loss = 0.67713453\n",
      "Iteration 4990, loss = 0.67713287\n",
      "Iteration 4991, loss = 0.67712139\n",
      "Iteration 4992, loss = 0.67711587\n",
      "Iteration 4993, loss = 0.67711227\n",
      "Iteration 4994, loss = 0.67710577\n",
      "Iteration 4995, loss = 0.67710784\n",
      "Iteration 4996, loss = 0.67709750\n",
      "Iteration 4997, loss = 0.67709052\n",
      "Iteration 4998, loss = 0.67708522\n",
      "Iteration 4999, loss = 0.67708666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5000, loss = 0.67708064\n",
      "Iteration 5001, loss = 0.67707461\n",
      "Iteration 5002, loss = 0.67706567\n",
      "Iteration 5003, loss = 0.67705869\n",
      "Iteration 5004, loss = 0.67705343\n",
      "Iteration 5005, loss = 0.67704972\n",
      "Iteration 5006, loss = 0.67704814\n",
      "Iteration 5007, loss = 0.67705960\n",
      "Iteration 5008, loss = 0.67703773\n",
      "Iteration 5009, loss = 0.67703496\n",
      "Iteration 5010, loss = 0.67702658\n",
      "Iteration 5011, loss = 0.67701778\n",
      "Iteration 5012, loss = 0.67701199\n",
      "Iteration 5013, loss = 0.67700783\n",
      "Iteration 5014, loss = 0.67700683\n",
      "Iteration 5015, loss = 0.67699829\n",
      "Iteration 5016, loss = 0.67699871\n",
      "Iteration 5017, loss = 0.67699074\n",
      "Iteration 5018, loss = 0.67698584\n",
      "Iteration 5019, loss = 0.67698232\n",
      "Iteration 5020, loss = 0.67697319\n",
      "Iteration 5021, loss = 0.67696847\n",
      "Iteration 5022, loss = 0.67696384\n",
      "Iteration 5023, loss = 0.67695548\n",
      "Iteration 5024, loss = 0.67695805\n",
      "Iteration 5025, loss = 0.67694451\n",
      "Iteration 5026, loss = 0.67694129\n",
      "Iteration 5027, loss = 0.67693581\n",
      "Iteration 5028, loss = 0.67693002\n",
      "Iteration 5029, loss = 0.67692450\n",
      "Iteration 5030, loss = 0.67692245\n",
      "Iteration 5031, loss = 0.67691669\n",
      "Iteration 5032, loss = 0.67691245\n",
      "Iteration 5033, loss = 0.67690169\n",
      "Iteration 5034, loss = 0.67690183\n",
      "Iteration 5035, loss = 0.67689117\n",
      "Iteration 5036, loss = 0.67688587\n",
      "Iteration 5037, loss = 0.67688079\n",
      "Iteration 5038, loss = 0.67687571\n",
      "Iteration 5039, loss = 0.67687290\n",
      "Iteration 5040, loss = 0.67686575\n",
      "Iteration 5041, loss = 0.67686219\n",
      "Iteration 5042, loss = 0.67685808\n",
      "Iteration 5043, loss = 0.67685306\n",
      "Iteration 5044, loss = 0.67684513\n",
      "Iteration 5045, loss = 0.67684512\n",
      "Iteration 5046, loss = 0.67683816\n",
      "Iteration 5047, loss = 0.67683305\n",
      "Iteration 5048, loss = 0.67683230\n",
      "Iteration 5049, loss = 0.67682444\n",
      "Iteration 5050, loss = 0.67682280\n",
      "Iteration 5051, loss = 0.67681175\n",
      "Iteration 5052, loss = 0.67680540\n",
      "Iteration 5053, loss = 0.67680149\n",
      "Iteration 5054, loss = 0.67679522\n",
      "Iteration 5055, loss = 0.67679333\n",
      "Iteration 5056, loss = 0.67678786\n",
      "Iteration 5057, loss = 0.67678144\n",
      "Iteration 5058, loss = 0.67677823\n",
      "Iteration 5059, loss = 0.67677425\n",
      "Iteration 5060, loss = 0.67676905\n",
      "Iteration 5061, loss = 0.67676283\n",
      "Iteration 5062, loss = 0.67675822\n",
      "Iteration 5063, loss = 0.67675751\n",
      "Iteration 5064, loss = 0.67676047\n",
      "Iteration 5065, loss = 0.67674991\n",
      "Iteration 5066, loss = 0.67674450\n",
      "Iteration 5067, loss = 0.67673576\n",
      "Iteration 5068, loss = 0.67673148\n",
      "Iteration 5069, loss = 0.67673037\n",
      "Iteration 5070, loss = 0.67672142\n",
      "Iteration 5071, loss = 0.67671949\n",
      "Iteration 5072, loss = 0.67671587\n",
      "Iteration 5073, loss = 0.67671084\n",
      "Iteration 5074, loss = 0.67670490\n",
      "Iteration 5075, loss = 0.67670021\n",
      "Iteration 5076, loss = 0.67670466\n",
      "Iteration 5077, loss = 0.67669246\n",
      "Iteration 5078, loss = 0.67668811\n",
      "Iteration 5079, loss = 0.67668573\n",
      "Iteration 5080, loss = 0.67667990\n",
      "Iteration 5081, loss = 0.67668647\n",
      "Iteration 5082, loss = 0.67667526\n",
      "Iteration 5083, loss = 0.67666941\n",
      "Iteration 5084, loss = 0.67666658\n",
      "Iteration 5085, loss = 0.67665937\n",
      "Iteration 5086, loss = 0.67665465\n",
      "Iteration 5087, loss = 0.67665207\n",
      "Iteration 5088, loss = 0.67664335\n",
      "Iteration 5089, loss = 0.67664076\n",
      "Iteration 5090, loss = 0.67664244\n",
      "Iteration 5091, loss = 0.67663469\n",
      "Iteration 5092, loss = 0.67663676\n",
      "Iteration 5093, loss = 0.67662509\n",
      "Iteration 5094, loss = 0.67662258\n",
      "Iteration 5095, loss = 0.67661656\n",
      "Iteration 5096, loss = 0.67661265\n",
      "Iteration 5097, loss = 0.67660799\n",
      "Iteration 5098, loss = 0.67660219\n",
      "Iteration 5099, loss = 0.67659769\n",
      "Iteration 5100, loss = 0.67660313\n",
      "Iteration 5101, loss = 0.67658955\n",
      "Iteration 5102, loss = 0.67658456\n",
      "Iteration 5103, loss = 0.67658035\n",
      "Iteration 5104, loss = 0.67657837\n",
      "Iteration 5105, loss = 0.67657388\n",
      "Iteration 5106, loss = 0.67657337\n",
      "Iteration 5107, loss = 0.67656459\n",
      "Iteration 5108, loss = 0.67656321\n",
      "Iteration 5109, loss = 0.67655885\n",
      "Iteration 5110, loss = 0.67655204\n",
      "Iteration 5111, loss = 0.67654820\n",
      "Iteration 5112, loss = 0.67654573\n",
      "Iteration 5113, loss = 0.67653776\n",
      "Iteration 5114, loss = 0.67654081\n",
      "Iteration 5115, loss = 0.67652960\n",
      "Iteration 5116, loss = 0.67652632\n",
      "Iteration 5117, loss = 0.67652797\n",
      "Iteration 5118, loss = 0.67651942\n",
      "Iteration 5119, loss = 0.67651604\n",
      "Iteration 5120, loss = 0.67651224\n",
      "Iteration 5121, loss = 0.67650629\n",
      "Iteration 5122, loss = 0.67650156\n",
      "Iteration 5123, loss = 0.67649953\n",
      "Iteration 5124, loss = 0.67649863\n",
      "Iteration 5125, loss = 0.67648876\n",
      "Iteration 5126, loss = 0.67648563\n",
      "Iteration 5127, loss = 0.67648033\n",
      "Iteration 5128, loss = 0.67647510\n",
      "Iteration 5129, loss = 0.67647871\n",
      "Iteration 5130, loss = 0.67646739\n",
      "Iteration 5131, loss = 0.67646049\n",
      "Iteration 5132, loss = 0.67645680\n",
      "Iteration 5133, loss = 0.67645789\n",
      "Iteration 5134, loss = 0.67645242\n",
      "Iteration 5135, loss = 0.67645179\n",
      "Iteration 5136, loss = 0.67644042\n",
      "Iteration 5137, loss = 0.67643805\n",
      "Iteration 5138, loss = 0.67643153\n",
      "Iteration 5139, loss = 0.67642741\n",
      "Iteration 5140, loss = 0.67642155\n",
      "Iteration 5141, loss = 0.67641923\n",
      "Iteration 5142, loss = 0.67641462\n",
      "Iteration 5143, loss = 0.67641310\n",
      "Iteration 5144, loss = 0.67640824\n",
      "Iteration 5145, loss = 0.67641837\n",
      "Iteration 5146, loss = 0.67639478\n",
      "Iteration 5147, loss = 0.67639325\n",
      "Iteration 5148, loss = 0.67638824\n",
      "Iteration 5149, loss = 0.67638286\n",
      "Iteration 5150, loss = 0.67637681\n",
      "Iteration 5151, loss = 0.67637258\n",
      "Iteration 5152, loss = 0.67636811\n",
      "Iteration 5153, loss = 0.67637277\n",
      "Iteration 5154, loss = 0.67636022\n",
      "Iteration 5155, loss = 0.67636311\n",
      "Iteration 5156, loss = 0.67635776\n",
      "Iteration 5157, loss = 0.67635073\n",
      "Iteration 5158, loss = 0.67634281\n",
      "Iteration 5159, loss = 0.67633800\n",
      "Iteration 5160, loss = 0.67633307\n",
      "Iteration 5161, loss = 0.67633013\n",
      "Iteration 5162, loss = 0.67632296\n",
      "Iteration 5163, loss = 0.67632818\n",
      "Iteration 5164, loss = 0.67631588\n",
      "Iteration 5165, loss = 0.67631500\n",
      "Iteration 5166, loss = 0.67630752\n",
      "Iteration 5167, loss = 0.67630891\n",
      "Iteration 5168, loss = 0.67630041\n",
      "Iteration 5169, loss = 0.67629555\n",
      "Iteration 5170, loss = 0.67629124\n",
      "Iteration 5171, loss = 0.67628800\n",
      "Iteration 5172, loss = 0.67628541\n",
      "Iteration 5173, loss = 0.67627908\n",
      "Iteration 5174, loss = 0.67627536\n",
      "Iteration 5175, loss = 0.67627269\n",
      "Iteration 5176, loss = 0.67626514\n",
      "Iteration 5177, loss = 0.67626734\n",
      "Iteration 5178, loss = 0.67625648\n",
      "Iteration 5179, loss = 0.67625208\n",
      "Iteration 5180, loss = 0.67625146\n",
      "Iteration 5181, loss = 0.67624379\n",
      "Iteration 5182, loss = 0.67624055\n",
      "Iteration 5183, loss = 0.67623575\n",
      "Iteration 5184, loss = 0.67623801\n",
      "Iteration 5185, loss = 0.67623097\n",
      "Iteration 5186, loss = 0.67622179\n",
      "Iteration 5187, loss = 0.67621938\n",
      "Iteration 5188, loss = 0.67621208\n",
      "Iteration 5189, loss = 0.67621757\n",
      "Iteration 5190, loss = 0.67621102\n",
      "Iteration 5191, loss = 0.67620352\n",
      "Iteration 5192, loss = 0.67620037\n",
      "Iteration 5193, loss = 0.67619319\n",
      "Iteration 5194, loss = 0.67618992\n",
      "Iteration 5195, loss = 0.67618473\n",
      "Iteration 5196, loss = 0.67618309\n",
      "Iteration 5197, loss = 0.67617865\n",
      "Iteration 5198, loss = 0.67616924\n",
      "Iteration 5199, loss = 0.67617347\n",
      "Iteration 5200, loss = 0.67616264\n",
      "Iteration 5201, loss = 0.67615832\n",
      "Iteration 5202, loss = 0.67615470\n",
      "Iteration 5203, loss = 0.67614839\n",
      "Iteration 5204, loss = 0.67614466\n",
      "Iteration 5205, loss = 0.67614230\n",
      "Iteration 5206, loss = 0.67613595\n",
      "Iteration 5207, loss = 0.67613193\n",
      "Iteration 5208, loss = 0.67612840\n",
      "Iteration 5209, loss = 0.67611837\n",
      "Iteration 5210, loss = 0.67611562\n",
      "Iteration 5211, loss = 0.67611285\n",
      "Iteration 5212, loss = 0.67610846\n",
      "Iteration 5213, loss = 0.67610434\n",
      "Iteration 5214, loss = 0.67610046\n",
      "Iteration 5215, loss = 0.67609666\n",
      "Iteration 5216, loss = 0.67609252\n",
      "Iteration 5217, loss = 0.67608228\n",
      "Iteration 5218, loss = 0.67607623\n",
      "Iteration 5219, loss = 0.67607989\n",
      "Iteration 5220, loss = 0.67606962\n",
      "Iteration 5221, loss = 0.67607007\n",
      "Iteration 5222, loss = 0.67606284\n",
      "Iteration 5223, loss = 0.67606191\n",
      "Iteration 5224, loss = 0.67604879\n",
      "Iteration 5225, loss = 0.67604906\n",
      "Iteration 5226, loss = 0.67603669\n",
      "Iteration 5227, loss = 0.67603333\n",
      "Iteration 5228, loss = 0.67603363\n",
      "Iteration 5229, loss = 0.67602933\n",
      "Iteration 5230, loss = 0.67603272\n",
      "Iteration 5231, loss = 0.67602552\n",
      "Iteration 5232, loss = 0.67601660\n",
      "Iteration 5233, loss = 0.67600777\n",
      "Iteration 5234, loss = 0.67600289\n",
      "Iteration 5235, loss = 0.67599746\n",
      "Iteration 5236, loss = 0.67599650\n",
      "Iteration 5237, loss = 0.67599459\n",
      "Iteration 5238, loss = 0.67599262\n",
      "Iteration 5239, loss = 0.67598055\n",
      "Iteration 5240, loss = 0.67597793\n",
      "Iteration 5241, loss = 0.67598210\n",
      "Iteration 5242, loss = 0.67597558\n",
      "Iteration 5243, loss = 0.67597453\n",
      "Iteration 5244, loss = 0.67596326\n",
      "Iteration 5245, loss = 0.67595605\n",
      "Iteration 5246, loss = 0.67596570\n",
      "Iteration 5247, loss = 0.67595072\n",
      "Iteration 5248, loss = 0.67594320\n",
      "Iteration 5249, loss = 0.67594791\n",
      "Iteration 5250, loss = 0.67593572\n",
      "Iteration 5251, loss = 0.67593050\n",
      "Iteration 5252, loss = 0.67592904\n",
      "Iteration 5253, loss = 0.67592598\n",
      "Iteration 5254, loss = 0.67592698\n",
      "Iteration 5255, loss = 0.67591656\n",
      "Iteration 5256, loss = 0.67591768\n",
      "Iteration 5257, loss = 0.67590705\n",
      "Iteration 5258, loss = 0.67590272\n",
      "Iteration 5259, loss = 0.67589875\n",
      "Iteration 5260, loss = 0.67589616\n",
      "Iteration 5261, loss = 0.67589091\n",
      "Iteration 5262, loss = 0.67588892\n",
      "Iteration 5263, loss = 0.67587941\n",
      "Iteration 5264, loss = 0.67587837\n",
      "Iteration 5265, loss = 0.67587234\n",
      "Iteration 5266, loss = 0.67586695\n",
      "Iteration 5267, loss = 0.67586530\n",
      "Iteration 5268, loss = 0.67587047\n",
      "Iteration 5269, loss = 0.67585825\n",
      "Iteration 5270, loss = 0.67585130\n",
      "Iteration 5271, loss = 0.67584799\n",
      "Iteration 5272, loss = 0.67585357\n",
      "Iteration 5273, loss = 0.67585591\n",
      "Iteration 5274, loss = 0.67583876\n",
      "Iteration 5275, loss = 0.67583496\n",
      "Iteration 5276, loss = 0.67583371\n",
      "Iteration 5277, loss = 0.67582895\n",
      "Iteration 5278, loss = 0.67582256\n",
      "Iteration 5279, loss = 0.67582666\n",
      "Iteration 5280, loss = 0.67581159\n",
      "Iteration 5281, loss = 0.67581267\n",
      "Iteration 5282, loss = 0.67580562\n",
      "Iteration 5283, loss = 0.67580065\n",
      "Iteration 5284, loss = 0.67579914\n",
      "Iteration 5285, loss = 0.67579232\n",
      "Iteration 5286, loss = 0.67579371\n",
      "Iteration 5287, loss = 0.67578167\n",
      "Iteration 5288, loss = 0.67578405\n",
      "Iteration 5289, loss = 0.67577634\n",
      "Iteration 5290, loss = 0.67577244\n",
      "Iteration 5291, loss = 0.67578641\n",
      "Iteration 5292, loss = 0.67576120\n",
      "Iteration 5293, loss = 0.67576064\n",
      "Iteration 5294, loss = 0.67575600\n",
      "Iteration 5295, loss = 0.67575925\n",
      "Iteration 5296, loss = 0.67574588\n",
      "Iteration 5297, loss = 0.67574130\n",
      "Iteration 5298, loss = 0.67574025\n",
      "Iteration 5299, loss = 0.67573310\n",
      "Iteration 5300, loss = 0.67573589\n",
      "Iteration 5301, loss = 0.67573152\n",
      "Iteration 5302, loss = 0.67572636\n",
      "Iteration 5303, loss = 0.67572882\n",
      "Iteration 5304, loss = 0.67571541\n",
      "Iteration 5305, loss = 0.67570974\n",
      "Iteration 5306, loss = 0.67570963\n",
      "Iteration 5307, loss = 0.67570092\n",
      "Iteration 5308, loss = 0.67570960\n",
      "Iteration 5309, loss = 0.67569452\n",
      "Iteration 5310, loss = 0.67569073\n",
      "Iteration 5311, loss = 0.67568299\n",
      "Iteration 5312, loss = 0.67568666\n",
      "Iteration 5313, loss = 0.67567467\n",
      "Iteration 5314, loss = 0.67567942\n",
      "Iteration 5315, loss = 0.67567182\n",
      "Iteration 5316, loss = 0.67566293\n",
      "Iteration 5317, loss = 0.67566254\n",
      "Iteration 5318, loss = 0.67565334\n",
      "Iteration 5319, loss = 0.67565141\n",
      "Iteration 5320, loss = 0.67564382\n",
      "Iteration 5321, loss = 0.67564392\n",
      "Iteration 5322, loss = 0.67563742\n",
      "Iteration 5323, loss = 0.67563329\n",
      "Iteration 5324, loss = 0.67563358\n",
      "Iteration 5325, loss = 0.67562503\n",
      "Iteration 5326, loss = 0.67562246\n",
      "Iteration 5327, loss = 0.67562072\n",
      "Iteration 5328, loss = 0.67560826\n",
      "Iteration 5329, loss = 0.67560607\n",
      "Iteration 5330, loss = 0.67559890\n",
      "Iteration 5331, loss = 0.67559790\n",
      "Iteration 5332, loss = 0.67559503\n",
      "Iteration 5333, loss = 0.67558836\n",
      "Iteration 5334, loss = 0.67558602\n",
      "Iteration 5335, loss = 0.67558129\n",
      "Iteration 5336, loss = 0.67557881\n",
      "Iteration 5337, loss = 0.67557150\n",
      "Iteration 5338, loss = 0.67556686\n",
      "Iteration 5339, loss = 0.67556886\n",
      "Iteration 5340, loss = 0.67556223\n",
      "Iteration 5341, loss = 0.67555709\n",
      "Iteration 5342, loss = 0.67556171\n",
      "Iteration 5343, loss = 0.67554382\n",
      "Iteration 5344, loss = 0.67554185\n",
      "Iteration 5345, loss = 0.67553521\n",
      "Iteration 5346, loss = 0.67553235\n",
      "Iteration 5347, loss = 0.67552949\n",
      "Iteration 5348, loss = 0.67552230\n",
      "Iteration 5349, loss = 0.67552476\n",
      "Iteration 5350, loss = 0.67551385\n",
      "Iteration 5351, loss = 0.67551357\n",
      "Iteration 5352, loss = 0.67550572\n",
      "Iteration 5353, loss = 0.67550630\n",
      "Iteration 5354, loss = 0.67549771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5355, loss = 0.67549348\n",
      "Iteration 5356, loss = 0.67549182\n",
      "Iteration 5357, loss = 0.67548186\n",
      "Iteration 5358, loss = 0.67547980\n",
      "Iteration 5359, loss = 0.67547226\n",
      "Iteration 5360, loss = 0.67548852\n",
      "Iteration 5361, loss = 0.67546363\n",
      "Iteration 5362, loss = 0.67546209\n",
      "Iteration 5363, loss = 0.67546349\n",
      "Iteration 5364, loss = 0.67544900\n",
      "Iteration 5365, loss = 0.67544922\n",
      "Iteration 5366, loss = 0.67544359\n",
      "Iteration 5367, loss = 0.67544041\n",
      "Iteration 5368, loss = 0.67542976\n",
      "Iteration 5369, loss = 0.67542971\n",
      "Iteration 5370, loss = 0.67542414\n",
      "Iteration 5371, loss = 0.67541988\n",
      "Iteration 5372, loss = 0.67541315\n",
      "Iteration 5373, loss = 0.67541049\n",
      "Iteration 5374, loss = 0.67540298\n",
      "Iteration 5375, loss = 0.67540224\n",
      "Iteration 5376, loss = 0.67539493\n",
      "Iteration 5377, loss = 0.67538942\n",
      "Iteration 5378, loss = 0.67538551\n",
      "Iteration 5379, loss = 0.67537839\n",
      "Iteration 5380, loss = 0.67538011\n",
      "Iteration 5381, loss = 0.67537062\n",
      "Iteration 5382, loss = 0.67536383\n",
      "Iteration 5383, loss = 0.67536382\n",
      "Iteration 5384, loss = 0.67535630\n",
      "Iteration 5385, loss = 0.67535572\n",
      "Iteration 5386, loss = 0.67534394\n",
      "Iteration 5387, loss = 0.67533939\n",
      "Iteration 5388, loss = 0.67533224\n",
      "Iteration 5389, loss = 0.67532522\n",
      "Iteration 5390, loss = 0.67532320\n",
      "Iteration 5391, loss = 0.67532002\n",
      "Iteration 5392, loss = 0.67531258\n",
      "Iteration 5393, loss = 0.67530735\n",
      "Iteration 5394, loss = 0.67530200\n",
      "Iteration 5395, loss = 0.67530202\n",
      "Iteration 5396, loss = 0.67529474\n",
      "Iteration 5397, loss = 0.67528363\n",
      "Iteration 5398, loss = 0.67528024\n",
      "Iteration 5399, loss = 0.67526992\n",
      "Iteration 5400, loss = 0.67526981\n",
      "Iteration 5401, loss = 0.67525992\n",
      "Iteration 5402, loss = 0.67526160\n",
      "Iteration 5403, loss = 0.67526008\n",
      "Iteration 5404, loss = 0.67524243\n",
      "Iteration 5405, loss = 0.67523830\n",
      "Iteration 5406, loss = 0.67524094\n",
      "Iteration 5407, loss = 0.67522871\n",
      "Iteration 5408, loss = 0.67523492\n",
      "Iteration 5409, loss = 0.67521456\n",
      "Iteration 5410, loss = 0.67520894\n",
      "Iteration 5411, loss = 0.67521106\n",
      "Iteration 5412, loss = 0.67519883\n",
      "Iteration 5413, loss = 0.67520500\n",
      "Iteration 5414, loss = 0.67519415\n",
      "Iteration 5415, loss = 0.67518436\n",
      "Iteration 5416, loss = 0.67518085\n",
      "Iteration 5417, loss = 0.67517649\n",
      "Iteration 5418, loss = 0.67516406\n",
      "Iteration 5419, loss = 0.67516515\n",
      "Iteration 5420, loss = 0.67515066\n",
      "Iteration 5421, loss = 0.67515016\n",
      "Iteration 5422, loss = 0.67514309\n",
      "Iteration 5423, loss = 0.67513691\n",
      "Iteration 5424, loss = 0.67516376\n",
      "Iteration 5425, loss = 0.67512599\n",
      "Iteration 5426, loss = 0.67512937\n",
      "Iteration 5427, loss = 0.67511890\n",
      "Iteration 5428, loss = 0.67512116\n",
      "Iteration 5429, loss = 0.67510353\n",
      "Iteration 5430, loss = 0.67510551\n",
      "Iteration 5431, loss = 0.67509416\n",
      "Iteration 5432, loss = 0.67508894\n",
      "Iteration 5433, loss = 0.67508158\n",
      "Iteration 5434, loss = 0.67507292\n",
      "Iteration 5435, loss = 0.67506961\n",
      "Iteration 5436, loss = 0.67506440\n",
      "Iteration 5437, loss = 0.67505846\n",
      "Iteration 5438, loss = 0.67505518\n",
      "Iteration 5439, loss = 0.67504598\n",
      "Iteration 5440, loss = 0.67503969\n",
      "Iteration 5441, loss = 0.67503449\n",
      "Iteration 5442, loss = 0.67502984\n",
      "Iteration 5443, loss = 0.67502216\n",
      "Iteration 5444, loss = 0.67501952\n",
      "Iteration 5445, loss = 0.67501465\n",
      "Iteration 5446, loss = 0.67500903\n",
      "Iteration 5447, loss = 0.67499962\n",
      "Iteration 5448, loss = 0.67499780\n",
      "Iteration 5449, loss = 0.67499344\n",
      "Iteration 5450, loss = 0.67498453\n",
      "Iteration 5451, loss = 0.67498071\n",
      "Iteration 5452, loss = 0.67497430\n",
      "Iteration 5453, loss = 0.67496910\n",
      "Iteration 5454, loss = 0.67496808\n",
      "Iteration 5455, loss = 0.67496385\n",
      "Iteration 5456, loss = 0.67495231\n",
      "Iteration 5457, loss = 0.67494808\n",
      "Iteration 5458, loss = 0.67494428\n",
      "Iteration 5459, loss = 0.67493712\n",
      "Iteration 5460, loss = 0.67493174\n",
      "Iteration 5461, loss = 0.67492636\n",
      "Iteration 5462, loss = 0.67492673\n",
      "Iteration 5463, loss = 0.67491525\n",
      "Iteration 5464, loss = 0.67491022\n",
      "Iteration 5465, loss = 0.67490558\n",
      "Iteration 5466, loss = 0.67490262\n",
      "Iteration 5467, loss = 0.67489655\n",
      "Iteration 5468, loss = 0.67488794\n",
      "Iteration 5469, loss = 0.67488855\n",
      "Iteration 5470, loss = 0.67487880\n",
      "Iteration 5471, loss = 0.67487352\n",
      "Iteration 5472, loss = 0.67487260\n",
      "Iteration 5473, loss = 0.67487038\n",
      "Iteration 5474, loss = 0.67485624\n",
      "Iteration 5475, loss = 0.67485300\n",
      "Iteration 5476, loss = 0.67484890\n",
      "Iteration 5477, loss = 0.67484365\n",
      "Iteration 5478, loss = 0.67484005\n",
      "Iteration 5479, loss = 0.67483230\n",
      "Iteration 5480, loss = 0.67483033\n",
      "Iteration 5481, loss = 0.67482459\n",
      "Iteration 5482, loss = 0.67481999\n",
      "Iteration 5483, loss = 0.67481903\n",
      "Iteration 5484, loss = 0.67481005\n",
      "Iteration 5485, loss = 0.67480789\n",
      "Iteration 5486, loss = 0.67479808\n",
      "Iteration 5487, loss = 0.67479956\n",
      "Iteration 5488, loss = 0.67478655\n",
      "Iteration 5489, loss = 0.67477932\n",
      "Iteration 5490, loss = 0.67477728\n",
      "Iteration 5491, loss = 0.67476858\n",
      "Iteration 5492, loss = 0.67477089\n",
      "Iteration 5493, loss = 0.67476469\n",
      "Iteration 5494, loss = 0.67475685\n",
      "Iteration 5495, loss = 0.67475692\n",
      "Iteration 5496, loss = 0.67475132\n",
      "Iteration 5497, loss = 0.67473776\n",
      "Iteration 5498, loss = 0.67473409\n",
      "Iteration 5499, loss = 0.67472904\n",
      "Iteration 5500, loss = 0.67471969\n",
      "Iteration 5501, loss = 0.67472048\n",
      "Iteration 5502, loss = 0.67471997\n",
      "Iteration 5503, loss = 0.67470714\n",
      "Iteration 5504, loss = 0.67470821\n",
      "Iteration 5505, loss = 0.67469293\n",
      "Iteration 5506, loss = 0.67469071\n",
      "Iteration 5507, loss = 0.67468636\n",
      "Iteration 5508, loss = 0.67468201\n",
      "Iteration 5509, loss = 0.67467485\n",
      "Iteration 5510, loss = 0.67467332\n",
      "Iteration 5511, loss = 0.67466669\n",
      "Iteration 5512, loss = 0.67465728\n",
      "Iteration 5513, loss = 0.67465349\n",
      "Iteration 5514, loss = 0.67466123\n",
      "Iteration 5515, loss = 0.67464316\n",
      "Iteration 5516, loss = 0.67463910\n",
      "Iteration 5517, loss = 0.67463115\n",
      "Iteration 5518, loss = 0.67462785\n",
      "Iteration 5519, loss = 0.67462248\n",
      "Iteration 5520, loss = 0.67461611\n",
      "Iteration 5521, loss = 0.67461310\n",
      "Iteration 5522, loss = 0.67460373\n",
      "Iteration 5523, loss = 0.67461062\n",
      "Iteration 5524, loss = 0.67459388\n",
      "Iteration 5525, loss = 0.67458665\n",
      "Iteration 5526, loss = 0.67458344\n",
      "Iteration 5527, loss = 0.67457921\n",
      "Iteration 5528, loss = 0.67457104\n",
      "Iteration 5529, loss = 0.67456398\n",
      "Iteration 5530, loss = 0.67455928\n",
      "Iteration 5531, loss = 0.67455149\n",
      "Iteration 5532, loss = 0.67454798\n",
      "Iteration 5533, loss = 0.67454121\n",
      "Iteration 5534, loss = 0.67453668\n",
      "Iteration 5535, loss = 0.67453994\n",
      "Iteration 5536, loss = 0.67453030\n",
      "Iteration 5537, loss = 0.67452024\n",
      "Iteration 5538, loss = 0.67451242\n",
      "Iteration 5539, loss = 0.67450973\n",
      "Iteration 5540, loss = 0.67450491\n",
      "Iteration 5541, loss = 0.67450034\n",
      "Iteration 5542, loss = 0.67449887\n",
      "Iteration 5543, loss = 0.67448741\n",
      "Iteration 5544, loss = 0.67448960\n",
      "Iteration 5545, loss = 0.67447507\n",
      "Iteration 5546, loss = 0.67447063\n",
      "Iteration 5547, loss = 0.67446929\n",
      "Iteration 5548, loss = 0.67446282\n",
      "Iteration 5549, loss = 0.67445590\n",
      "Iteration 5550, loss = 0.67444984\n",
      "Iteration 5551, loss = 0.67444991\n",
      "Iteration 5552, loss = 0.67443999\n",
      "Iteration 5553, loss = 0.67443973\n",
      "Iteration 5554, loss = 0.67442499\n",
      "Iteration 5555, loss = 0.67442121\n",
      "Iteration 5556, loss = 0.67442575\n",
      "Iteration 5557, loss = 0.67441271\n",
      "Iteration 5558, loss = 0.67440622\n",
      "Iteration 5559, loss = 0.67440151\n",
      "Iteration 5560, loss = 0.67440817\n",
      "Iteration 5561, loss = 0.67439715\n",
      "Iteration 5562, loss = 0.67438277\n",
      "Iteration 5563, loss = 0.67437953\n",
      "Iteration 5564, loss = 0.67437410\n",
      "Iteration 5565, loss = 0.67436742\n",
      "Iteration 5566, loss = 0.67436189\n",
      "Iteration 5567, loss = 0.67436060\n",
      "Iteration 5568, loss = 0.67435046\n",
      "Iteration 5569, loss = 0.67435360\n",
      "Iteration 5570, loss = 0.67434207\n",
      "Iteration 5571, loss = 0.67433506\n",
      "Iteration 5572, loss = 0.67433197\n",
      "Iteration 5573, loss = 0.67432787\n",
      "Iteration 5574, loss = 0.67431888\n",
      "Iteration 5575, loss = 0.67431387\n",
      "Iteration 5576, loss = 0.67431518\n",
      "Iteration 5577, loss = 0.67430254\n",
      "Iteration 5578, loss = 0.67429680\n",
      "Iteration 5579, loss = 0.67429233\n",
      "Iteration 5580, loss = 0.67428956\n",
      "Iteration 5581, loss = 0.67427874\n",
      "Iteration 5582, loss = 0.67427340\n",
      "Iteration 5583, loss = 0.67427202\n",
      "Iteration 5584, loss = 0.67427372\n",
      "Iteration 5585, loss = 0.67425898\n",
      "Iteration 5586, loss = 0.67425547\n",
      "Iteration 5587, loss = 0.67424725\n",
      "Iteration 5588, loss = 0.67424119\n",
      "Iteration 5589, loss = 0.67424761\n",
      "Iteration 5590, loss = 0.67424015\n",
      "Iteration 5591, loss = 0.67423294\n",
      "Iteration 5592, loss = 0.67422669\n",
      "Iteration 5593, loss = 0.67422106\n",
      "Iteration 5594, loss = 0.67420955\n",
      "Iteration 5595, loss = 0.67420406\n",
      "Iteration 5596, loss = 0.67420527\n",
      "Iteration 5597, loss = 0.67419329\n",
      "Iteration 5598, loss = 0.67419050\n",
      "Iteration 5599, loss = 0.67418471\n",
      "Iteration 5600, loss = 0.67417832\n",
      "Iteration 5601, loss = 0.67417470\n",
      "Iteration 5602, loss = 0.67416964\n",
      "Iteration 5603, loss = 0.67416206\n",
      "Iteration 5604, loss = 0.67416442\n",
      "Iteration 5605, loss = 0.67415146\n",
      "Iteration 5606, loss = 0.67415020\n",
      "Iteration 5607, loss = 0.67413977\n",
      "Iteration 5608, loss = 0.67413916\n",
      "Iteration 5609, loss = 0.67413826\n",
      "Iteration 5610, loss = 0.67412546\n",
      "Iteration 5611, loss = 0.67412064\n",
      "Iteration 5612, loss = 0.67411440\n",
      "Iteration 5613, loss = 0.67411265\n",
      "Iteration 5614, loss = 0.67411082\n",
      "Iteration 5615, loss = 0.67410194\n",
      "Iteration 5616, loss = 0.67409360\n",
      "Iteration 5617, loss = 0.67409104\n",
      "Iteration 5618, loss = 0.67409027\n",
      "Iteration 5619, loss = 0.67408378\n",
      "Iteration 5620, loss = 0.67409033\n",
      "Iteration 5621, loss = 0.67407112\n",
      "Iteration 5622, loss = 0.67406802\n",
      "Iteration 5623, loss = 0.67406117\n",
      "Iteration 5624, loss = 0.67406048\n",
      "Iteration 5625, loss = 0.67406344\n",
      "Iteration 5626, loss = 0.67404947\n",
      "Iteration 5627, loss = 0.67404248\n",
      "Iteration 5628, loss = 0.67404829\n",
      "Iteration 5629, loss = 0.67403935\n",
      "Iteration 5630, loss = 0.67403570\n",
      "Iteration 5631, loss = 0.67402586\n",
      "Iteration 5632, loss = 0.67402518\n",
      "Iteration 5633, loss = 0.67401512\n",
      "Iteration 5634, loss = 0.67401344\n",
      "Iteration 5635, loss = 0.67401579\n",
      "Iteration 5636, loss = 0.67400408\n",
      "Iteration 5637, loss = 0.67400213\n",
      "Iteration 5638, loss = 0.67401231\n",
      "Iteration 5639, loss = 0.67398795\n",
      "Iteration 5640, loss = 0.67398572\n",
      "Iteration 5641, loss = 0.67398024\n",
      "Iteration 5642, loss = 0.67398376\n",
      "Iteration 5643, loss = 0.67397460\n",
      "Iteration 5644, loss = 0.67397085\n",
      "Iteration 5645, loss = 0.67396345\n",
      "Iteration 5646, loss = 0.67395748\n",
      "Iteration 5647, loss = 0.67395388\n",
      "Iteration 5648, loss = 0.67396047\n",
      "Iteration 5649, loss = 0.67394455\n",
      "Iteration 5650, loss = 0.67394615\n",
      "Iteration 5651, loss = 0.67393484\n",
      "Iteration 5652, loss = 0.67393066\n",
      "Iteration 5653, loss = 0.67392616\n",
      "Iteration 5654, loss = 0.67392424\n",
      "Iteration 5655, loss = 0.67391748\n",
      "Iteration 5656, loss = 0.67391511\n",
      "Iteration 5657, loss = 0.67390794\n",
      "Iteration 5658, loss = 0.67390115\n",
      "Iteration 5659, loss = 0.67389965\n",
      "Iteration 5660, loss = 0.67389307\n",
      "Iteration 5661, loss = 0.67389174\n",
      "Iteration 5662, loss = 0.67388833\n",
      "Iteration 5663, loss = 0.67388842\n",
      "Iteration 5664, loss = 0.67388486\n",
      "Iteration 5665, loss = 0.67386778\n",
      "Iteration 5666, loss = 0.67386429\n",
      "Iteration 5667, loss = 0.67385954\n",
      "Iteration 5668, loss = 0.67386174\n",
      "Iteration 5669, loss = 0.67385589\n",
      "Iteration 5670, loss = 0.67385276\n",
      "Iteration 5671, loss = 0.67384295\n",
      "Iteration 5672, loss = 0.67383606\n",
      "Iteration 5673, loss = 0.67383164\n",
      "Iteration 5674, loss = 0.67383223\n",
      "Iteration 5675, loss = 0.67382733\n",
      "Iteration 5676, loss = 0.67384311\n",
      "Iteration 5677, loss = 0.67381671\n",
      "Iteration 5678, loss = 0.67381038\n",
      "Iteration 5679, loss = 0.67380779\n",
      "Iteration 5680, loss = 0.67380227\n",
      "Iteration 5681, loss = 0.67379544\n",
      "Iteration 5682, loss = 0.67379010\n",
      "Iteration 5683, loss = 0.67379056\n",
      "Iteration 5684, loss = 0.67378581\n",
      "Iteration 5685, loss = 0.67378123\n",
      "Iteration 5686, loss = 0.67377467\n",
      "Iteration 5687, loss = 0.67376725\n",
      "Iteration 5688, loss = 0.67377520\n",
      "Iteration 5689, loss = 0.67376183\n",
      "Iteration 5690, loss = 0.67375875\n",
      "Iteration 5691, loss = 0.67375024\n",
      "Iteration 5692, loss = 0.67374730\n",
      "Iteration 5693, loss = 0.67374295\n",
      "Iteration 5694, loss = 0.67373584\n",
      "Iteration 5695, loss = 0.67373290\n",
      "Iteration 5696, loss = 0.67372997\n",
      "Iteration 5697, loss = 0.67372688\n",
      "Iteration 5698, loss = 0.67371677\n",
      "Iteration 5699, loss = 0.67371734\n",
      "Iteration 5700, loss = 0.67371080\n",
      "Iteration 5701, loss = 0.67370322\n",
      "Iteration 5702, loss = 0.67369949\n",
      "Iteration 5703, loss = 0.67369454\n",
      "Iteration 5704, loss = 0.67369429\n",
      "Iteration 5705, loss = 0.67368221\n",
      "Iteration 5706, loss = 0.67367736\n",
      "Iteration 5707, loss = 0.67367552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5708, loss = 0.67367737\n",
      "Iteration 5709, loss = 0.67366203\n",
      "Iteration 5710, loss = 0.67366707\n",
      "Iteration 5711, loss = 0.67365949\n",
      "Iteration 5712, loss = 0.67365169\n",
      "Iteration 5713, loss = 0.67364380\n",
      "Iteration 5714, loss = 0.67364084\n",
      "Iteration 5715, loss = 0.67363920\n",
      "Iteration 5716, loss = 0.67363028\n",
      "Iteration 5717, loss = 0.67363990\n",
      "Iteration 5718, loss = 0.67362043\n",
      "Iteration 5719, loss = 0.67361775\n",
      "Iteration 5720, loss = 0.67361319\n",
      "Iteration 5721, loss = 0.67360463\n",
      "Iteration 5722, loss = 0.67359833\n",
      "Iteration 5723, loss = 0.67360455\n",
      "Iteration 5724, loss = 0.67358905\n",
      "Iteration 5725, loss = 0.67358819\n",
      "Iteration 5726, loss = 0.67359187\n",
      "Iteration 5727, loss = 0.67357523\n",
      "Iteration 5728, loss = 0.67357991\n",
      "Iteration 5729, loss = 0.67356453\n",
      "Iteration 5730, loss = 0.67355993\n",
      "Iteration 5731, loss = 0.67355701\n",
      "Iteration 5732, loss = 0.67355282\n",
      "Iteration 5733, loss = 0.67354181\n",
      "Iteration 5734, loss = 0.67354343\n",
      "Iteration 5735, loss = 0.67353592\n",
      "Iteration 5736, loss = 0.67353412\n",
      "Iteration 5737, loss = 0.67352538\n",
      "Iteration 5738, loss = 0.67352084\n",
      "Iteration 5739, loss = 0.67351469\n",
      "Iteration 5740, loss = 0.67350813\n",
      "Iteration 5741, loss = 0.67350349\n",
      "Iteration 5742, loss = 0.67349798\n",
      "Iteration 5743, loss = 0.67349929\n",
      "Iteration 5744, loss = 0.67348937\n",
      "Iteration 5745, loss = 0.67348839\n",
      "Iteration 5746, loss = 0.67347619\n",
      "Iteration 5747, loss = 0.67347376\n",
      "Iteration 5748, loss = 0.67346867\n",
      "Iteration 5749, loss = 0.67346243\n",
      "Iteration 5750, loss = 0.67345835\n",
      "Iteration 5751, loss = 0.67345108\n",
      "Iteration 5752, loss = 0.67345141\n",
      "Iteration 5753, loss = 0.67344927\n",
      "Iteration 5754, loss = 0.67343515\n",
      "Iteration 5755, loss = 0.67343902\n",
      "Iteration 5756, loss = 0.67342571\n",
      "Iteration 5757, loss = 0.67342310\n",
      "Iteration 5758, loss = 0.67341831\n",
      "Iteration 5759, loss = 0.67341094\n",
      "Iteration 5760, loss = 0.67340663\n",
      "Iteration 5761, loss = 0.67340415\n",
      "Iteration 5762, loss = 0.67339575\n",
      "Iteration 5763, loss = 0.67339397\n",
      "Iteration 5764, loss = 0.67338644\n",
      "Iteration 5765, loss = 0.67338069\n",
      "Iteration 5766, loss = 0.67337391\n",
      "Iteration 5767, loss = 0.67337249\n",
      "Iteration 5768, loss = 0.67336931\n",
      "Iteration 5769, loss = 0.67336144\n",
      "Iteration 5770, loss = 0.67335546\n",
      "Iteration 5771, loss = 0.67335373\n",
      "Iteration 5772, loss = 0.67334372\n",
      "Iteration 5773, loss = 0.67334043\n",
      "Iteration 5774, loss = 0.67333401\n",
      "Iteration 5775, loss = 0.67332941\n",
      "Iteration 5776, loss = 0.67332340\n",
      "Iteration 5777, loss = 0.67331706\n",
      "Iteration 5778, loss = 0.67331467\n",
      "Iteration 5779, loss = 0.67330918\n",
      "Iteration 5780, loss = 0.67331391\n",
      "Iteration 5781, loss = 0.67329777\n",
      "Iteration 5782, loss = 0.67329739\n",
      "Iteration 5783, loss = 0.67328782\n",
      "Iteration 5784, loss = 0.67328537\n",
      "Iteration 5785, loss = 0.67327965\n",
      "Iteration 5786, loss = 0.67327427\n",
      "Iteration 5787, loss = 0.67326852\n",
      "Iteration 5788, loss = 0.67326255\n",
      "Iteration 5789, loss = 0.67326030\n",
      "Iteration 5790, loss = 0.67325929\n",
      "Iteration 5791, loss = 0.67324867\n",
      "Iteration 5792, loss = 0.67324258\n",
      "Iteration 5793, loss = 0.67323492\n",
      "Iteration 5794, loss = 0.67323850\n",
      "Iteration 5795, loss = 0.67322912\n",
      "Iteration 5796, loss = 0.67322524\n",
      "Iteration 5797, loss = 0.67321635\n",
      "Iteration 5798, loss = 0.67320994\n",
      "Iteration 5799, loss = 0.67320525\n",
      "Iteration 5800, loss = 0.67320201\n",
      "Iteration 5801, loss = 0.67319884\n",
      "Iteration 5802, loss = 0.67319363\n",
      "Iteration 5803, loss = 0.67319185\n",
      "Iteration 5804, loss = 0.67318770\n",
      "Iteration 5805, loss = 0.67317219\n",
      "Iteration 5806, loss = 0.67317251\n",
      "Iteration 5807, loss = 0.67316495\n",
      "Iteration 5808, loss = 0.67316628\n",
      "Iteration 5809, loss = 0.67316025\n",
      "Iteration 5810, loss = 0.67316180\n",
      "Iteration 5811, loss = 0.67315453\n",
      "Iteration 5812, loss = 0.67314706\n",
      "Iteration 5813, loss = 0.67313282\n",
      "Iteration 5814, loss = 0.67313990\n",
      "Iteration 5815, loss = 0.67312852\n",
      "Iteration 5816, loss = 0.67312103\n",
      "Iteration 5817, loss = 0.67311511\n",
      "Iteration 5818, loss = 0.67311759\n",
      "Iteration 5819, loss = 0.67310038\n",
      "Iteration 5820, loss = 0.67310000\n",
      "Iteration 5821, loss = 0.67309111\n",
      "Iteration 5822, loss = 0.67308988\n",
      "Iteration 5823, loss = 0.67307821\n",
      "Iteration 5824, loss = 0.67308380\n",
      "Iteration 5825, loss = 0.67307041\n",
      "Iteration 5826, loss = 0.67306527\n",
      "Iteration 5827, loss = 0.67305894\n",
      "Iteration 5828, loss = 0.67305854\n",
      "Iteration 5829, loss = 0.67305446\n",
      "Iteration 5830, loss = 0.67304671\n",
      "Iteration 5831, loss = 0.67303807\n",
      "Iteration 5832, loss = 0.67303507\n",
      "Iteration 5833, loss = 0.67302928\n",
      "Iteration 5834, loss = 0.67302499\n",
      "Iteration 5835, loss = 0.67301621\n",
      "Iteration 5836, loss = 0.67302330\n",
      "Iteration 5837, loss = 0.67300680\n",
      "Iteration 5838, loss = 0.67299925\n",
      "Iteration 5839, loss = 0.67300794\n",
      "Iteration 5840, loss = 0.67299000\n",
      "Iteration 5841, loss = 0.67298302\n",
      "Iteration 5842, loss = 0.67298173\n",
      "Iteration 5843, loss = 0.67298169\n",
      "Iteration 5844, loss = 0.67297049\n",
      "Iteration 5845, loss = 0.67296234\n",
      "Iteration 5846, loss = 0.67295661\n",
      "Iteration 5847, loss = 0.67295040\n",
      "Iteration 5848, loss = 0.67294575\n",
      "Iteration 5849, loss = 0.67295324\n",
      "Iteration 5850, loss = 0.67293884\n",
      "Iteration 5851, loss = 0.67292651\n",
      "Iteration 5852, loss = 0.67292221\n",
      "Iteration 5853, loss = 0.67292041\n",
      "Iteration 5854, loss = 0.67291563\n",
      "Iteration 5855, loss = 0.67292119\n",
      "Iteration 5856, loss = 0.67290314\n",
      "Iteration 5857, loss = 0.67289846\n",
      "Iteration 5858, loss = 0.67289652\n",
      "Iteration 5859, loss = 0.67288483\n",
      "Iteration 5860, loss = 0.67288312\n",
      "Iteration 5861, loss = 0.67288032\n",
      "Iteration 5862, loss = 0.67286968\n",
      "Iteration 5863, loss = 0.67286866\n",
      "Iteration 5864, loss = 0.67285909\n",
      "Iteration 5865, loss = 0.67285844\n",
      "Iteration 5866, loss = 0.67285197\n",
      "Iteration 5867, loss = 0.67285390\n",
      "Iteration 5868, loss = 0.67284505\n",
      "Iteration 5869, loss = 0.67284254\n",
      "Iteration 5870, loss = 0.67283448\n",
      "Iteration 5871, loss = 0.67282743\n",
      "Iteration 5872, loss = 0.67282388\n",
      "Iteration 5873, loss = 0.67281938\n",
      "Iteration 5874, loss = 0.67281925\n",
      "Iteration 5875, loss = 0.67280564\n",
      "Iteration 5876, loss = 0.67280560\n",
      "Iteration 5877, loss = 0.67279680\n",
      "Iteration 5878, loss = 0.67279045\n",
      "Iteration 5879, loss = 0.67279911\n",
      "Iteration 5880, loss = 0.67278419\n",
      "Iteration 5881, loss = 0.67277904\n",
      "Iteration 5882, loss = 0.67277975\n",
      "Iteration 5883, loss = 0.67277023\n",
      "Iteration 5884, loss = 0.67276456\n",
      "Iteration 5885, loss = 0.67276071\n",
      "Iteration 5886, loss = 0.67275284\n",
      "Iteration 5887, loss = 0.67275874\n",
      "Iteration 5888, loss = 0.67274481\n",
      "Iteration 5889, loss = 0.67273970\n",
      "Iteration 5890, loss = 0.67274116\n",
      "Iteration 5891, loss = 0.67273243\n",
      "Iteration 5892, loss = 0.67272289\n",
      "Iteration 5893, loss = 0.67271792\n",
      "Iteration 5894, loss = 0.67271932\n",
      "Iteration 5895, loss = 0.67270954\n",
      "Iteration 5896, loss = 0.67270380\n",
      "Iteration 5897, loss = 0.67271393\n",
      "Iteration 5898, loss = 0.67269755\n",
      "Iteration 5899, loss = 0.67269250\n",
      "Iteration 5900, loss = 0.67268003\n",
      "Iteration 5901, loss = 0.67268454\n",
      "Iteration 5902, loss = 0.67267284\n",
      "Iteration 5903, loss = 0.67266682\n",
      "Iteration 5904, loss = 0.67265974\n",
      "Iteration 5905, loss = 0.67265454\n",
      "Iteration 5906, loss = 0.67265106\n",
      "Iteration 5907, loss = 0.67265497\n",
      "Iteration 5908, loss = 0.67264491\n",
      "Iteration 5909, loss = 0.67263613\n",
      "Iteration 5910, loss = 0.67263194\n",
      "Iteration 5911, loss = 0.67263222\n",
      "Iteration 5912, loss = 0.67262287\n",
      "Iteration 5913, loss = 0.67262037\n",
      "Iteration 5914, loss = 0.67261076\n",
      "Iteration 5915, loss = 0.67260511\n",
      "Iteration 5916, loss = 0.67260324\n",
      "Iteration 5917, loss = 0.67260075\n",
      "Iteration 5918, loss = 0.67259559\n",
      "Iteration 5919, loss = 0.67258665\n",
      "Iteration 5920, loss = 0.67258477\n",
      "Iteration 5921, loss = 0.67258465\n",
      "Iteration 5922, loss = 0.67257241\n",
      "Iteration 5923, loss = 0.67256877\n",
      "Iteration 5924, loss = 0.67256733\n",
      "Iteration 5925, loss = 0.67255139\n",
      "Iteration 5926, loss = 0.67256247\n",
      "Iteration 5927, loss = 0.67255441\n",
      "Iteration 5928, loss = 0.67254648\n",
      "Iteration 5929, loss = 0.67253645\n",
      "Iteration 5930, loss = 0.67252701\n",
      "Iteration 5931, loss = 0.67253159\n",
      "Iteration 5932, loss = 0.67252424\n",
      "Iteration 5933, loss = 0.67252753\n",
      "Iteration 5934, loss = 0.67250552\n",
      "Iteration 5935, loss = 0.67250459\n",
      "Iteration 5936, loss = 0.67250556\n",
      "Iteration 5937, loss = 0.67248883\n",
      "Iteration 5938, loss = 0.67248478\n",
      "Iteration 5939, loss = 0.67248266\n",
      "Iteration 5940, loss = 0.67247533\n",
      "Iteration 5941, loss = 0.67247183\n",
      "Iteration 5942, loss = 0.67246346\n",
      "Iteration 5943, loss = 0.67246198\n",
      "Iteration 5944, loss = 0.67245090\n",
      "Iteration 5945, loss = 0.67245540\n",
      "Iteration 5946, loss = 0.67244138\n",
      "Iteration 5947, loss = 0.67243999\n",
      "Iteration 5948, loss = 0.67243296\n",
      "Iteration 5949, loss = 0.67243208\n",
      "Iteration 5950, loss = 0.67242238\n",
      "Iteration 5951, loss = 0.67241872\n",
      "Iteration 5952, loss = 0.67241290\n",
      "Iteration 5953, loss = 0.67240875\n",
      "Iteration 5954, loss = 0.67242034\n",
      "Iteration 5955, loss = 0.67240053\n",
      "Iteration 5956, loss = 0.67239847\n",
      "Iteration 5957, loss = 0.67238732\n",
      "Iteration 5958, loss = 0.67238317\n",
      "Iteration 5959, loss = 0.67238950\n",
      "Iteration 5960, loss = 0.67237684\n",
      "Iteration 5961, loss = 0.67236971\n",
      "Iteration 5962, loss = 0.67236389\n",
      "Iteration 5963, loss = 0.67236009\n",
      "Iteration 5964, loss = 0.67235640\n",
      "Iteration 5965, loss = 0.67235601\n",
      "Iteration 5966, loss = 0.67234593\n",
      "Iteration 5967, loss = 0.67234556\n",
      "Iteration 5968, loss = 0.67233663\n",
      "Iteration 5969, loss = 0.67233357\n",
      "Iteration 5970, loss = 0.67232480\n",
      "Iteration 5971, loss = 0.67232273\n",
      "Iteration 5972, loss = 0.67232102\n",
      "Iteration 5973, loss = 0.67231166\n",
      "Iteration 5974, loss = 0.67230790\n",
      "Iteration 5975, loss = 0.67231070\n",
      "Iteration 5976, loss = 0.67229833\n",
      "Iteration 5977, loss = 0.67229830\n",
      "Iteration 5978, loss = 0.67228895\n",
      "Iteration 5979, loss = 0.67229828\n",
      "Iteration 5980, loss = 0.67227762\n",
      "Iteration 5981, loss = 0.67228002\n",
      "Iteration 5982, loss = 0.67226954\n",
      "Iteration 5983, loss = 0.67226907\n",
      "Iteration 5984, loss = 0.67226235\n",
      "Iteration 5985, loss = 0.67225667\n",
      "Iteration 5986, loss = 0.67225760\n",
      "Iteration 5987, loss = 0.67224584\n",
      "Iteration 5988, loss = 0.67224364\n",
      "Iteration 5989, loss = 0.67223731\n",
      "Iteration 5990, loss = 0.67222972\n",
      "Iteration 5991, loss = 0.67223315\n",
      "Iteration 5992, loss = 0.67222164\n",
      "Iteration 5993, loss = 0.67222141\n",
      "Iteration 5994, loss = 0.67221311\n",
      "Iteration 5995, loss = 0.67221040\n",
      "Iteration 5996, loss = 0.67220155\n",
      "Iteration 5997, loss = 0.67220141\n",
      "Iteration 5998, loss = 0.67219393\n",
      "Iteration 5999, loss = 0.67218905\n",
      "Iteration 6000, loss = 0.67220246\n",
      "Iteration 6001, loss = 0.67217984\n",
      "Iteration 6002, loss = 0.67218265\n",
      "Iteration 6003, loss = 0.67216943\n",
      "Iteration 6004, loss = 0.67216712\n",
      "Iteration 6005, loss = 0.67217411\n",
      "Iteration 6006, loss = 0.67215457\n",
      "Iteration 6007, loss = 0.67215408\n",
      "Iteration 6008, loss = 0.67215499\n",
      "Iteration 6009, loss = 0.67217066\n",
      "Iteration 6010, loss = 0.67214770\n",
      "Iteration 6011, loss = 0.67213219\n",
      "Iteration 6012, loss = 0.67212916\n",
      "Iteration 6013, loss = 0.67212130\n",
      "Iteration 6014, loss = 0.67212981\n",
      "Iteration 6015, loss = 0.67211186\n",
      "Iteration 6016, loss = 0.67210889\n",
      "Iteration 6017, loss = 0.67210206\n",
      "Iteration 6018, loss = 0.67209794\n",
      "Iteration 6019, loss = 0.67209385\n",
      "Iteration 6020, loss = 0.67208430\n",
      "Iteration 6021, loss = 0.67208715\n",
      "Iteration 6022, loss = 0.67207615\n",
      "Iteration 6023, loss = 0.67206901\n",
      "Iteration 6024, loss = 0.67205965\n",
      "Iteration 6025, loss = 0.67205859\n",
      "Iteration 6026, loss = 0.67204829\n",
      "Iteration 6027, loss = 0.67205292\n",
      "Iteration 6028, loss = 0.67203557\n",
      "Iteration 6029, loss = 0.67203295\n",
      "Iteration 6030, loss = 0.67203506\n",
      "Iteration 6031, loss = 0.67202144\n",
      "Iteration 6032, loss = 0.67201450\n",
      "Iteration 6033, loss = 0.67200904\n",
      "Iteration 6034, loss = 0.67201089\n",
      "Iteration 6035, loss = 0.67199989\n",
      "Iteration 6036, loss = 0.67198884\n",
      "Iteration 6037, loss = 0.67198342\n",
      "Iteration 6038, loss = 0.67198885\n",
      "Iteration 6039, loss = 0.67197729\n",
      "Iteration 6040, loss = 0.67196965\n",
      "Iteration 6041, loss = 0.67196024\n",
      "Iteration 6042, loss = 0.67196980\n",
      "Iteration 6043, loss = 0.67195336\n",
      "Iteration 6044, loss = 0.67194196\n",
      "Iteration 6045, loss = 0.67193718\n",
      "Iteration 6046, loss = 0.67192992\n",
      "Iteration 6047, loss = 0.67192646\n",
      "Iteration 6048, loss = 0.67191658\n",
      "Iteration 6049, loss = 0.67191491\n",
      "Iteration 6050, loss = 0.67190884\n",
      "Iteration 6051, loss = 0.67190727\n",
      "Iteration 6052, loss = 0.67189415\n",
      "Iteration 6053, loss = 0.67188945\n",
      "Iteration 6054, loss = 0.67188964\n",
      "Iteration 6055, loss = 0.67187430\n",
      "Iteration 6056, loss = 0.67188725\n",
      "Iteration 6057, loss = 0.67186249\n",
      "Iteration 6058, loss = 0.67185910\n",
      "Iteration 6059, loss = 0.67185056\n",
      "Iteration 6060, loss = 0.67184254\n",
      "Iteration 6061, loss = 0.67184574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6062, loss = 0.67183726\n",
      "Iteration 6063, loss = 0.67182514\n",
      "Iteration 6064, loss = 0.67181748\n",
      "Iteration 6065, loss = 0.67181300\n",
      "Iteration 6066, loss = 0.67180657\n",
      "Iteration 6067, loss = 0.67180717\n",
      "Iteration 6068, loss = 0.67179391\n",
      "Iteration 6069, loss = 0.67178486\n",
      "Iteration 6070, loss = 0.67177829\n",
      "Iteration 6071, loss = 0.67178405\n",
      "Iteration 6072, loss = 0.67177208\n",
      "Iteration 6073, loss = 0.67176795\n",
      "Iteration 6074, loss = 0.67175211\n",
      "Iteration 6075, loss = 0.67174516\n",
      "Iteration 6076, loss = 0.67174306\n",
      "Iteration 6077, loss = 0.67173084\n",
      "Iteration 6078, loss = 0.67173115\n",
      "Iteration 6079, loss = 0.67171976\n",
      "Iteration 6080, loss = 0.67171159\n",
      "Iteration 6081, loss = 0.67170687\n",
      "Iteration 6082, loss = 0.67169693\n",
      "Iteration 6083, loss = 0.67169289\n",
      "Iteration 6084, loss = 0.67168558\n",
      "Iteration 6085, loss = 0.67167821\n",
      "Iteration 6086, loss = 0.67166930\n",
      "Iteration 6087, loss = 0.67166642\n",
      "Iteration 6088, loss = 0.67165758\n",
      "Iteration 6089, loss = 0.67165547\n",
      "Iteration 6090, loss = 0.67164283\n",
      "Iteration 6091, loss = 0.67163784\n",
      "Iteration 6092, loss = 0.67164028\n",
      "Iteration 6093, loss = 0.67162187\n",
      "Iteration 6094, loss = 0.67161692\n",
      "Iteration 6095, loss = 0.67161283\n",
      "Iteration 6096, loss = 0.67160811\n",
      "Iteration 6097, loss = 0.67159604\n",
      "Iteration 6098, loss = 0.67160294\n",
      "Iteration 6099, loss = 0.67158538\n",
      "Iteration 6100, loss = 0.67158065\n",
      "Iteration 6101, loss = 0.67157710\n",
      "Iteration 6102, loss = 0.67157127\n",
      "Iteration 6103, loss = 0.67156683\n",
      "Iteration 6104, loss = 0.67156304\n",
      "Iteration 6105, loss = 0.67155347\n",
      "Iteration 6106, loss = 0.67154526\n",
      "Iteration 6107, loss = 0.67153880\n",
      "Iteration 6108, loss = 0.67153837\n",
      "Iteration 6109, loss = 0.67155051\n",
      "Iteration 6110, loss = 0.67152627\n",
      "Iteration 6111, loss = 0.67151645\n",
      "Iteration 6112, loss = 0.67150583\n",
      "Iteration 6113, loss = 0.67150291\n",
      "Iteration 6114, loss = 0.67149964\n",
      "Iteration 6115, loss = 0.67149500\n",
      "Iteration 6116, loss = 0.67149365\n",
      "Iteration 6117, loss = 0.67148217\n",
      "Iteration 6118, loss = 0.67147131\n",
      "Iteration 6119, loss = 0.67146588\n",
      "Iteration 6120, loss = 0.67146347\n",
      "Iteration 6121, loss = 0.67146100\n",
      "Iteration 6122, loss = 0.67145862\n",
      "Iteration 6123, loss = 0.67144023\n",
      "Iteration 6124, loss = 0.67143921\n",
      "Iteration 6125, loss = 0.67142799\n",
      "Iteration 6126, loss = 0.67142230\n",
      "Iteration 6127, loss = 0.67141560\n",
      "Iteration 6128, loss = 0.67140782\n",
      "Iteration 6129, loss = 0.67140265\n",
      "Iteration 6130, loss = 0.67139907\n",
      "Iteration 6131, loss = 0.67138622\n",
      "Iteration 6132, loss = 0.67138273\n",
      "Iteration 6133, loss = 0.67137462\n",
      "Iteration 6134, loss = 0.67136531\n",
      "Iteration 6135, loss = 0.67137443\n",
      "Iteration 6136, loss = 0.67135299\n",
      "Iteration 6137, loss = 0.67135505\n",
      "Iteration 6138, loss = 0.67133609\n",
      "Iteration 6139, loss = 0.67132987\n",
      "Iteration 6140, loss = 0.67132770\n",
      "Iteration 6141, loss = 0.67131665\n",
      "Iteration 6142, loss = 0.67130876\n",
      "Iteration 6143, loss = 0.67130385\n",
      "Iteration 6144, loss = 0.67129706\n",
      "Iteration 6145, loss = 0.67129293\n",
      "Iteration 6146, loss = 0.67128369\n",
      "Iteration 6147, loss = 0.67128423\n",
      "Iteration 6148, loss = 0.67127082\n",
      "Iteration 6149, loss = 0.67126677\n",
      "Iteration 6150, loss = 0.67125783\n",
      "Iteration 6151, loss = 0.67125294\n",
      "Iteration 6152, loss = 0.67125341\n",
      "Iteration 6153, loss = 0.67124112\n",
      "Iteration 6154, loss = 0.67123284\n",
      "Iteration 6155, loss = 0.67122793\n",
      "Iteration 6156, loss = 0.67122460\n",
      "Iteration 6157, loss = 0.67121601\n",
      "Iteration 6158, loss = 0.67120372\n",
      "Iteration 6159, loss = 0.67120503\n",
      "Iteration 6160, loss = 0.67120079\n",
      "Iteration 6161, loss = 0.67118828\n",
      "Iteration 6162, loss = 0.67118779\n",
      "Iteration 6163, loss = 0.67117694\n",
      "Iteration 6164, loss = 0.67117286\n",
      "Iteration 6165, loss = 0.67116607\n",
      "Iteration 6166, loss = 0.67116117\n",
      "Iteration 6167, loss = 0.67114966\n",
      "Iteration 6168, loss = 0.67114524\n",
      "Iteration 6169, loss = 0.67114055\n",
      "Iteration 6170, loss = 0.67113487\n",
      "Iteration 6171, loss = 0.67112491\n",
      "Iteration 6172, loss = 0.67112392\n",
      "Iteration 6173, loss = 0.67112241\n",
      "Iteration 6174, loss = 0.67110564\n",
      "Iteration 6175, loss = 0.67110191\n",
      "Iteration 6176, loss = 0.67109836\n",
      "Iteration 6177, loss = 0.67108764\n",
      "Iteration 6178, loss = 0.67107559\n",
      "Iteration 6179, loss = 0.67107353\n",
      "Iteration 6180, loss = 0.67106794\n",
      "Iteration 6181, loss = 0.67105375\n",
      "Iteration 6182, loss = 0.67105465\n",
      "Iteration 6183, loss = 0.67104319\n",
      "Iteration 6184, loss = 0.67103785\n",
      "Iteration 6185, loss = 0.67103466\n",
      "Iteration 6186, loss = 0.67102774\n",
      "Iteration 6187, loss = 0.67101725\n",
      "Iteration 6188, loss = 0.67101289\n",
      "Iteration 6189, loss = 0.67100943\n",
      "Iteration 6190, loss = 0.67100605\n",
      "Iteration 6191, loss = 0.67098782\n",
      "Iteration 6192, loss = 0.67098859\n",
      "Iteration 6193, loss = 0.67098273\n",
      "Iteration 6194, loss = 0.67097852\n",
      "Iteration 6195, loss = 0.67096926\n",
      "Iteration 6196, loss = 0.67095876\n",
      "Iteration 6197, loss = 0.67095169\n",
      "Iteration 6198, loss = 0.67095852\n",
      "Iteration 6199, loss = 0.67094749\n",
      "Iteration 6200, loss = 0.67093804\n",
      "Iteration 6201, loss = 0.67092591\n",
      "Iteration 6202, loss = 0.67091419\n",
      "Iteration 6203, loss = 0.67090888\n",
      "Iteration 6204, loss = 0.67090264\n",
      "Iteration 6205, loss = 0.67089184\n",
      "Iteration 6206, loss = 0.67089066\n",
      "Iteration 6207, loss = 0.67087859\n",
      "Iteration 6208, loss = 0.67087406\n",
      "Iteration 6209, loss = 0.67086190\n",
      "Iteration 6210, loss = 0.67085546\n",
      "Iteration 6211, loss = 0.67085189\n",
      "Iteration 6212, loss = 0.67084603\n",
      "Iteration 6213, loss = 0.67083968\n",
      "Iteration 6214, loss = 0.67082373\n",
      "Iteration 6215, loss = 0.67083173\n",
      "Iteration 6216, loss = 0.67081327\n",
      "Iteration 6217, loss = 0.67081122\n",
      "Iteration 6218, loss = 0.67079974\n",
      "Iteration 6219, loss = 0.67079837\n",
      "Iteration 6220, loss = 0.67078127\n",
      "Iteration 6221, loss = 0.67077626\n",
      "Iteration 6222, loss = 0.67077446\n",
      "Iteration 6223, loss = 0.67077367\n",
      "Iteration 6224, loss = 0.67075900\n",
      "Iteration 6225, loss = 0.67075086\n",
      "Iteration 6226, loss = 0.67074313\n",
      "Iteration 6227, loss = 0.67073456\n",
      "Iteration 6228, loss = 0.67072618\n",
      "Iteration 6229, loss = 0.67071906\n",
      "Iteration 6230, loss = 0.67071771\n",
      "Iteration 6231, loss = 0.67070727\n",
      "Iteration 6232, loss = 0.67070082\n",
      "Iteration 6233, loss = 0.67069659\n",
      "Iteration 6234, loss = 0.67068583\n",
      "Iteration 6235, loss = 0.67068026\n",
      "Iteration 6236, loss = 0.67067245\n",
      "Iteration 6237, loss = 0.67066640\n",
      "Iteration 6238, loss = 0.67066219\n",
      "Iteration 6239, loss = 0.67065902\n",
      "Iteration 6240, loss = 0.67064878\n",
      "Iteration 6241, loss = 0.67064361\n",
      "Iteration 6242, loss = 0.67064290\n",
      "Iteration 6243, loss = 0.67062643\n",
      "Iteration 6244, loss = 0.67062557\n",
      "Iteration 6245, loss = 0.67062860\n",
      "Iteration 6246, loss = 0.67061415\n",
      "Iteration 6247, loss = 0.67062521\n",
      "Iteration 6248, loss = 0.67059396\n",
      "Iteration 6249, loss = 0.67058975\n",
      "Iteration 6250, loss = 0.67059055\n",
      "Iteration 6251, loss = 0.67058345\n",
      "Iteration 6252, loss = 0.67057892\n",
      "Iteration 6253, loss = 0.67056538\n",
      "Iteration 6254, loss = 0.67055489\n",
      "Iteration 6255, loss = 0.67056977\n",
      "Iteration 6256, loss = 0.67055627\n",
      "Iteration 6257, loss = 0.67054532\n",
      "Iteration 6258, loss = 0.67053687\n",
      "Iteration 6259, loss = 0.67053143\n",
      "Iteration 6260, loss = 0.67052063\n",
      "Iteration 6261, loss = 0.67051154\n",
      "Iteration 6262, loss = 0.67051649\n",
      "Iteration 6263, loss = 0.67050228\n",
      "Iteration 6264, loss = 0.67049417\n",
      "Iteration 6265, loss = 0.67049434\n",
      "Iteration 6266, loss = 0.67048448\n",
      "Iteration 6267, loss = 0.67047792\n",
      "Iteration 6268, loss = 0.67047517\n",
      "Iteration 6269, loss = 0.67046745\n",
      "Iteration 6270, loss = 0.67046565\n",
      "Iteration 6271, loss = 0.67045266\n",
      "Iteration 6272, loss = 0.67045146\n",
      "Iteration 6273, loss = 0.67044931\n",
      "Iteration 6274, loss = 0.67043730\n",
      "Iteration 6275, loss = 0.67043835\n",
      "Iteration 6276, loss = 0.67043142\n",
      "Iteration 6277, loss = 0.67042224\n",
      "Iteration 6278, loss = 0.67041239\n",
      "Iteration 6279, loss = 0.67041093\n",
      "Iteration 6280, loss = 0.67040059\n",
      "Iteration 6281, loss = 0.67039452\n",
      "Iteration 6282, loss = 0.67039896\n",
      "Iteration 6283, loss = 0.67038629\n",
      "Iteration 6284, loss = 0.67037637\n",
      "Iteration 6285, loss = 0.67037033\n",
      "Iteration 6286, loss = 0.67036578\n",
      "Iteration 6287, loss = 0.67036343\n",
      "Iteration 6288, loss = 0.67035125\n",
      "Iteration 6289, loss = 0.67034309\n",
      "Iteration 6290, loss = 0.67033601\n",
      "Iteration 6291, loss = 0.67033261\n",
      "Iteration 6292, loss = 0.67033326\n",
      "Iteration 6293, loss = 0.67031823\n",
      "Iteration 6294, loss = 0.67030837\n",
      "Iteration 6295, loss = 0.67030098\n",
      "Iteration 6296, loss = 0.67029283\n",
      "Iteration 6297, loss = 0.67028410\n",
      "Iteration 6298, loss = 0.67028751\n",
      "Iteration 6299, loss = 0.67027148\n",
      "Iteration 6300, loss = 0.67026410\n",
      "Iteration 6301, loss = 0.67025664\n",
      "Iteration 6302, loss = 0.67024739\n",
      "Iteration 6303, loss = 0.67024146\n",
      "Iteration 6304, loss = 0.67023253\n",
      "Iteration 6305, loss = 0.67022283\n",
      "Iteration 6306, loss = 0.67021713\n",
      "Iteration 6307, loss = 0.67021356\n",
      "Iteration 6308, loss = 0.67019671\n",
      "Iteration 6309, loss = 0.67019632\n",
      "Iteration 6310, loss = 0.67018714\n",
      "Iteration 6311, loss = 0.67017775\n",
      "Iteration 6312, loss = 0.67016662\n",
      "Iteration 6313, loss = 0.67016353\n",
      "Iteration 6314, loss = 0.67015337\n",
      "Iteration 6315, loss = 0.67014129\n",
      "Iteration 6316, loss = 0.67014076\n",
      "Iteration 6317, loss = 0.67012759\n",
      "Iteration 6318, loss = 0.67012294\n",
      "Iteration 6319, loss = 0.67011358\n",
      "Iteration 6320, loss = 0.67010649\n",
      "Iteration 6321, loss = 0.67009745\n",
      "Iteration 6322, loss = 0.67009685\n",
      "Iteration 6323, loss = 0.67008719\n",
      "Iteration 6324, loss = 0.67007808\n",
      "Iteration 6325, loss = 0.67008404\n",
      "Iteration 6326, loss = 0.67007449\n",
      "Iteration 6327, loss = 0.67005691\n",
      "Iteration 6328, loss = 0.67005433\n",
      "Iteration 6329, loss = 0.67005020\n",
      "Iteration 6330, loss = 0.67004561\n",
      "Iteration 6331, loss = 0.67003575\n",
      "Iteration 6332, loss = 0.67002641\n",
      "Iteration 6333, loss = 0.67001575\n",
      "Iteration 6334, loss = 0.67000496\n",
      "Iteration 6335, loss = 0.66999987\n",
      "Iteration 6336, loss = 0.66999458\n",
      "Iteration 6337, loss = 0.66998628\n",
      "Iteration 6338, loss = 0.66997801\n",
      "Iteration 6339, loss = 0.66997306\n",
      "Iteration 6340, loss = 0.66996305\n",
      "Iteration 6341, loss = 0.66995788\n",
      "Iteration 6342, loss = 0.66994923\n",
      "Iteration 6343, loss = 0.66994450\n",
      "Iteration 6344, loss = 0.66993732\n",
      "Iteration 6345, loss = 0.66993095\n",
      "Iteration 6346, loss = 0.66992201\n",
      "Iteration 6347, loss = 0.66991823\n",
      "Iteration 6348, loss = 0.66991684\n",
      "Iteration 6349, loss = 0.66990130\n",
      "Iteration 6350, loss = 0.66990431\n",
      "Iteration 6351, loss = 0.66988740\n",
      "Iteration 6352, loss = 0.66989059\n",
      "Iteration 6353, loss = 0.66987241\n",
      "Iteration 6354, loss = 0.66986965\n",
      "Iteration 6355, loss = 0.66987224\n",
      "Iteration 6356, loss = 0.66985548\n",
      "Iteration 6357, loss = 0.66984783\n",
      "Iteration 6358, loss = 0.66984131\n",
      "Iteration 6359, loss = 0.66983393\n",
      "Iteration 6360, loss = 0.66983117\n",
      "Iteration 6361, loss = 0.66982426\n",
      "Iteration 6362, loss = 0.66981787\n",
      "Iteration 6363, loss = 0.66980642\n",
      "Iteration 6364, loss = 0.66980412\n",
      "Iteration 6365, loss = 0.66979393\n",
      "Iteration 6366, loss = 0.66979881\n",
      "Iteration 6367, loss = 0.66978305\n",
      "Iteration 6368, loss = 0.66977196\n",
      "Iteration 6369, loss = 0.66977215\n",
      "Iteration 6370, loss = 0.66977136\n",
      "Iteration 6371, loss = 0.66975361\n",
      "Iteration 6372, loss = 0.66975538\n",
      "Iteration 6373, loss = 0.66974515\n",
      "Iteration 6374, loss = 0.66974347\n",
      "Iteration 6375, loss = 0.66973103\n",
      "Iteration 6376, loss = 0.66972993\n",
      "Iteration 6377, loss = 0.66971764\n",
      "Iteration 6378, loss = 0.66971186\n",
      "Iteration 6379, loss = 0.66971016\n",
      "Iteration 6380, loss = 0.66969746\n",
      "Iteration 6381, loss = 0.66969675\n",
      "Iteration 6382, loss = 0.66968813\n",
      "Iteration 6383, loss = 0.66967719\n",
      "Iteration 6384, loss = 0.66967327\n",
      "Iteration 6385, loss = 0.66967074\n",
      "Iteration 6386, loss = 0.66968197\n",
      "Iteration 6387, loss = 0.66965320\n",
      "Iteration 6388, loss = 0.66965162\n",
      "Iteration 6389, loss = 0.66964864\n",
      "Iteration 6390, loss = 0.66963337\n",
      "Iteration 6391, loss = 0.66962877\n",
      "Iteration 6392, loss = 0.66962203\n",
      "Iteration 6393, loss = 0.66961917\n",
      "Iteration 6394, loss = 0.66960898\n",
      "Iteration 6395, loss = 0.66960513\n",
      "Iteration 6396, loss = 0.66959353\n",
      "Iteration 6397, loss = 0.66959255\n",
      "Iteration 6398, loss = 0.66958381\n",
      "Iteration 6399, loss = 0.66958193\n",
      "Iteration 6400, loss = 0.66957676\n",
      "Iteration 6401, loss = 0.66956028\n",
      "Iteration 6402, loss = 0.66955143\n",
      "Iteration 6403, loss = 0.66954195\n",
      "Iteration 6404, loss = 0.66954449\n",
      "Iteration 6405, loss = 0.66952976\n",
      "Iteration 6406, loss = 0.66951988\n",
      "Iteration 6407, loss = 0.66950758\n",
      "Iteration 6408, loss = 0.66949879\n",
      "Iteration 6409, loss = 0.66949133\n",
      "Iteration 6410, loss = 0.66947933\n",
      "Iteration 6411, loss = 0.66946838\n",
      "Iteration 6412, loss = 0.66946105\n",
      "Iteration 6413, loss = 0.66945306\n",
      "Iteration 6414, loss = 0.66945023\n",
      "Iteration 6415, loss = 0.66944074\n",
      "Iteration 6416, loss = 0.66942008\n",
      "Iteration 6417, loss = 0.66942004\n",
      "Iteration 6418, loss = 0.66940422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6419, loss = 0.66939563\n",
      "Iteration 6420, loss = 0.66938649\n",
      "Iteration 6421, loss = 0.66938212\n",
      "Iteration 6422, loss = 0.66937160\n",
      "Iteration 6423, loss = 0.66937294\n",
      "Iteration 6424, loss = 0.66935597\n",
      "Iteration 6425, loss = 0.66934580\n",
      "Iteration 6426, loss = 0.66934242\n",
      "Iteration 6427, loss = 0.66933146\n",
      "Iteration 6428, loss = 0.66931876\n",
      "Iteration 6429, loss = 0.66931654\n",
      "Iteration 6430, loss = 0.66930480\n",
      "Iteration 6431, loss = 0.66930373\n",
      "Iteration 6432, loss = 0.66928221\n",
      "Iteration 6433, loss = 0.66927715\n",
      "Iteration 6434, loss = 0.66927108\n",
      "Iteration 6435, loss = 0.66925832\n",
      "Iteration 6436, loss = 0.66926243\n",
      "Iteration 6437, loss = 0.66924499\n",
      "Iteration 6438, loss = 0.66924541\n",
      "Iteration 6439, loss = 0.66922686\n",
      "Iteration 6440, loss = 0.66922336\n",
      "Iteration 6441, loss = 0.66921755\n",
      "Iteration 6442, loss = 0.66920862\n",
      "Iteration 6443, loss = 0.66921147\n",
      "Iteration 6444, loss = 0.66919455\n",
      "Iteration 6445, loss = 0.66918286\n",
      "Iteration 6446, loss = 0.66917569\n",
      "Iteration 6447, loss = 0.66917030\n",
      "Iteration 6448, loss = 0.66916033\n",
      "Iteration 6449, loss = 0.66915869\n",
      "Iteration 6450, loss = 0.66915945\n",
      "Iteration 6451, loss = 0.66914641\n",
      "Iteration 6452, loss = 0.66913418\n",
      "Iteration 6453, loss = 0.66912753\n",
      "Iteration 6454, loss = 0.66911491\n",
      "Iteration 6455, loss = 0.66911316\n",
      "Iteration 6456, loss = 0.66910643\n",
      "Iteration 6457, loss = 0.66910196\n",
      "Iteration 6458, loss = 0.66909728\n",
      "Iteration 6459, loss = 0.66908308\n",
      "Iteration 6460, loss = 0.66908453\n",
      "Iteration 6461, loss = 0.66907033\n",
      "Iteration 6462, loss = 0.66906322\n",
      "Iteration 6463, loss = 0.66905927\n",
      "Iteration 6464, loss = 0.66905926\n",
      "Iteration 6465, loss = 0.66904794\n",
      "Iteration 6466, loss = 0.66903448\n",
      "Iteration 6467, loss = 0.66903851\n",
      "Iteration 6468, loss = 0.66902140\n",
      "Iteration 6469, loss = 0.66901320\n",
      "Iteration 6470, loss = 0.66901079\n",
      "Iteration 6471, loss = 0.66900585\n",
      "Iteration 6472, loss = 0.66899357\n",
      "Iteration 6473, loss = 0.66899381\n",
      "Iteration 6474, loss = 0.66898467\n",
      "Iteration 6475, loss = 0.66897952\n",
      "Iteration 6476, loss = 0.66897115\n",
      "Iteration 6477, loss = 0.66896735\n",
      "Iteration 6478, loss = 0.66895660\n",
      "Iteration 6479, loss = 0.66896072\n",
      "Iteration 6480, loss = 0.66894328\n",
      "Iteration 6481, loss = 0.66894101\n",
      "Iteration 6482, loss = 0.66893496\n",
      "Iteration 6483, loss = 0.66892655\n",
      "Iteration 6484, loss = 0.66891842\n",
      "Iteration 6485, loss = 0.66891287\n",
      "Iteration 6486, loss = 0.66893062\n",
      "Iteration 6487, loss = 0.66890847\n",
      "Iteration 6488, loss = 0.66889749\n",
      "Iteration 6489, loss = 0.66889497\n",
      "Iteration 6490, loss = 0.66888202\n",
      "Iteration 6491, loss = 0.66888266\n",
      "Iteration 6492, loss = 0.66887170\n",
      "Iteration 6493, loss = 0.66886528\n",
      "Iteration 6494, loss = 0.66885760\n",
      "Iteration 6495, loss = 0.66885328\n",
      "Iteration 6496, loss = 0.66884557\n",
      "Iteration 6497, loss = 0.66885245\n",
      "Iteration 6498, loss = 0.66883225\n",
      "Iteration 6499, loss = 0.66882643\n",
      "Iteration 6500, loss = 0.66882148\n",
      "Iteration 6501, loss = 0.66881461\n",
      "Iteration 6502, loss = 0.66881567\n",
      "Iteration 6503, loss = 0.66880377\n",
      "Iteration 6504, loss = 0.66880232\n",
      "Iteration 6505, loss = 0.66879780\n",
      "Iteration 6506, loss = 0.66878640\n",
      "Iteration 6507, loss = 0.66878040\n",
      "Iteration 6508, loss = 0.66877510\n",
      "Iteration 6509, loss = 0.66876961\n",
      "Iteration 6510, loss = 0.66876305\n",
      "Iteration 6511, loss = 0.66875559\n",
      "Iteration 6512, loss = 0.66875147\n",
      "Iteration 6513, loss = 0.66874270\n",
      "Iteration 6514, loss = 0.66873751\n",
      "Iteration 6515, loss = 0.66873485\n",
      "Iteration 6516, loss = 0.66872724\n",
      "Iteration 6517, loss = 0.66871779\n",
      "Iteration 6518, loss = 0.66872030\n",
      "Iteration 6519, loss = 0.66871144\n",
      "Iteration 6520, loss = 0.66870462\n",
      "Iteration 6521, loss = 0.66870085\n",
      "Iteration 6522, loss = 0.66869926\n",
      "Iteration 6523, loss = 0.66869042\n",
      "Iteration 6524, loss = 0.66867658\n",
      "Iteration 6525, loss = 0.66867230\n",
      "Iteration 6526, loss = 0.66867541\n",
      "Iteration 6527, loss = 0.66866161\n",
      "Iteration 6528, loss = 0.66866102\n",
      "Iteration 6529, loss = 0.66864992\n",
      "Iteration 6530, loss = 0.66864842\n",
      "Iteration 6531, loss = 0.66864734\n",
      "Iteration 6532, loss = 0.66863026\n",
      "Iteration 6533, loss = 0.66862464\n",
      "Iteration 6534, loss = 0.66861879\n",
      "Iteration 6535, loss = 0.66861481\n",
      "Iteration 6536, loss = 0.66860984\n",
      "Iteration 6537, loss = 0.66860555\n",
      "Iteration 6538, loss = 0.66859989\n",
      "Iteration 6539, loss = 0.66859379\n",
      "Iteration 6540, loss = 0.66858588\n",
      "Iteration 6541, loss = 0.66857764\n",
      "Iteration 6542, loss = 0.66857270\n",
      "Iteration 6543, loss = 0.66857343\n",
      "Iteration 6544, loss = 0.66856033\n",
      "Iteration 6545, loss = 0.66855743\n",
      "Iteration 6546, loss = 0.66855625\n",
      "Iteration 6547, loss = 0.66854492\n",
      "Iteration 6548, loss = 0.66854576\n",
      "Iteration 6549, loss = 0.66853190\n",
      "Iteration 6550, loss = 0.66852960\n",
      "Iteration 6551, loss = 0.66852106\n",
      "Iteration 6552, loss = 0.66851921\n",
      "Iteration 6553, loss = 0.66854137\n",
      "Iteration 6554, loss = 0.66851205\n",
      "Iteration 6555, loss = 0.66850428\n",
      "Iteration 6556, loss = 0.66849894\n",
      "Iteration 6557, loss = 0.66848603\n",
      "Iteration 6558, loss = 0.66848266\n",
      "Iteration 6559, loss = 0.66849438\n",
      "Iteration 6560, loss = 0.66848231\n",
      "Iteration 6561, loss = 0.66847930\n",
      "Iteration 6562, loss = 0.66846082\n",
      "Iteration 6563, loss = 0.66845151\n",
      "Iteration 6564, loss = 0.66844892\n",
      "Iteration 6565, loss = 0.66844706\n",
      "Iteration 6566, loss = 0.66843748\n",
      "Iteration 6567, loss = 0.66843238\n",
      "Iteration 6568, loss = 0.66842385\n",
      "Iteration 6569, loss = 0.66842402\n",
      "Iteration 6570, loss = 0.66841550\n",
      "Iteration 6571, loss = 0.66841566\n",
      "Iteration 6572, loss = 0.66840228\n",
      "Iteration 6573, loss = 0.66839465\n",
      "Iteration 6574, loss = 0.66839262\n",
      "Iteration 6575, loss = 0.66839038\n",
      "Iteration 6576, loss = 0.66838971\n",
      "Iteration 6577, loss = 0.66837569\n",
      "Iteration 6578, loss = 0.66837393\n",
      "Iteration 6579, loss = 0.66836194\n",
      "Iteration 6580, loss = 0.66836210\n",
      "Iteration 6581, loss = 0.66835959\n",
      "Iteration 6582, loss = 0.66834511\n",
      "Iteration 6583, loss = 0.66833754\n",
      "Iteration 6584, loss = 0.66833600\n",
      "Iteration 6585, loss = 0.66832810\n",
      "Iteration 6586, loss = 0.66833392\n",
      "Iteration 6587, loss = 0.66831611\n",
      "Iteration 6588, loss = 0.66832140\n",
      "Iteration 6589, loss = 0.66830398\n",
      "Iteration 6590, loss = 0.66829833\n",
      "Iteration 6591, loss = 0.66829439\n",
      "Iteration 6592, loss = 0.66828719\n",
      "Iteration 6593, loss = 0.66828213\n",
      "Iteration 6594, loss = 0.66828345\n",
      "Iteration 6595, loss = 0.66827197\n",
      "Iteration 6596, loss = 0.66826683\n",
      "Iteration 6597, loss = 0.66827158\n",
      "Iteration 6598, loss = 0.66825410\n",
      "Iteration 6599, loss = 0.66825195\n",
      "Iteration 6600, loss = 0.66824715\n",
      "Iteration 6601, loss = 0.66823788\n",
      "Iteration 6602, loss = 0.66823333\n",
      "Iteration 6603, loss = 0.66823292\n",
      "Iteration 6604, loss = 0.66822526\n",
      "Iteration 6605, loss = 0.66821836\n",
      "Iteration 6606, loss = 0.66821546\n",
      "Iteration 6607, loss = 0.66821854\n",
      "Iteration 6608, loss = 0.66820109\n",
      "Iteration 6609, loss = 0.66819346\n",
      "Iteration 6610, loss = 0.66818881\n",
      "Iteration 6611, loss = 0.66818579\n",
      "Iteration 6612, loss = 0.66817964\n",
      "Iteration 6613, loss = 0.66817658\n",
      "Iteration 6614, loss = 0.66817631\n",
      "Iteration 6615, loss = 0.66816183\n",
      "Iteration 6616, loss = 0.66815996\n",
      "Iteration 6617, loss = 0.66816152\n",
      "Iteration 6618, loss = 0.66815172\n",
      "Iteration 6619, loss = 0.66814300\n",
      "Iteration 6620, loss = 0.66814747\n",
      "Iteration 6621, loss = 0.66813147\n",
      "Iteration 6622, loss = 0.66813019\n",
      "Iteration 6623, loss = 0.66812926\n",
      "Iteration 6624, loss = 0.66811483\n",
      "Iteration 6625, loss = 0.66810956\n",
      "Iteration 6626, loss = 0.66811435\n",
      "Iteration 6627, loss = 0.66809696\n",
      "Iteration 6628, loss = 0.66809502\n",
      "Iteration 6629, loss = 0.66808849\n",
      "Iteration 6630, loss = 0.66808168\n",
      "Iteration 6631, loss = 0.66808074\n",
      "Iteration 6632, loss = 0.66807156\n",
      "Iteration 6633, loss = 0.66806250\n",
      "Iteration 6634, loss = 0.66806052\n",
      "Iteration 6635, loss = 0.66805215\n",
      "Iteration 6636, loss = 0.66804810\n",
      "Iteration 6637, loss = 0.66804288\n",
      "Iteration 6638, loss = 0.66804023\n",
      "Iteration 6639, loss = 0.66803173\n",
      "Iteration 6640, loss = 0.66802847\n",
      "Iteration 6641, loss = 0.66802063\n",
      "Iteration 6642, loss = 0.66801229\n",
      "Iteration 6643, loss = 0.66800845\n",
      "Iteration 6644, loss = 0.66800641\n",
      "Iteration 6645, loss = 0.66799901\n",
      "Iteration 6646, loss = 0.66799498\n",
      "Iteration 6647, loss = 0.66798906\n",
      "Iteration 6648, loss = 0.66798128\n",
      "Iteration 6649, loss = 0.66797813\n",
      "Iteration 6650, loss = 0.66797256\n",
      "Iteration 6651, loss = 0.66796439\n",
      "Iteration 6652, loss = 0.66796457\n",
      "Iteration 6653, loss = 0.66795554\n",
      "Iteration 6654, loss = 0.66794905\n",
      "Iteration 6655, loss = 0.66794447\n",
      "Iteration 6656, loss = 0.66794204\n",
      "Iteration 6657, loss = 0.66793811\n",
      "Iteration 6658, loss = 0.66793074\n",
      "Iteration 6659, loss = 0.66791965\n",
      "Iteration 6660, loss = 0.66792335\n",
      "Iteration 6661, loss = 0.66791259\n",
      "Iteration 6662, loss = 0.66791575\n",
      "Iteration 6663, loss = 0.66789948\n",
      "Iteration 6664, loss = 0.66790818\n",
      "Iteration 6665, loss = 0.66789203\n",
      "Iteration 6666, loss = 0.66788372\n",
      "Iteration 6667, loss = 0.66788071\n",
      "Iteration 6668, loss = 0.66787588\n",
      "Iteration 6669, loss = 0.66788428\n",
      "Iteration 6670, loss = 0.66786587\n",
      "Iteration 6671, loss = 0.66785759\n",
      "Iteration 6672, loss = 0.66785245\n",
      "Iteration 6673, loss = 0.66785554\n",
      "Iteration 6674, loss = 0.66784508\n",
      "Iteration 6675, loss = 0.66783720\n",
      "Iteration 6676, loss = 0.66783307\n",
      "Iteration 6677, loss = 0.66782900\n",
      "Iteration 6678, loss = 0.66782405\n",
      "Iteration 6679, loss = 0.66782194\n",
      "Iteration 6680, loss = 0.66781376\n",
      "Iteration 6681, loss = 0.66781444\n",
      "Iteration 6682, loss = 0.66780420\n",
      "Iteration 6683, loss = 0.66779756\n",
      "Iteration 6684, loss = 0.66779801\n",
      "Iteration 6685, loss = 0.66778301\n",
      "Iteration 6686, loss = 0.66778054\n",
      "Iteration 6687, loss = 0.66777273\n",
      "Iteration 6688, loss = 0.66777552\n",
      "Iteration 6689, loss = 0.66776936\n",
      "Iteration 6690, loss = 0.66776459\n",
      "Iteration 6691, loss = 0.66776030\n",
      "Iteration 6692, loss = 0.66774626\n",
      "Iteration 6693, loss = 0.66774986\n",
      "Iteration 6694, loss = 0.66774380\n",
      "Iteration 6695, loss = 0.66772936\n",
      "Iteration 6696, loss = 0.66772626\n",
      "Iteration 6697, loss = 0.66772201\n",
      "Iteration 6698, loss = 0.66772141\n",
      "Iteration 6699, loss = 0.66771679\n",
      "Iteration 6700, loss = 0.66770370\n",
      "Iteration 6701, loss = 0.66770243\n",
      "Iteration 6702, loss = 0.66769183\n",
      "Iteration 6703, loss = 0.66768923\n",
      "Iteration 6704, loss = 0.66768944\n",
      "Iteration 6705, loss = 0.66768109\n",
      "Iteration 6706, loss = 0.66767029\n",
      "Iteration 6707, loss = 0.66767265\n",
      "Iteration 6708, loss = 0.66766979\n",
      "Iteration 6709, loss = 0.66767146\n",
      "Iteration 6710, loss = 0.66764997\n",
      "Iteration 6711, loss = 0.66764363\n",
      "Iteration 6712, loss = 0.66764503\n",
      "Iteration 6713, loss = 0.66763329\n",
      "Iteration 6714, loss = 0.66762895\n",
      "Iteration 6715, loss = 0.66762432\n",
      "Iteration 6716, loss = 0.66761856\n",
      "Iteration 6717, loss = 0.66761302\n",
      "Iteration 6718, loss = 0.66760568\n",
      "Iteration 6719, loss = 0.66760133\n",
      "Iteration 6720, loss = 0.66759410\n",
      "Iteration 6721, loss = 0.66758738\n",
      "Iteration 6722, loss = 0.66759555\n",
      "Iteration 6723, loss = 0.66757486\n",
      "Iteration 6724, loss = 0.66756768\n",
      "Iteration 6725, loss = 0.66756518\n",
      "Iteration 6726, loss = 0.66756480\n",
      "Iteration 6727, loss = 0.66756425\n",
      "Iteration 6728, loss = 0.66754616\n",
      "Iteration 6729, loss = 0.66754145\n",
      "Iteration 6730, loss = 0.66754046\n",
      "Iteration 6731, loss = 0.66752696\n",
      "Iteration 6732, loss = 0.66752677\n",
      "Iteration 6733, loss = 0.66751659\n",
      "Iteration 6734, loss = 0.66751395\n",
      "Iteration 6735, loss = 0.66750920\n",
      "Iteration 6736, loss = 0.66750132\n",
      "Iteration 6737, loss = 0.66751240\n",
      "Iteration 6738, loss = 0.66749205\n",
      "Iteration 6739, loss = 0.66749261\n",
      "Iteration 6740, loss = 0.66748183\n",
      "Iteration 6741, loss = 0.66748035\n",
      "Iteration 6742, loss = 0.66746508\n",
      "Iteration 6743, loss = 0.66746221\n",
      "Iteration 6744, loss = 0.66745755\n",
      "Iteration 6745, loss = 0.66744653\n",
      "Iteration 6746, loss = 0.66745079\n",
      "Iteration 6747, loss = 0.66743944\n",
      "Iteration 6748, loss = 0.66742998\n",
      "Iteration 6749, loss = 0.66742076\n",
      "Iteration 6750, loss = 0.66741470\n",
      "Iteration 6751, loss = 0.66742573\n",
      "Iteration 6752, loss = 0.66740295\n",
      "Iteration 6753, loss = 0.66740045\n",
      "Iteration 6754, loss = 0.66739596\n",
      "Iteration 6755, loss = 0.66738816\n",
      "Iteration 6756, loss = 0.66738855\n",
      "Iteration 6757, loss = 0.66737442\n",
      "Iteration 6758, loss = 0.66736826\n",
      "Iteration 6759, loss = 0.66736694\n",
      "Iteration 6760, loss = 0.66736062\n",
      "Iteration 6761, loss = 0.66735281\n",
      "Iteration 6762, loss = 0.66734754\n",
      "Iteration 6763, loss = 0.66734554\n",
      "Iteration 6764, loss = 0.66733278\n",
      "Iteration 6765, loss = 0.66733577\n",
      "Iteration 6766, loss = 0.66732372\n",
      "Iteration 6767, loss = 0.66732612\n",
      "Iteration 6768, loss = 0.66731500\n",
      "Iteration 6769, loss = 0.66731531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6770, loss = 0.66730102\n",
      "Iteration 6771, loss = 0.66729571\n",
      "Iteration 6772, loss = 0.66729933\n",
      "Iteration 6773, loss = 0.66728233\n",
      "Iteration 6774, loss = 0.66727875\n",
      "Iteration 6775, loss = 0.66727030\n",
      "Iteration 6776, loss = 0.66728073\n",
      "Iteration 6777, loss = 0.66725808\n",
      "Iteration 6778, loss = 0.66725427\n",
      "Iteration 6779, loss = 0.66725014\n",
      "Iteration 6780, loss = 0.66725260\n",
      "Iteration 6781, loss = 0.66723717\n",
      "Iteration 6782, loss = 0.66723656\n",
      "Iteration 6783, loss = 0.66723665\n",
      "Iteration 6784, loss = 0.66722011\n",
      "Iteration 6785, loss = 0.66721331\n",
      "Iteration 6786, loss = 0.66721741\n",
      "Iteration 6787, loss = 0.66720613\n",
      "Iteration 6788, loss = 0.66720387\n",
      "Iteration 6789, loss = 0.66719480\n",
      "Iteration 6790, loss = 0.66718505\n",
      "Iteration 6791, loss = 0.66718360\n",
      "Iteration 6792, loss = 0.66717412\n",
      "Iteration 6793, loss = 0.66718515\n",
      "Iteration 6794, loss = 0.66716470\n",
      "Iteration 6795, loss = 0.66715736\n",
      "Iteration 6796, loss = 0.66715922\n",
      "Iteration 6797, loss = 0.66715210\n",
      "Iteration 6798, loss = 0.66714306\n",
      "Iteration 6799, loss = 0.66713644\n",
      "Iteration 6800, loss = 0.66713581\n",
      "Iteration 6801, loss = 0.66713220\n",
      "Iteration 6802, loss = 0.66713470\n",
      "Iteration 6803, loss = 0.66711856\n",
      "Iteration 6804, loss = 0.66711316\n",
      "Iteration 6805, loss = 0.66710678\n",
      "Iteration 6806, loss = 0.66709843\n",
      "Iteration 6807, loss = 0.66709513\n",
      "Iteration 6808, loss = 0.66709809\n",
      "Iteration 6809, loss = 0.66710014\n",
      "Iteration 6810, loss = 0.66708410\n",
      "Iteration 6811, loss = 0.66709724\n",
      "Iteration 6812, loss = 0.66707095\n",
      "Iteration 6813, loss = 0.66706788\n",
      "Iteration 6814, loss = 0.66705672\n",
      "Iteration 6815, loss = 0.66705499\n",
      "Iteration 6816, loss = 0.66704918\n",
      "Iteration 6817, loss = 0.66704548\n",
      "Iteration 6818, loss = 0.66703760\n",
      "Iteration 6819, loss = 0.66703472\n",
      "Iteration 6820, loss = 0.66703018\n",
      "Iteration 6821, loss = 0.66702268\n",
      "Iteration 6822, loss = 0.66702128\n",
      "Iteration 6823, loss = 0.66700745\n",
      "Iteration 6824, loss = 0.66700375\n",
      "Iteration 6825, loss = 0.66700907\n",
      "Iteration 6826, loss = 0.66700845\n",
      "Iteration 6827, loss = 0.66698401\n",
      "Iteration 6828, loss = 0.66698420\n",
      "Iteration 6829, loss = 0.66698417\n",
      "Iteration 6830, loss = 0.66697651\n",
      "Iteration 6831, loss = 0.66697132\n",
      "Iteration 6832, loss = 0.66696437\n",
      "Iteration 6833, loss = 0.66695651\n",
      "Iteration 6834, loss = 0.66695183\n",
      "Iteration 6835, loss = 0.66694808\n",
      "Iteration 6836, loss = 0.66695266\n",
      "Iteration 6837, loss = 0.66694047\n",
      "Iteration 6838, loss = 0.66693139\n",
      "Iteration 6839, loss = 0.66693187\n",
      "Iteration 6840, loss = 0.66692037\n",
      "Iteration 6841, loss = 0.66691668\n",
      "Iteration 6842, loss = 0.66691331\n",
      "Iteration 6843, loss = 0.66690544\n",
      "Iteration 6844, loss = 0.66690645\n",
      "Iteration 6845, loss = 0.66689275\n",
      "Iteration 6846, loss = 0.66689108\n",
      "Iteration 6847, loss = 0.66688049\n",
      "Iteration 6848, loss = 0.66688578\n",
      "Iteration 6849, loss = 0.66687194\n",
      "Iteration 6850, loss = 0.66688764\n",
      "Iteration 6851, loss = 0.66686643\n",
      "Iteration 6852, loss = 0.66686350\n",
      "Iteration 6853, loss = 0.66685421\n",
      "Iteration 6854, loss = 0.66685013\n",
      "Iteration 6855, loss = 0.66684098\n",
      "Iteration 6856, loss = 0.66683690\n",
      "Iteration 6857, loss = 0.66683454\n",
      "Iteration 6858, loss = 0.66682697\n",
      "Iteration 6859, loss = 0.66682372\n",
      "Iteration 6860, loss = 0.66682766\n",
      "Iteration 6861, loss = 0.66680809\n",
      "Iteration 6862, loss = 0.66680393\n",
      "Iteration 6863, loss = 0.66679820\n",
      "Iteration 6864, loss = 0.66680584\n",
      "Iteration 6865, loss = 0.66679643\n",
      "Iteration 6866, loss = 0.66679152\n",
      "Iteration 6867, loss = 0.66678403\n",
      "Iteration 6868, loss = 0.66678006\n",
      "Iteration 6869, loss = 0.66677713\n",
      "Iteration 6870, loss = 0.66676750\n",
      "Iteration 6871, loss = 0.66676144\n",
      "Iteration 6872, loss = 0.66676207\n",
      "Iteration 6873, loss = 0.66675247\n",
      "Iteration 6874, loss = 0.66674315\n",
      "Iteration 6875, loss = 0.66674205\n",
      "Iteration 6876, loss = 0.66673158\n",
      "Iteration 6877, loss = 0.66673239\n",
      "Iteration 6878, loss = 0.66672644\n",
      "Iteration 6879, loss = 0.66672268\n",
      "Iteration 6880, loss = 0.66672279\n",
      "Iteration 6881, loss = 0.66671380\n",
      "Iteration 6882, loss = 0.66670956\n",
      "Iteration 6883, loss = 0.66669965\n",
      "Iteration 6884, loss = 0.66670831\n",
      "Iteration 6885, loss = 0.66668721\n",
      "Iteration 6886, loss = 0.66668193\n",
      "Iteration 6887, loss = 0.66667887\n",
      "Iteration 6888, loss = 0.66667649\n",
      "Iteration 6889, loss = 0.66667692\n",
      "Iteration 6890, loss = 0.66667901\n",
      "Iteration 6891, loss = 0.66665697\n",
      "Iteration 6892, loss = 0.66665399\n",
      "Iteration 6893, loss = 0.66665724\n",
      "Iteration 6894, loss = 0.66664871\n",
      "Iteration 6895, loss = 0.66664043\n",
      "Iteration 6896, loss = 0.66663228\n",
      "Iteration 6897, loss = 0.66663735\n",
      "Iteration 6898, loss = 0.66663262\n",
      "Iteration 6899, loss = 0.66661483\n",
      "Iteration 6900, loss = 0.66661448\n",
      "Iteration 6901, loss = 0.66660810\n",
      "Iteration 6902, loss = 0.66660315\n",
      "Iteration 6903, loss = 0.66659033\n",
      "Iteration 6904, loss = 0.66658343\n",
      "Iteration 6905, loss = 0.66658377\n",
      "Iteration 6906, loss = 0.66656771\n",
      "Iteration 6907, loss = 0.66656781\n",
      "Iteration 6908, loss = 0.66656700\n",
      "Iteration 6909, loss = 0.66655900\n",
      "Iteration 6910, loss = 0.66654977\n",
      "Iteration 6911, loss = 0.66654222\n",
      "Iteration 6912, loss = 0.66653718\n",
      "Iteration 6913, loss = 0.66653005\n",
      "Iteration 6914, loss = 0.66652564\n",
      "Iteration 6915, loss = 0.66651716\n",
      "Iteration 6916, loss = 0.66650563\n",
      "Iteration 6917, loss = 0.66649937\n",
      "Iteration 6918, loss = 0.66648975\n",
      "Iteration 6919, loss = 0.66648396\n",
      "Iteration 6920, loss = 0.66647475\n",
      "Iteration 6921, loss = 0.66647690\n",
      "Iteration 6922, loss = 0.66647943\n",
      "Iteration 6923, loss = 0.66645733\n",
      "Iteration 6924, loss = 0.66644899\n",
      "Iteration 6925, loss = 0.66646040\n",
      "Iteration 6926, loss = 0.66644048\n",
      "Iteration 6927, loss = 0.66644970\n",
      "Iteration 6928, loss = 0.66642847\n",
      "Iteration 6929, loss = 0.66641829\n",
      "Iteration 6930, loss = 0.66642561\n",
      "Iteration 6931, loss = 0.66640753\n",
      "Iteration 6932, loss = 0.66639602\n",
      "Iteration 6933, loss = 0.66640394\n",
      "Iteration 6934, loss = 0.66638531\n",
      "Iteration 6935, loss = 0.66637935\n",
      "Iteration 6936, loss = 0.66638612\n",
      "Iteration 6937, loss = 0.66636617\n",
      "Iteration 6938, loss = 0.66636209\n",
      "Iteration 6939, loss = 0.66635771\n",
      "Iteration 6940, loss = 0.66635301\n",
      "Iteration 6941, loss = 0.66634846\n",
      "Iteration 6942, loss = 0.66633687\n",
      "Iteration 6943, loss = 0.66632943\n",
      "Iteration 6944, loss = 0.66632227\n",
      "Iteration 6945, loss = 0.66631710\n",
      "Iteration 6946, loss = 0.66633490\n",
      "Iteration 6947, loss = 0.66631302\n",
      "Iteration 6948, loss = 0.66629763\n",
      "Iteration 6949, loss = 0.66629828\n",
      "Iteration 6950, loss = 0.66629217\n",
      "Iteration 6951, loss = 0.66628337\n",
      "Iteration 6952, loss = 0.66627893\n",
      "Iteration 6953, loss = 0.66627408\n",
      "Iteration 6954, loss = 0.66626673\n",
      "Iteration 6955, loss = 0.66625713\n",
      "Iteration 6956, loss = 0.66625626\n",
      "Iteration 6957, loss = 0.66624713\n",
      "Iteration 6958, loss = 0.66625485\n",
      "Iteration 6959, loss = 0.66623521\n",
      "Iteration 6960, loss = 0.66623072\n",
      "Iteration 6961, loss = 0.66623028\n",
      "Iteration 6962, loss = 0.66622312\n",
      "Iteration 6963, loss = 0.66622065\n",
      "Iteration 6964, loss = 0.66620417\n",
      "Iteration 6965, loss = 0.66620752\n",
      "Iteration 6966, loss = 0.66620203\n",
      "Iteration 6967, loss = 0.66619043\n",
      "Iteration 6968, loss = 0.66618079\n",
      "Iteration 6969, loss = 0.66618240\n",
      "Iteration 6970, loss = 0.66616768\n",
      "Iteration 6971, loss = 0.66616449\n",
      "Iteration 6972, loss = 0.66616234\n",
      "Iteration 6973, loss = 0.66615104\n",
      "Iteration 6974, loss = 0.66614912\n",
      "Iteration 6975, loss = 0.66614527\n",
      "Iteration 6976, loss = 0.66613878\n",
      "Iteration 6977, loss = 0.66613378\n",
      "Iteration 6978, loss = 0.66613094\n",
      "Iteration 6979, loss = 0.66611933\n",
      "Iteration 6980, loss = 0.66611093\n",
      "Iteration 6981, loss = 0.66610443\n",
      "Iteration 6982, loss = 0.66609934\n",
      "Iteration 6983, loss = 0.66610380\n",
      "Iteration 6984, loss = 0.66608959\n",
      "Iteration 6985, loss = 0.66608250\n",
      "Iteration 6986, loss = 0.66607905\n",
      "Iteration 6987, loss = 0.66607158\n",
      "Iteration 6988, loss = 0.66607220\n",
      "Iteration 6989, loss = 0.66606554\n",
      "Iteration 6990, loss = 0.66606211\n",
      "Iteration 6991, loss = 0.66605941\n",
      "Iteration 6992, loss = 0.66604459\n",
      "Iteration 6993, loss = 0.66603955\n",
      "Iteration 6994, loss = 0.66602832\n",
      "Iteration 6995, loss = 0.66602669\n",
      "Iteration 6996, loss = 0.66602078\n",
      "Iteration 6997, loss = 0.66601203\n",
      "Iteration 6998, loss = 0.66600985\n",
      "Iteration 6999, loss = 0.66600497\n",
      "Iteration 7000, loss = 0.66599713\n",
      "Iteration 7001, loss = 0.66598458\n",
      "Iteration 7002, loss = 0.66597830\n",
      "Iteration 7003, loss = 0.66597866\n",
      "Iteration 7004, loss = 0.66597007\n",
      "Iteration 7005, loss = 0.66596243\n",
      "Iteration 7006, loss = 0.66595927\n",
      "Iteration 7007, loss = 0.66595195\n",
      "Iteration 7008, loss = 0.66594599\n",
      "Iteration 7009, loss = 0.66594373\n",
      "Iteration 7010, loss = 0.66593214\n",
      "Iteration 7011, loss = 0.66593034\n",
      "Iteration 7012, loss = 0.66592212\n",
      "Iteration 7013, loss = 0.66591205\n",
      "Iteration 7014, loss = 0.66591008\n",
      "Iteration 7015, loss = 0.66590164\n",
      "Iteration 7016, loss = 0.66589917\n",
      "Iteration 7017, loss = 0.66589276\n",
      "Iteration 7018, loss = 0.66588206\n",
      "Iteration 7019, loss = 0.66587901\n",
      "Iteration 7020, loss = 0.66587902\n",
      "Iteration 7021, loss = 0.66587220\n",
      "Iteration 7022, loss = 0.66586613\n",
      "Iteration 7023, loss = 0.66585674\n",
      "Iteration 7024, loss = 0.66585329\n",
      "Iteration 7025, loss = 0.66585129\n",
      "Iteration 7026, loss = 0.66583911\n",
      "Iteration 7027, loss = 0.66584188\n",
      "Iteration 7028, loss = 0.66583250\n",
      "Iteration 7029, loss = 0.66582302\n",
      "Iteration 7030, loss = 0.66581734\n",
      "Iteration 7031, loss = 0.66581079\n",
      "Iteration 7032, loss = 0.66581980\n",
      "Iteration 7033, loss = 0.66579787\n",
      "Iteration 7034, loss = 0.66579999\n",
      "Iteration 7035, loss = 0.66578566\n",
      "Iteration 7036, loss = 0.66578618\n",
      "Iteration 7037, loss = 0.66577444\n",
      "Iteration 7038, loss = 0.66576447\n",
      "Iteration 7039, loss = 0.66577106\n",
      "Iteration 7040, loss = 0.66576116\n",
      "Iteration 7041, loss = 0.66575467\n",
      "Iteration 7042, loss = 0.66574675\n",
      "Iteration 7043, loss = 0.66575168\n",
      "Iteration 7044, loss = 0.66573250\n",
      "Iteration 7045, loss = 0.66574022\n",
      "Iteration 7046, loss = 0.66572566\n",
      "Iteration 7047, loss = 0.66572063\n",
      "Iteration 7048, loss = 0.66571271\n",
      "Iteration 7049, loss = 0.66570489\n",
      "Iteration 7050, loss = 0.66570271\n",
      "Iteration 7051, loss = 0.66569302\n",
      "Iteration 7052, loss = 0.66568746\n",
      "Iteration 7053, loss = 0.66568870\n",
      "Iteration 7054, loss = 0.66568635\n",
      "Iteration 7055, loss = 0.66567731\n",
      "Iteration 7056, loss = 0.66567716\n",
      "Iteration 7057, loss = 0.66566781\n",
      "Iteration 7058, loss = 0.66566770\n",
      "Iteration 7059, loss = 0.66566824\n",
      "Iteration 7060, loss = 0.66564920\n",
      "Iteration 7061, loss = 0.66563853\n",
      "Iteration 7062, loss = 0.66564274\n",
      "Iteration 7063, loss = 0.66562586\n",
      "Iteration 7064, loss = 0.66563220\n",
      "Iteration 7065, loss = 0.66561634\n",
      "Iteration 7066, loss = 0.66561304\n",
      "Iteration 7067, loss = 0.66560822\n",
      "Iteration 7068, loss = 0.66560949\n",
      "Iteration 7069, loss = 0.66559591\n",
      "Iteration 7070, loss = 0.66558986\n",
      "Iteration 7071, loss = 0.66558289\n",
      "Iteration 7072, loss = 0.66557843\n",
      "Iteration 7073, loss = 0.66557792\n",
      "Iteration 7074, loss = 0.66557023\n",
      "Iteration 7075, loss = 0.66556216\n",
      "Iteration 7076, loss = 0.66556621\n",
      "Iteration 7077, loss = 0.66554824\n",
      "Iteration 7078, loss = 0.66555217\n",
      "Iteration 7079, loss = 0.66555300\n",
      "Iteration 7080, loss = 0.66553690\n",
      "Iteration 7081, loss = 0.66552823\n",
      "Iteration 7082, loss = 0.66552852\n",
      "Iteration 7083, loss = 0.66551744\n",
      "Iteration 7084, loss = 0.66551179\n",
      "Iteration 7085, loss = 0.66551074\n",
      "Iteration 7086, loss = 0.66550385\n",
      "Iteration 7087, loss = 0.66549420\n",
      "Iteration 7088, loss = 0.66551083\n",
      "Iteration 7089, loss = 0.66549002\n",
      "Iteration 7090, loss = 0.66547992\n",
      "Iteration 7091, loss = 0.66547563\n",
      "Iteration 7092, loss = 0.66547091\n",
      "Iteration 7093, loss = 0.66547126\n",
      "Iteration 7094, loss = 0.66545933\n",
      "Iteration 7095, loss = 0.66546462\n",
      "Iteration 7096, loss = 0.66544542\n",
      "Iteration 7097, loss = 0.66544525\n",
      "Iteration 7098, loss = 0.66545847\n",
      "Iteration 7099, loss = 0.66542949\n",
      "Iteration 7100, loss = 0.66542739\n",
      "Iteration 7101, loss = 0.66542245\n",
      "Iteration 7102, loss = 0.66542514\n",
      "Iteration 7103, loss = 0.66541439\n",
      "Iteration 7104, loss = 0.66540351\n",
      "Iteration 7105, loss = 0.66540276\n",
      "Iteration 7106, loss = 0.66539288\n",
      "Iteration 7107, loss = 0.66538632\n",
      "Iteration 7108, loss = 0.66538405\n",
      "Iteration 7109, loss = 0.66537910\n",
      "Iteration 7110, loss = 0.66537323\n",
      "Iteration 7111, loss = 0.66536597\n",
      "Iteration 7112, loss = 0.66535905\n",
      "Iteration 7113, loss = 0.66535588\n",
      "Iteration 7114, loss = 0.66535360\n",
      "Iteration 7115, loss = 0.66534420\n",
      "Iteration 7116, loss = 0.66534068\n",
      "Iteration 7117, loss = 0.66533373\n",
      "Iteration 7118, loss = 0.66532957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7119, loss = 0.66532503\n",
      "Iteration 7120, loss = 0.66532169\n",
      "Iteration 7121, loss = 0.66531328\n",
      "Iteration 7122, loss = 0.66531074\n",
      "Iteration 7123, loss = 0.66529828\n",
      "Iteration 7124, loss = 0.66529457\n",
      "Iteration 7125, loss = 0.66528613\n",
      "Iteration 7126, loss = 0.66528255\n",
      "Iteration 7127, loss = 0.66527960\n",
      "Iteration 7128, loss = 0.66527280\n",
      "Iteration 7129, loss = 0.66526563\n",
      "Iteration 7130, loss = 0.66526122\n",
      "Iteration 7131, loss = 0.66525502\n",
      "Iteration 7132, loss = 0.66525295\n",
      "Iteration 7133, loss = 0.66524829\n",
      "Iteration 7134, loss = 0.66524952\n",
      "Iteration 7135, loss = 0.66524544\n",
      "Iteration 7136, loss = 0.66524596\n",
      "Iteration 7137, loss = 0.66521970\n",
      "Iteration 7138, loss = 0.66521780\n",
      "Iteration 7139, loss = 0.66520979\n",
      "Iteration 7140, loss = 0.66520611\n",
      "Iteration 7141, loss = 0.66520224\n",
      "Iteration 7142, loss = 0.66519307\n",
      "Iteration 7143, loss = 0.66518634\n",
      "Iteration 7144, loss = 0.66518576\n",
      "Iteration 7145, loss = 0.66518054\n",
      "Iteration 7146, loss = 0.66516834\n",
      "Iteration 7147, loss = 0.66516591\n",
      "Iteration 7148, loss = 0.66516542\n",
      "Iteration 7149, loss = 0.66515818\n",
      "Iteration 7150, loss = 0.66514558\n",
      "Iteration 7151, loss = 0.66514190\n",
      "Iteration 7152, loss = 0.66513443\n",
      "Iteration 7153, loss = 0.66512534\n",
      "Iteration 7154, loss = 0.66511396\n",
      "Iteration 7155, loss = 0.66511032\n",
      "Iteration 7156, loss = 0.66510271\n",
      "Iteration 7157, loss = 0.66508980\n",
      "Iteration 7158, loss = 0.66509145\n",
      "Iteration 7159, loss = 0.66509026\n",
      "Iteration 7160, loss = 0.66507608\n",
      "Iteration 7161, loss = 0.66505858\n",
      "Iteration 7162, loss = 0.66505587\n",
      "Iteration 7163, loss = 0.66504795\n",
      "Iteration 7164, loss = 0.66504148\n",
      "Iteration 7165, loss = 0.66504647\n",
      "Iteration 7166, loss = 0.66502029\n",
      "Iteration 7167, loss = 0.66502552\n",
      "Iteration 7168, loss = 0.66500943\n",
      "Iteration 7169, loss = 0.66499753\n",
      "Iteration 7170, loss = 0.66498719\n",
      "Iteration 7171, loss = 0.66498544\n",
      "Iteration 7172, loss = 0.66497063\n",
      "Iteration 7173, loss = 0.66496538\n",
      "Iteration 7174, loss = 0.66496399\n",
      "Iteration 7175, loss = 0.66495320\n",
      "Iteration 7176, loss = 0.66494082\n",
      "Iteration 7177, loss = 0.66493670\n",
      "Iteration 7178, loss = 0.66492007\n",
      "Iteration 7179, loss = 0.66491156\n",
      "Iteration 7180, loss = 0.66490303\n",
      "Iteration 7181, loss = 0.66489204\n",
      "Iteration 7182, loss = 0.66488528\n",
      "Iteration 7183, loss = 0.66488093\n",
      "Iteration 7184, loss = 0.66487611\n",
      "Iteration 7185, loss = 0.66486319\n",
      "Iteration 7186, loss = 0.66486115\n",
      "Iteration 7187, loss = 0.66484314\n",
      "Iteration 7188, loss = 0.66484433\n",
      "Iteration 7189, loss = 0.66482864\n",
      "Iteration 7190, loss = 0.66481738\n",
      "Iteration 7191, loss = 0.66482343\n",
      "Iteration 7192, loss = 0.66480544\n",
      "Iteration 7193, loss = 0.66479391\n",
      "Iteration 7194, loss = 0.66478189\n",
      "Iteration 7195, loss = 0.66478306\n",
      "Iteration 7196, loss = 0.66477057\n",
      "Iteration 7197, loss = 0.66476100\n",
      "Iteration 7198, loss = 0.66475865\n",
      "Iteration 7199, loss = 0.66474520\n",
      "Iteration 7200, loss = 0.66474666\n",
      "Iteration 7201, loss = 0.66472869\n",
      "Iteration 7202, loss = 0.66472297\n",
      "Iteration 7203, loss = 0.66471454\n",
      "Iteration 7204, loss = 0.66471053\n",
      "Iteration 7205, loss = 0.66470013\n",
      "Iteration 7206, loss = 0.66468870\n",
      "Iteration 7207, loss = 0.66467846\n",
      "Iteration 7208, loss = 0.66466818\n",
      "Iteration 7209, loss = 0.66466257\n",
      "Iteration 7210, loss = 0.66465627\n",
      "Iteration 7211, loss = 0.66464953\n",
      "Iteration 7212, loss = 0.66464134\n",
      "Iteration 7213, loss = 0.66464654\n",
      "Iteration 7214, loss = 0.66461531\n",
      "Iteration 7215, loss = 0.66461688\n",
      "Iteration 7216, loss = 0.66460127\n",
      "Iteration 7217, loss = 0.66459427\n",
      "Iteration 7218, loss = 0.66459365\n",
      "Iteration 7219, loss = 0.66458821\n",
      "Iteration 7220, loss = 0.66456978\n",
      "Iteration 7221, loss = 0.66456096\n",
      "Iteration 7222, loss = 0.66456576\n",
      "Iteration 7223, loss = 0.66455679\n",
      "Iteration 7224, loss = 0.66453961\n",
      "Iteration 7225, loss = 0.66452594\n",
      "Iteration 7226, loss = 0.66452097\n",
      "Iteration 7227, loss = 0.66450679\n",
      "Iteration 7228, loss = 0.66449957\n",
      "Iteration 7229, loss = 0.66449747\n",
      "Iteration 7230, loss = 0.66448718\n",
      "Iteration 7231, loss = 0.66447780\n",
      "Iteration 7232, loss = 0.66447276\n",
      "Iteration 7233, loss = 0.66446121\n",
      "Iteration 7234, loss = 0.66445440\n",
      "Iteration 7235, loss = 0.66444373\n",
      "Iteration 7236, loss = 0.66443036\n",
      "Iteration 7237, loss = 0.66442601\n",
      "Iteration 7238, loss = 0.66442637\n",
      "Iteration 7239, loss = 0.66441705\n",
      "Iteration 7240, loss = 0.66440288\n",
      "Iteration 7241, loss = 0.66440086\n",
      "Iteration 7242, loss = 0.66438570\n",
      "Iteration 7243, loss = 0.66437219\n",
      "Iteration 7244, loss = 0.66436862\n",
      "Iteration 7245, loss = 0.66436180\n",
      "Iteration 7246, loss = 0.66434592\n",
      "Iteration 7247, loss = 0.66433994\n",
      "Iteration 7248, loss = 0.66433153\n",
      "Iteration 7249, loss = 0.66432683\n",
      "Iteration 7250, loss = 0.66432002\n",
      "Iteration 7251, loss = 0.66430941\n",
      "Iteration 7252, loss = 0.66430548\n",
      "Iteration 7253, loss = 0.66428936\n",
      "Iteration 7254, loss = 0.66427888\n",
      "Iteration 7255, loss = 0.66427323\n",
      "Iteration 7256, loss = 0.66426251\n",
      "Iteration 7257, loss = 0.66425962\n",
      "Iteration 7258, loss = 0.66424244\n",
      "Iteration 7259, loss = 0.66424117\n",
      "Iteration 7260, loss = 0.66423227\n",
      "Iteration 7261, loss = 0.66422052\n",
      "Iteration 7262, loss = 0.66421492\n",
      "Iteration 7263, loss = 0.66420103\n",
      "Iteration 7264, loss = 0.66420087\n",
      "Iteration 7265, loss = 0.66419084\n",
      "Iteration 7266, loss = 0.66418261\n",
      "Iteration 7267, loss = 0.66417567\n",
      "Iteration 7268, loss = 0.66416366\n",
      "Iteration 7269, loss = 0.66416836\n",
      "Iteration 7270, loss = 0.66414657\n",
      "Iteration 7271, loss = 0.66413896\n",
      "Iteration 7272, loss = 0.66413005\n",
      "Iteration 7273, loss = 0.66412296\n",
      "Iteration 7274, loss = 0.66411742\n",
      "Iteration 7275, loss = 0.66410807\n",
      "Iteration 7276, loss = 0.66409725\n",
      "Iteration 7277, loss = 0.66409468\n",
      "Iteration 7278, loss = 0.66408836\n",
      "Iteration 7279, loss = 0.66408370\n",
      "Iteration 7280, loss = 0.66406797\n",
      "Iteration 7281, loss = 0.66406420\n",
      "Iteration 7282, loss = 0.66404867\n",
      "Iteration 7283, loss = 0.66403993\n",
      "Iteration 7284, loss = 0.66404134\n",
      "Iteration 7285, loss = 0.66402386\n",
      "Iteration 7286, loss = 0.66401571\n",
      "Iteration 7287, loss = 0.66401702\n",
      "Iteration 7288, loss = 0.66401398\n",
      "Iteration 7289, loss = 0.66399561\n",
      "Iteration 7290, loss = 0.66398430\n",
      "Iteration 7291, loss = 0.66397456\n",
      "Iteration 7292, loss = 0.66397847\n",
      "Iteration 7293, loss = 0.66395591\n",
      "Iteration 7294, loss = 0.66395025\n",
      "Iteration 7295, loss = 0.66393813\n",
      "Iteration 7296, loss = 0.66393219\n",
      "Iteration 7297, loss = 0.66394295\n",
      "Iteration 7298, loss = 0.66392025\n",
      "Iteration 7299, loss = 0.66389945\n",
      "Iteration 7300, loss = 0.66388821\n",
      "Iteration 7301, loss = 0.66388333\n",
      "Iteration 7302, loss = 0.66386724\n",
      "Iteration 7303, loss = 0.66386086\n",
      "Iteration 7304, loss = 0.66384513\n",
      "Iteration 7305, loss = 0.66384140\n",
      "Iteration 7306, loss = 0.66382776\n",
      "Iteration 7307, loss = 0.66383045\n",
      "Iteration 7308, loss = 0.66381169\n",
      "Iteration 7309, loss = 0.66381628\n",
      "Iteration 7310, loss = 0.66378701\n",
      "Iteration 7311, loss = 0.66377779\n",
      "Iteration 7312, loss = 0.66376787\n",
      "Iteration 7313, loss = 0.66375568\n",
      "Iteration 7314, loss = 0.66373791\n",
      "Iteration 7315, loss = 0.66373657\n",
      "Iteration 7316, loss = 0.66371703\n",
      "Iteration 7317, loss = 0.66370646\n",
      "Iteration 7318, loss = 0.66370851\n",
      "Iteration 7319, loss = 0.66369698\n",
      "Iteration 7320, loss = 0.66368467\n",
      "Iteration 7321, loss = 0.66366484\n",
      "Iteration 7322, loss = 0.66365430\n",
      "Iteration 7323, loss = 0.66364313\n",
      "Iteration 7324, loss = 0.66363414\n",
      "Iteration 7325, loss = 0.66362859\n",
      "Iteration 7326, loss = 0.66361231\n",
      "Iteration 7327, loss = 0.66360772\n",
      "Iteration 7328, loss = 0.66360267\n",
      "Iteration 7329, loss = 0.66358116\n",
      "Iteration 7330, loss = 0.66356823\n",
      "Iteration 7331, loss = 0.66357131\n",
      "Iteration 7332, loss = 0.66355864\n",
      "Iteration 7333, loss = 0.66353976\n",
      "Iteration 7334, loss = 0.66353037\n",
      "Iteration 7335, loss = 0.66352250\n",
      "Iteration 7336, loss = 0.66352242\n",
      "Iteration 7337, loss = 0.66350794\n",
      "Iteration 7338, loss = 0.66349160\n",
      "Iteration 7339, loss = 0.66349287\n",
      "Iteration 7340, loss = 0.66346710\n",
      "Iteration 7341, loss = 0.66345769\n",
      "Iteration 7342, loss = 0.66344824\n",
      "Iteration 7343, loss = 0.66344264\n",
      "Iteration 7344, loss = 0.66342899\n",
      "Iteration 7345, loss = 0.66342433\n",
      "Iteration 7346, loss = 0.66341882\n",
      "Iteration 7347, loss = 0.66340173\n",
      "Iteration 7348, loss = 0.66339172\n",
      "Iteration 7349, loss = 0.66338955\n",
      "Iteration 7350, loss = 0.66337719\n",
      "Iteration 7351, loss = 0.66336682\n",
      "Iteration 7352, loss = 0.66335991\n",
      "Iteration 7353, loss = 0.66335038\n",
      "Iteration 7354, loss = 0.66333522\n",
      "Iteration 7355, loss = 0.66333827\n",
      "Iteration 7356, loss = 0.66331849\n",
      "Iteration 7357, loss = 0.66331746\n",
      "Iteration 7358, loss = 0.66329755\n",
      "Iteration 7359, loss = 0.66328656\n",
      "Iteration 7360, loss = 0.66329079\n",
      "Iteration 7361, loss = 0.66326877\n",
      "Iteration 7362, loss = 0.66326268\n",
      "Iteration 7363, loss = 0.66325153\n",
      "Iteration 7364, loss = 0.66323152\n",
      "Iteration 7365, loss = 0.66322769\n",
      "Iteration 7366, loss = 0.66321973\n",
      "Iteration 7367, loss = 0.66320526\n",
      "Iteration 7368, loss = 0.66318910\n",
      "Iteration 7369, loss = 0.66318229\n",
      "Iteration 7370, loss = 0.66317675\n",
      "Iteration 7371, loss = 0.66316361\n",
      "Iteration 7372, loss = 0.66314697\n",
      "Iteration 7373, loss = 0.66314250\n",
      "Iteration 7374, loss = 0.66312612\n",
      "Iteration 7375, loss = 0.66310903\n",
      "Iteration 7376, loss = 0.66310419\n",
      "Iteration 7377, loss = 0.66308849\n",
      "Iteration 7378, loss = 0.66307513\n",
      "Iteration 7379, loss = 0.66306349\n",
      "Iteration 7380, loss = 0.66304892\n",
      "Iteration 7381, loss = 0.66304950\n",
      "Iteration 7382, loss = 0.66302202\n",
      "Iteration 7383, loss = 0.66301083\n",
      "Iteration 7384, loss = 0.66300464\n",
      "Iteration 7385, loss = 0.66299261\n",
      "Iteration 7386, loss = 0.66297898\n",
      "Iteration 7387, loss = 0.66296050\n",
      "Iteration 7388, loss = 0.66295296\n",
      "Iteration 7389, loss = 0.66293711\n",
      "Iteration 7390, loss = 0.66292023\n",
      "Iteration 7391, loss = 0.66292037\n",
      "Iteration 7392, loss = 0.66290718\n",
      "Iteration 7393, loss = 0.66289208\n",
      "Iteration 7394, loss = 0.66287834\n",
      "Iteration 7395, loss = 0.66286356\n",
      "Iteration 7396, loss = 0.66285581\n",
      "Iteration 7397, loss = 0.66283903\n",
      "Iteration 7398, loss = 0.66282568\n",
      "Iteration 7399, loss = 0.66283270\n",
      "Iteration 7400, loss = 0.66281450\n",
      "Iteration 7401, loss = 0.66278595\n",
      "Iteration 7402, loss = 0.66277382\n",
      "Iteration 7403, loss = 0.66276002\n",
      "Iteration 7404, loss = 0.66275500\n",
      "Iteration 7405, loss = 0.66273574\n",
      "Iteration 7406, loss = 0.66272889\n",
      "Iteration 7407, loss = 0.66271041\n",
      "Iteration 7408, loss = 0.66270620\n",
      "Iteration 7409, loss = 0.66268767\n",
      "Iteration 7410, loss = 0.66267613\n",
      "Iteration 7411, loss = 0.66266081\n",
      "Iteration 7412, loss = 0.66265384\n",
      "Iteration 7413, loss = 0.66264742\n",
      "Iteration 7414, loss = 0.66262302\n",
      "Iteration 7415, loss = 0.66261453\n",
      "Iteration 7416, loss = 0.66259995\n",
      "Iteration 7417, loss = 0.66258895\n",
      "Iteration 7418, loss = 0.66258093\n",
      "Iteration 7419, loss = 0.66256890\n",
      "Iteration 7420, loss = 0.66256143\n",
      "Iteration 7421, loss = 0.66254410\n",
      "Iteration 7422, loss = 0.66252296\n",
      "Iteration 7423, loss = 0.66250763\n",
      "Iteration 7424, loss = 0.66250545\n",
      "Iteration 7425, loss = 0.66249493\n",
      "Iteration 7426, loss = 0.66249130\n",
      "Iteration 7427, loss = 0.66246789\n",
      "Iteration 7428, loss = 0.66244819\n",
      "Iteration 7429, loss = 0.66244821\n",
      "Iteration 7430, loss = 0.66242912\n",
      "Iteration 7431, loss = 0.66241834\n",
      "Iteration 7432, loss = 0.66240398\n",
      "Iteration 7433, loss = 0.66239618\n",
      "Iteration 7434, loss = 0.66238904\n",
      "Iteration 7435, loss = 0.66237580\n",
      "Iteration 7436, loss = 0.66236783\n",
      "Iteration 7437, loss = 0.66234475\n",
      "Iteration 7438, loss = 0.66233518\n",
      "Iteration 7439, loss = 0.66231865\n",
      "Iteration 7440, loss = 0.66230746\n",
      "Iteration 7441, loss = 0.66231375\n",
      "Iteration 7442, loss = 0.66228708\n",
      "Iteration 7443, loss = 0.66227725\n",
      "Iteration 7444, loss = 0.66226221\n",
      "Iteration 7445, loss = 0.66225338\n",
      "Iteration 7446, loss = 0.66224117\n",
      "Iteration 7447, loss = 0.66222970\n",
      "Iteration 7448, loss = 0.66221840\n",
      "Iteration 7449, loss = 0.66220956\n",
      "Iteration 7450, loss = 0.66220935\n",
      "Iteration 7451, loss = 0.66218940\n",
      "Iteration 7452, loss = 0.66217098\n",
      "Iteration 7453, loss = 0.66215593\n",
      "Iteration 7454, loss = 0.66214668\n",
      "Iteration 7455, loss = 0.66214000\n",
      "Iteration 7456, loss = 0.66211766\n",
      "Iteration 7457, loss = 0.66211970\n",
      "Iteration 7458, loss = 0.66210665\n",
      "Iteration 7459, loss = 0.66209044\n",
      "Iteration 7460, loss = 0.66207124\n",
      "Iteration 7461, loss = 0.66206335\n",
      "Iteration 7462, loss = 0.66205017\n",
      "Iteration 7463, loss = 0.66203838\n",
      "Iteration 7464, loss = 0.66203391\n",
      "Iteration 7465, loss = 0.66202953\n",
      "Iteration 7466, loss = 0.66201064\n",
      "Iteration 7467, loss = 0.66199557\n",
      "Iteration 7468, loss = 0.66199714\n",
      "Iteration 7469, loss = 0.66197787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7470, loss = 0.66196424\n",
      "Iteration 7471, loss = 0.66197547\n",
      "Iteration 7472, loss = 0.66195123\n",
      "Iteration 7473, loss = 0.66194384\n",
      "Iteration 7474, loss = 0.66195070\n",
      "Iteration 7475, loss = 0.66191329\n",
      "Iteration 7476, loss = 0.66191070\n",
      "Iteration 7477, loss = 0.66189310\n",
      "Iteration 7478, loss = 0.66188450\n",
      "Iteration 7479, loss = 0.66186996\n",
      "Iteration 7480, loss = 0.66186572\n",
      "Iteration 7481, loss = 0.66185597\n",
      "Iteration 7482, loss = 0.66184082\n",
      "Iteration 7483, loss = 0.66183889\n",
      "Iteration 7484, loss = 0.66182445\n",
      "Iteration 7485, loss = 0.66181275\n",
      "Iteration 7486, loss = 0.66182198\n",
      "Iteration 7487, loss = 0.66179213\n",
      "Iteration 7488, loss = 0.66178932\n",
      "Iteration 7489, loss = 0.66177594\n",
      "Iteration 7490, loss = 0.66177403\n",
      "Iteration 7491, loss = 0.66176625\n",
      "Iteration 7492, loss = 0.66176145\n",
      "Iteration 7493, loss = 0.66173754\n",
      "Iteration 7494, loss = 0.66172713\n",
      "Iteration 7495, loss = 0.66171587\n",
      "Iteration 7496, loss = 0.66170707\n",
      "Iteration 7497, loss = 0.66171304\n",
      "Iteration 7498, loss = 0.66170068\n",
      "Iteration 7499, loss = 0.66169976\n",
      "Iteration 7500, loss = 0.66167027\n",
      "Iteration 7501, loss = 0.66166761\n",
      "Iteration 7502, loss = 0.66165455\n",
      "Iteration 7503, loss = 0.66164431\n",
      "Iteration 7504, loss = 0.66162892\n",
      "Iteration 7505, loss = 0.66162638\n",
      "Iteration 7506, loss = 0.66161151\n",
      "Iteration 7507, loss = 0.66160568\n",
      "Iteration 7508, loss = 0.66161034\n",
      "Iteration 7509, loss = 0.66158639\n",
      "Iteration 7510, loss = 0.66158525\n",
      "Iteration 7511, loss = 0.66156910\n",
      "Iteration 7512, loss = 0.66156201\n",
      "Iteration 7513, loss = 0.66155512\n",
      "Iteration 7514, loss = 0.66154541\n",
      "Iteration 7515, loss = 0.66153451\n",
      "Iteration 7516, loss = 0.66150249\n",
      "Iteration 7517, loss = 0.66148607\n",
      "Iteration 7518, loss = 0.66148250\n",
      "Iteration 7519, loss = 0.66146832\n",
      "Iteration 7520, loss = 0.66145293\n",
      "Iteration 7521, loss = 0.66143332\n",
      "Iteration 7522, loss = 0.66142865\n",
      "Iteration 7523, loss = 0.66140415\n",
      "Iteration 7524, loss = 0.66138609\n",
      "Iteration 7525, loss = 0.66139641\n",
      "Iteration 7526, loss = 0.66135890\n",
      "Iteration 7527, loss = 0.66135004\n",
      "Iteration 7528, loss = 0.66133914\n",
      "Iteration 7529, loss = 0.66131925\n",
      "Iteration 7530, loss = 0.66131508\n",
      "Iteration 7531, loss = 0.66130640\n",
      "Iteration 7532, loss = 0.66128259\n",
      "Iteration 7533, loss = 0.66127597\n",
      "Iteration 7534, loss = 0.66125440\n",
      "Iteration 7535, loss = 0.66125188\n",
      "Iteration 7536, loss = 0.66123323\n",
      "Iteration 7537, loss = 0.66121860\n",
      "Iteration 7538, loss = 0.66120602\n",
      "Iteration 7539, loss = 0.66118973\n",
      "Iteration 7540, loss = 0.66117089\n",
      "Iteration 7541, loss = 0.66115958\n",
      "Iteration 7542, loss = 0.66115531\n",
      "Iteration 7543, loss = 0.66113786\n",
      "Iteration 7544, loss = 0.66111584\n",
      "Iteration 7545, loss = 0.66110375\n",
      "Iteration 7546, loss = 0.66110177\n",
      "Iteration 7547, loss = 0.66108867\n",
      "Iteration 7548, loss = 0.66107218\n",
      "Iteration 7549, loss = 0.66105662\n",
      "Iteration 7550, loss = 0.66103837\n",
      "Iteration 7551, loss = 0.66102922\n",
      "Iteration 7552, loss = 0.66100972\n",
      "Iteration 7553, loss = 0.66101501\n",
      "Iteration 7554, loss = 0.66098926\n",
      "Iteration 7555, loss = 0.66098079\n",
      "Iteration 7556, loss = 0.66097109\n",
      "Iteration 7557, loss = 0.66094404\n",
      "Iteration 7558, loss = 0.66094206\n",
      "Iteration 7559, loss = 0.66091960\n",
      "Iteration 7560, loss = 0.66092677\n",
      "Iteration 7561, loss = 0.66089898\n",
      "Iteration 7562, loss = 0.66088281\n",
      "Iteration 7563, loss = 0.66088690\n",
      "Iteration 7564, loss = 0.66085540\n",
      "Iteration 7565, loss = 0.66084492\n",
      "Iteration 7566, loss = 0.66082850\n",
      "Iteration 7567, loss = 0.66081218\n",
      "Iteration 7568, loss = 0.66079878\n",
      "Iteration 7569, loss = 0.66078920\n",
      "Iteration 7570, loss = 0.66076771\n",
      "Iteration 7571, loss = 0.66075381\n",
      "Iteration 7572, loss = 0.66074847\n",
      "Iteration 7573, loss = 0.66073440\n",
      "Iteration 7574, loss = 0.66071935\n",
      "Iteration 7575, loss = 0.66071593\n",
      "Iteration 7576, loss = 0.66068954\n",
      "Iteration 7577, loss = 0.66066835\n",
      "Iteration 7578, loss = 0.66065911\n",
      "Iteration 7579, loss = 0.66064378\n",
      "Iteration 7580, loss = 0.66064972\n",
      "Iteration 7581, loss = 0.66063307\n",
      "Iteration 7582, loss = 0.66060731\n",
      "Iteration 7583, loss = 0.66058727\n",
      "Iteration 7584, loss = 0.66057316\n",
      "Iteration 7585, loss = 0.66055878\n",
      "Iteration 7586, loss = 0.66055076\n",
      "Iteration 7587, loss = 0.66053630\n",
      "Iteration 7588, loss = 0.66051522\n",
      "Iteration 7589, loss = 0.66051946\n",
      "Iteration 7590, loss = 0.66049378\n",
      "Iteration 7591, loss = 0.66048021\n",
      "Iteration 7592, loss = 0.66047351\n",
      "Iteration 7593, loss = 0.66044809\n",
      "Iteration 7594, loss = 0.66045639\n",
      "Iteration 7595, loss = 0.66042649\n",
      "Iteration 7596, loss = 0.66041596\n",
      "Iteration 7597, loss = 0.66040429\n",
      "Iteration 7598, loss = 0.66038840\n",
      "Iteration 7599, loss = 0.66037064\n",
      "Iteration 7600, loss = 0.66035879\n",
      "Iteration 7601, loss = 0.66036268\n",
      "Iteration 7602, loss = 0.66033641\n",
      "Iteration 7603, loss = 0.66031900\n",
      "Iteration 7604, loss = 0.66031095\n",
      "Iteration 7605, loss = 0.66029495\n",
      "Iteration 7606, loss = 0.66027905\n",
      "Iteration 7607, loss = 0.66026806\n",
      "Iteration 7608, loss = 0.66027131\n",
      "Iteration 7609, loss = 0.66024869\n",
      "Iteration 7610, loss = 0.66023779\n",
      "Iteration 7611, loss = 0.66023459\n",
      "Iteration 7612, loss = 0.66021487\n",
      "Iteration 7613, loss = 0.66020284\n",
      "Iteration 7614, loss = 0.66019426\n",
      "Iteration 7615, loss = 0.66017644\n",
      "Iteration 7616, loss = 0.66016220\n",
      "Iteration 7617, loss = 0.66015368\n",
      "Iteration 7618, loss = 0.66014957\n",
      "Iteration 7619, loss = 0.66012821\n",
      "Iteration 7620, loss = 0.66013172\n",
      "Iteration 7621, loss = 0.66011508\n",
      "Iteration 7622, loss = 0.66009236\n",
      "Iteration 7623, loss = 0.66009042\n",
      "Iteration 7624, loss = 0.66008285\n",
      "Iteration 7625, loss = 0.66009166\n",
      "Iteration 7626, loss = 0.66006409\n",
      "Iteration 7627, loss = 0.66004518\n",
      "Iteration 7628, loss = 0.66003988\n",
      "Iteration 7629, loss = 0.66002427\n",
      "Iteration 7630, loss = 0.66001902\n",
      "Iteration 7631, loss = 0.66000905\n",
      "Iteration 7632, loss = 0.65999688\n",
      "Iteration 7633, loss = 0.65998864\n",
      "Iteration 7634, loss = 0.65997585\n",
      "Iteration 7635, loss = 0.65995947\n",
      "Iteration 7636, loss = 0.65995163\n",
      "Iteration 7637, loss = 0.65994102\n",
      "Iteration 7638, loss = 0.65993174\n",
      "Iteration 7639, loss = 0.65992896\n",
      "Iteration 7640, loss = 0.65990995\n",
      "Iteration 7641, loss = 0.65991137\n",
      "Iteration 7642, loss = 0.65988880\n",
      "Iteration 7643, loss = 0.65988553\n",
      "Iteration 7644, loss = 0.65986917\n",
      "Iteration 7645, loss = 0.65986077\n",
      "Iteration 7646, loss = 0.65986791\n",
      "Iteration 7647, loss = 0.65984052\n",
      "Iteration 7648, loss = 0.65984628\n",
      "Iteration 7649, loss = 0.65982827\n",
      "Iteration 7650, loss = 0.65981006\n",
      "Iteration 7651, loss = 0.65980190\n",
      "Iteration 7652, loss = 0.65979143\n",
      "Iteration 7653, loss = 0.65980963\n",
      "Iteration 7654, loss = 0.65977914\n",
      "Iteration 7655, loss = 0.65977209\n",
      "Iteration 7656, loss = 0.65975887\n",
      "Iteration 7657, loss = 0.65974845\n",
      "Iteration 7658, loss = 0.65974377\n",
      "Iteration 7659, loss = 0.65973230\n",
      "Iteration 7660, loss = 0.65972683\n",
      "Iteration 7661, loss = 0.65973061\n",
      "Iteration 7662, loss = 0.65971245\n",
      "Iteration 7663, loss = 0.65969547\n",
      "Iteration 7664, loss = 0.65969574\n",
      "Iteration 7665, loss = 0.65967425\n",
      "Iteration 7666, loss = 0.65968488\n",
      "Iteration 7667, loss = 0.65966322\n",
      "Iteration 7668, loss = 0.65965863\n",
      "Iteration 7669, loss = 0.65964222\n",
      "Iteration 7670, loss = 0.65963537\n",
      "Iteration 7671, loss = 0.65963010\n",
      "Iteration 7672, loss = 0.65962012\n",
      "Iteration 7673, loss = 0.65961630\n",
      "Iteration 7674, loss = 0.65960303\n",
      "Iteration 7675, loss = 0.65959392\n",
      "Iteration 7676, loss = 0.65960951\n",
      "Iteration 7677, loss = 0.65958152\n",
      "Iteration 7678, loss = 0.65957656\n",
      "Iteration 7679, loss = 0.65956255\n",
      "Iteration 7680, loss = 0.65955117\n",
      "Iteration 7681, loss = 0.65954777\n",
      "Iteration 7682, loss = 0.65954919\n",
      "Iteration 7683, loss = 0.65954566\n",
      "Iteration 7684, loss = 0.65953824\n",
      "Iteration 7685, loss = 0.65951241\n",
      "Iteration 7686, loss = 0.65950701\n",
      "Iteration 7687, loss = 0.65949816\n",
      "Iteration 7688, loss = 0.65949012\n",
      "Iteration 7689, loss = 0.65948888\n",
      "Iteration 7690, loss = 0.65947446\n",
      "Iteration 7691, loss = 0.65947033\n",
      "Iteration 7692, loss = 0.65946480\n",
      "Iteration 7693, loss = 0.65945710\n",
      "Iteration 7694, loss = 0.65944644\n",
      "Iteration 7695, loss = 0.65945189\n",
      "Iteration 7696, loss = 0.65942904\n",
      "Iteration 7697, loss = 0.65943196\n",
      "Iteration 7698, loss = 0.65941660\n",
      "Iteration 7699, loss = 0.65942161\n",
      "Iteration 7700, loss = 0.65941423\n",
      "Iteration 7701, loss = 0.65941378\n",
      "Iteration 7702, loss = 0.65938436\n",
      "Iteration 7703, loss = 0.65938457\n",
      "Iteration 7704, loss = 0.65937500\n",
      "Iteration 7705, loss = 0.65936373\n",
      "Iteration 7706, loss = 0.65935617\n",
      "Iteration 7707, loss = 0.65934946\n",
      "Iteration 7708, loss = 0.65934357\n",
      "Iteration 7709, loss = 0.65934752\n",
      "Iteration 7710, loss = 0.65933584\n",
      "Iteration 7711, loss = 0.65932442\n",
      "Iteration 7712, loss = 0.65931375\n",
      "Iteration 7713, loss = 0.65930506\n",
      "Iteration 7714, loss = 0.65930490\n",
      "Iteration 7715, loss = 0.65930216\n",
      "Iteration 7716, loss = 0.65929418\n",
      "Iteration 7717, loss = 0.65928540\n",
      "Iteration 7718, loss = 0.65927630\n",
      "Iteration 7719, loss = 0.65926593\n",
      "Iteration 7720, loss = 0.65925966\n",
      "Iteration 7721, loss = 0.65924962\n",
      "Iteration 7722, loss = 0.65924207\n",
      "Iteration 7723, loss = 0.65925298\n",
      "Iteration 7724, loss = 0.65922810\n",
      "Iteration 7725, loss = 0.65923033\n",
      "Iteration 7726, loss = 0.65921716\n",
      "Iteration 7727, loss = 0.65922486\n",
      "Iteration 7728, loss = 0.65920476\n",
      "Iteration 7729, loss = 0.65921074\n",
      "Iteration 7730, loss = 0.65919319\n",
      "Iteration 7731, loss = 0.65918200\n",
      "Iteration 7732, loss = 0.65918641\n",
      "Iteration 7733, loss = 0.65916425\n",
      "Iteration 7734, loss = 0.65915556\n",
      "Iteration 7735, loss = 0.65915101\n",
      "Iteration 7736, loss = 0.65915832\n",
      "Iteration 7737, loss = 0.65913756\n",
      "Iteration 7738, loss = 0.65913918\n",
      "Iteration 7739, loss = 0.65912865\n",
      "Iteration 7740, loss = 0.65913715\n",
      "Iteration 7741, loss = 0.65910443\n",
      "Iteration 7742, loss = 0.65912706\n",
      "Iteration 7743, loss = 0.65910396\n",
      "Iteration 7744, loss = 0.65908700\n",
      "Iteration 7745, loss = 0.65907932\n",
      "Iteration 7746, loss = 0.65907239\n",
      "Iteration 7747, loss = 0.65906527\n",
      "Iteration 7748, loss = 0.65906182\n",
      "Iteration 7749, loss = 0.65905693\n",
      "Iteration 7750, loss = 0.65904452\n",
      "Iteration 7751, loss = 0.65905352\n",
      "Iteration 7752, loss = 0.65903647\n",
      "Iteration 7753, loss = 0.65902055\n",
      "Iteration 7754, loss = 0.65902229\n",
      "Iteration 7755, loss = 0.65901272\n",
      "Iteration 7756, loss = 0.65901079\n",
      "Iteration 7757, loss = 0.65900278\n",
      "Iteration 7758, loss = 0.65899181\n",
      "Iteration 7759, loss = 0.65900425\n",
      "Iteration 7760, loss = 0.65897455\n",
      "Iteration 7761, loss = 0.65896521\n",
      "Iteration 7762, loss = 0.65895601\n",
      "Iteration 7763, loss = 0.65895079\n",
      "Iteration 7764, loss = 0.65895917\n",
      "Iteration 7765, loss = 0.65893360\n",
      "Iteration 7766, loss = 0.65893842\n",
      "Iteration 7767, loss = 0.65893553\n",
      "Iteration 7768, loss = 0.65891848\n",
      "Iteration 7769, loss = 0.65890652\n",
      "Iteration 7770, loss = 0.65889893\n",
      "Iteration 7771, loss = 0.65889532\n",
      "Iteration 7772, loss = 0.65888813\n",
      "Iteration 7773, loss = 0.65889171\n",
      "Iteration 7774, loss = 0.65887799\n",
      "Iteration 7775, loss = 0.65886640\n",
      "Iteration 7776, loss = 0.65886264\n",
      "Iteration 7777, loss = 0.65885998\n",
      "Iteration 7778, loss = 0.65885364\n",
      "Iteration 7779, loss = 0.65884705\n",
      "Iteration 7780, loss = 0.65883365\n",
      "Iteration 7781, loss = 0.65882359\n",
      "Iteration 7782, loss = 0.65882362\n",
      "Iteration 7783, loss = 0.65882177\n",
      "Iteration 7784, loss = 0.65880741\n",
      "Iteration 7785, loss = 0.65880122\n",
      "Iteration 7786, loss = 0.65879292\n",
      "Iteration 7787, loss = 0.65878555\n",
      "Iteration 7788, loss = 0.65878353\n",
      "Iteration 7789, loss = 0.65877647\n",
      "Iteration 7790, loss = 0.65876306\n",
      "Iteration 7791, loss = 0.65875657\n",
      "Iteration 7792, loss = 0.65875221\n",
      "Iteration 7793, loss = 0.65877146\n",
      "Iteration 7794, loss = 0.65874274\n",
      "Iteration 7795, loss = 0.65873673\n",
      "Iteration 7796, loss = 0.65873019\n",
      "Iteration 7797, loss = 0.65873162\n",
      "Iteration 7798, loss = 0.65871439\n",
      "Iteration 7799, loss = 0.65871055\n",
      "Iteration 7800, loss = 0.65869840\n",
      "Iteration 7801, loss = 0.65869642\n",
      "Iteration 7802, loss = 0.65868303\n",
      "Iteration 7803, loss = 0.65868671\n",
      "Iteration 7804, loss = 0.65867490\n",
      "Iteration 7805, loss = 0.65866787\n",
      "Iteration 7806, loss = 0.65867172\n",
      "Iteration 7807, loss = 0.65865548\n",
      "Iteration 7808, loss = 0.65864788\n",
      "Iteration 7809, loss = 0.65864086\n",
      "Iteration 7810, loss = 0.65863514\n",
      "Iteration 7811, loss = 0.65863693\n",
      "Iteration 7812, loss = 0.65862217\n",
      "Iteration 7813, loss = 0.65861053\n",
      "Iteration 7814, loss = 0.65860922\n",
      "Iteration 7815, loss = 0.65860373\n",
      "Iteration 7816, loss = 0.65860947\n",
      "Iteration 7817, loss = 0.65858422\n",
      "Iteration 7818, loss = 0.65859287\n",
      "Iteration 7819, loss = 0.65858001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7820, loss = 0.65856392\n",
      "Iteration 7821, loss = 0.65857300\n",
      "Iteration 7822, loss = 0.65855807\n",
      "Iteration 7823, loss = 0.65857338\n",
      "Iteration 7824, loss = 0.65854607\n",
      "Iteration 7825, loss = 0.65853408\n",
      "Iteration 7826, loss = 0.65852279\n",
      "Iteration 7827, loss = 0.65853132\n",
      "Iteration 7828, loss = 0.65851119\n",
      "Iteration 7829, loss = 0.65851167\n",
      "Iteration 7830, loss = 0.65850697\n",
      "Iteration 7831, loss = 0.65850511\n",
      "Iteration 7832, loss = 0.65848746\n",
      "Iteration 7833, loss = 0.65848603\n",
      "Iteration 7834, loss = 0.65848054\n",
      "Iteration 7835, loss = 0.65847052\n",
      "Iteration 7836, loss = 0.65846420\n",
      "Iteration 7837, loss = 0.65846009\n",
      "Iteration 7838, loss = 0.65845354\n",
      "Iteration 7839, loss = 0.65845648\n",
      "Iteration 7840, loss = 0.65843586\n",
      "Iteration 7841, loss = 0.65844466\n",
      "Iteration 7842, loss = 0.65842573\n",
      "Iteration 7843, loss = 0.65842221\n",
      "Iteration 7844, loss = 0.65842744\n",
      "Iteration 7845, loss = 0.65840507\n",
      "Iteration 7846, loss = 0.65840333\n",
      "Iteration 7847, loss = 0.65839317\n",
      "Iteration 7848, loss = 0.65838970\n",
      "Iteration 7849, loss = 0.65838584\n",
      "Iteration 7850, loss = 0.65837430\n",
      "Iteration 7851, loss = 0.65837272\n",
      "Iteration 7852, loss = 0.65835958\n",
      "Iteration 7853, loss = 0.65835753\n",
      "Iteration 7854, loss = 0.65834927\n",
      "Iteration 7855, loss = 0.65833955\n",
      "Iteration 7856, loss = 0.65835727\n",
      "Iteration 7857, loss = 0.65832961\n",
      "Iteration 7858, loss = 0.65832994\n",
      "Iteration 7859, loss = 0.65832505\n",
      "Iteration 7860, loss = 0.65831101\n",
      "Iteration 7861, loss = 0.65831023\n",
      "Iteration 7862, loss = 0.65830623\n",
      "Iteration 7863, loss = 0.65830220\n",
      "Iteration 7864, loss = 0.65828388\n",
      "Iteration 7865, loss = 0.65828908\n",
      "Iteration 7866, loss = 0.65827811\n",
      "Iteration 7867, loss = 0.65826891\n",
      "Iteration 7868, loss = 0.65825760\n",
      "Iteration 7869, loss = 0.65825615\n",
      "Iteration 7870, loss = 0.65824559\n",
      "Iteration 7871, loss = 0.65824156\n",
      "Iteration 7872, loss = 0.65824096\n",
      "Iteration 7873, loss = 0.65822892\n",
      "Iteration 7874, loss = 0.65822751\n",
      "Iteration 7875, loss = 0.65822248\n",
      "Iteration 7876, loss = 0.65821777\n",
      "Iteration 7877, loss = 0.65820633\n",
      "Iteration 7878, loss = 0.65820846\n",
      "Iteration 7879, loss = 0.65821818\n",
      "Iteration 7880, loss = 0.65819231\n",
      "Iteration 7881, loss = 0.65817786\n",
      "Iteration 7882, loss = 0.65819616\n",
      "Iteration 7883, loss = 0.65816686\n",
      "Iteration 7884, loss = 0.65816611\n",
      "Iteration 7885, loss = 0.65816655\n",
      "Iteration 7886, loss = 0.65814454\n",
      "Iteration 7887, loss = 0.65813987\n",
      "Iteration 7888, loss = 0.65813571\n",
      "Iteration 7889, loss = 0.65814126\n",
      "Iteration 7890, loss = 0.65813067\n",
      "Iteration 7891, loss = 0.65811858\n",
      "Iteration 7892, loss = 0.65811719\n",
      "Iteration 7893, loss = 0.65810686\n",
      "Iteration 7894, loss = 0.65810020\n",
      "Iteration 7895, loss = 0.65809278\n",
      "Iteration 7896, loss = 0.65808984\n",
      "Iteration 7897, loss = 0.65808256\n",
      "Iteration 7898, loss = 0.65807573\n",
      "Iteration 7899, loss = 0.65806356\n",
      "Iteration 7900, loss = 0.65807045\n",
      "Iteration 7901, loss = 0.65805356\n",
      "Iteration 7902, loss = 0.65805080\n",
      "Iteration 7903, loss = 0.65804730\n",
      "Iteration 7904, loss = 0.65803985\n",
      "Iteration 7905, loss = 0.65804722\n",
      "Iteration 7906, loss = 0.65803203\n",
      "Iteration 7907, loss = 0.65801934\n",
      "Iteration 7908, loss = 0.65801132\n",
      "Iteration 7909, loss = 0.65800543\n",
      "Iteration 7910, loss = 0.65800532\n",
      "Iteration 7911, loss = 0.65800322\n",
      "Iteration 7912, loss = 0.65798479\n",
      "Iteration 7913, loss = 0.65798677\n",
      "Iteration 7914, loss = 0.65797991\n",
      "Iteration 7915, loss = 0.65799083\n",
      "Iteration 7916, loss = 0.65797864\n",
      "Iteration 7917, loss = 0.65797150\n",
      "Iteration 7918, loss = 0.65797326\n",
      "Iteration 7919, loss = 0.65795834\n",
      "Iteration 7920, loss = 0.65794129\n",
      "Iteration 7921, loss = 0.65793948\n",
      "Iteration 7922, loss = 0.65794108\n",
      "Iteration 7923, loss = 0.65793624\n",
      "Iteration 7924, loss = 0.65792607\n",
      "Iteration 7925, loss = 0.65791457\n",
      "Iteration 7926, loss = 0.65792077\n",
      "Iteration 7927, loss = 0.65790608\n",
      "Iteration 7928, loss = 0.65789679\n",
      "Iteration 7929, loss = 0.65789927\n",
      "Iteration 7930, loss = 0.65789208\n",
      "Iteration 7931, loss = 0.65788729\n",
      "Iteration 7932, loss = 0.65788763\n",
      "Iteration 7933, loss = 0.65787252\n",
      "Iteration 7934, loss = 0.65786611\n",
      "Iteration 7935, loss = 0.65786370\n",
      "Iteration 7936, loss = 0.65785996\n",
      "Iteration 7937, loss = 0.65784903\n",
      "Iteration 7938, loss = 0.65785424\n",
      "Iteration 7939, loss = 0.65784244\n",
      "Iteration 7940, loss = 0.65783681\n",
      "Iteration 7941, loss = 0.65783380\n",
      "Iteration 7942, loss = 0.65782769\n",
      "Iteration 7943, loss = 0.65782190\n",
      "Iteration 7944, loss = 0.65782310\n",
      "Iteration 7945, loss = 0.65781309\n",
      "Iteration 7946, loss = 0.65781652\n",
      "Iteration 7947, loss = 0.65781720\n",
      "Iteration 7948, loss = 0.65782095\n",
      "Iteration 7949, loss = 0.65780823\n",
      "Iteration 7950, loss = 0.65779321\n",
      "Iteration 7951, loss = 0.65778978\n",
      "Iteration 7952, loss = 0.65778627\n",
      "Iteration 7953, loss = 0.65778744\n",
      "Iteration 7954, loss = 0.65778359\n",
      "Iteration 7955, loss = 0.65776869\n",
      "Iteration 7956, loss = 0.65776308\n",
      "Iteration 7957, loss = 0.65776763\n",
      "Iteration 7958, loss = 0.65775830\n",
      "Iteration 7959, loss = 0.65775619\n",
      "Iteration 7960, loss = 0.65774764\n",
      "Iteration 7961, loss = 0.65774046\n",
      "Iteration 7962, loss = 0.65774922\n",
      "Iteration 7963, loss = 0.65773574\n",
      "Iteration 7964, loss = 0.65773465\n",
      "Iteration 7965, loss = 0.65773406\n",
      "Iteration 7966, loss = 0.65771545\n",
      "Iteration 7967, loss = 0.65771944\n",
      "Iteration 7968, loss = 0.65770801\n",
      "Iteration 7969, loss = 0.65770628\n",
      "Iteration 7970, loss = 0.65770481\n",
      "Iteration 7971, loss = 0.65770332\n",
      "Iteration 7972, loss = 0.65769109\n",
      "Iteration 7973, loss = 0.65768712\n",
      "Iteration 7974, loss = 0.65769025\n",
      "Iteration 7975, loss = 0.65767594\n",
      "Iteration 7976, loss = 0.65767760\n",
      "Iteration 7977, loss = 0.65770255\n",
      "Iteration 7978, loss = 0.65766589\n",
      "Iteration 7979, loss = 0.65766446\n",
      "Iteration 7980, loss = 0.65765520\n",
      "Iteration 7981, loss = 0.65765776\n",
      "Iteration 7982, loss = 0.65765066\n",
      "Iteration 7983, loss = 0.65764588\n",
      "Iteration 7984, loss = 0.65764025\n",
      "Iteration 7985, loss = 0.65763264\n",
      "Iteration 7986, loss = 0.65763441\n",
      "Iteration 7987, loss = 0.65763194\n",
      "Iteration 7988, loss = 0.65762546\n",
      "Iteration 7989, loss = 0.65762458\n",
      "Iteration 7990, loss = 0.65761524\n",
      "Iteration 7991, loss = 0.65762509\n",
      "Iteration 7992, loss = 0.65761777\n",
      "Iteration 7993, loss = 0.65761294\n",
      "Iteration 7994, loss = 0.65761414\n",
      "Iteration 7995, loss = 0.65762332\n",
      "Iteration 7996, loss = 0.65759368\n",
      "Iteration 7997, loss = 0.65759031\n",
      "Iteration 7998, loss = 0.65757577\n",
      "Iteration 7999, loss = 0.65758539\n",
      "Iteration 8000, loss = 0.65757018\n",
      "Iteration 8001, loss = 0.65756907\n",
      "Iteration 8002, loss = 0.65756833\n",
      "Iteration 8003, loss = 0.65755673\n",
      "Iteration 8004, loss = 0.65756302\n",
      "Iteration 8005, loss = 0.65755089\n",
      "Iteration 8006, loss = 0.65755118\n",
      "Iteration 8007, loss = 0.65754170\n",
      "Iteration 8008, loss = 0.65753668\n",
      "Iteration 8009, loss = 0.65756524\n",
      "Iteration 8010, loss = 0.65752705\n",
      "Iteration 8011, loss = 0.65752233\n",
      "Iteration 8012, loss = 0.65752609\n",
      "Iteration 8013, loss = 0.65752161\n",
      "Iteration 8014, loss = 0.65750818\n",
      "Iteration 8015, loss = 0.65752262\n",
      "Iteration 8016, loss = 0.65750377\n",
      "Iteration 8017, loss = 0.65749763\n",
      "Iteration 8018, loss = 0.65749464\n",
      "Iteration 8019, loss = 0.65749637\n",
      "Iteration 8020, loss = 0.65748115\n",
      "Iteration 8021, loss = 0.65748306\n",
      "Iteration 8022, loss = 0.65747543\n",
      "Iteration 8023, loss = 0.65747324\n",
      "Iteration 8024, loss = 0.65746599\n",
      "Iteration 8025, loss = 0.65746736\n",
      "Iteration 8026, loss = 0.65746692\n",
      "Iteration 8027, loss = 0.65745607\n",
      "Iteration 8028, loss = 0.65745686\n",
      "Iteration 8029, loss = 0.65745746\n",
      "Iteration 8030, loss = 0.65744010\n",
      "Iteration 8031, loss = 0.65744539\n",
      "Iteration 8032, loss = 0.65745605\n",
      "Iteration 8033, loss = 0.65743501\n",
      "Iteration 8034, loss = 0.65742931\n",
      "Iteration 8035, loss = 0.65742302\n",
      "Iteration 8036, loss = 0.65742649\n",
      "Iteration 8037, loss = 0.65741896\n",
      "Iteration 8038, loss = 0.65741977\n",
      "Iteration 8039, loss = 0.65740100\n",
      "Iteration 8040, loss = 0.65741472\n",
      "Iteration 8041, loss = 0.65741615\n",
      "Iteration 8042, loss = 0.65739922\n",
      "Iteration 8043, loss = 0.65739163\n",
      "Iteration 8044, loss = 0.65738990\n",
      "Iteration 8045, loss = 0.65740025\n",
      "Iteration 8046, loss = 0.65738168\n",
      "Iteration 8047, loss = 0.65737192\n",
      "Iteration 8048, loss = 0.65737217\n",
      "Iteration 8049, loss = 0.65736704\n",
      "Iteration 8050, loss = 0.65736492\n",
      "Iteration 8051, loss = 0.65736039\n",
      "Iteration 8052, loss = 0.65735660\n",
      "Iteration 8053, loss = 0.65734685\n",
      "Iteration 8054, loss = 0.65735077\n",
      "Iteration 8055, loss = 0.65734309\n",
      "Iteration 8056, loss = 0.65733221\n",
      "Iteration 8057, loss = 0.65734383\n",
      "Iteration 8058, loss = 0.65733578\n",
      "Iteration 8059, loss = 0.65732598\n",
      "Iteration 8060, loss = 0.65731542\n",
      "Iteration 8061, loss = 0.65731570\n",
      "Iteration 8062, loss = 0.65732825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8063, loss = 0.65731261\n",
      "Iteration 8064, loss = 0.65729933\n",
      "Iteration 8065, loss = 0.65730205\n",
      "Iteration 8066, loss = 0.65728855\n",
      "Iteration 8067, loss = 0.65729262\n",
      "Iteration 8068, loss = 0.65728282\n",
      "Iteration 8069, loss = 0.65728343\n",
      "Iteration 8070, loss = 0.65727764\n",
      "Iteration 8071, loss = 0.65728341\n",
      "Iteration 8072, loss = 0.65727686\n",
      "Iteration 8073, loss = 0.65727448\n",
      "Iteration 8074, loss = 0.65725880\n",
      "Iteration 8075, loss = 0.65726192\n",
      "Iteration 8076, loss = 0.65727549\n",
      "Iteration 8077, loss = 0.65724735\n",
      "Iteration 8078, loss = 0.65725979\n",
      "Iteration 8079, loss = 0.65723889\n",
      "Iteration 8080, loss = 0.65723708\n",
      "Iteration 8081, loss = 0.65723079\n",
      "Iteration 8082, loss = 0.65722467\n",
      "Iteration 8083, loss = 0.65722925\n",
      "Iteration 8084, loss = 0.65722967\n",
      "Iteration 8085, loss = 0.65722350\n",
      "Iteration 8086, loss = 0.65721551\n",
      "Iteration 8087, loss = 0.65720796\n",
      "Iteration 8088, loss = 0.65720630\n",
      "Iteration 8089, loss = 0.65720967\n",
      "Iteration 8090, loss = 0.65719124\n",
      "Iteration 8091, loss = 0.65718742\n",
      "Iteration 8092, loss = 0.65719478\n",
      "Iteration 8093, loss = 0.65719817\n",
      "Iteration 8094, loss = 0.65718554\n",
      "Iteration 8095, loss = 0.65717597\n",
      "Iteration 8096, loss = 0.65717615\n",
      "Iteration 8097, loss = 0.65717078\n",
      "Iteration 8098, loss = 0.65716588\n",
      "Iteration 8099, loss = 0.65716290\n",
      "Iteration 8100, loss = 0.65715509\n",
      "Iteration 8101, loss = 0.65715991\n",
      "Iteration 8102, loss = 0.65714833\n",
      "Iteration 8103, loss = 0.65715240\n",
      "Iteration 8104, loss = 0.65714547\n",
      "Iteration 8105, loss = 0.65713829\n",
      "Iteration 8106, loss = 0.65713315\n",
      "Iteration 8107, loss = 0.65715145\n",
      "Iteration 8108, loss = 0.65713085\n",
      "Iteration 8109, loss = 0.65712827\n",
      "Iteration 8110, loss = 0.65713049\n",
      "Iteration 8111, loss = 0.65712389\n",
      "Iteration 8112, loss = 0.65711795\n",
      "Iteration 8113, loss = 0.65710702\n",
      "Iteration 8114, loss = 0.65710270\n",
      "Iteration 8115, loss = 0.65710649\n",
      "Iteration 8116, loss = 0.65710072\n",
      "Iteration 8117, loss = 0.65710232\n",
      "Iteration 8118, loss = 0.65710059\n",
      "Iteration 8119, loss = 0.65709336\n",
      "Iteration 8120, loss = 0.65708295\n",
      "Iteration 8121, loss = 0.65708439\n",
      "Iteration 8122, loss = 0.65708393\n",
      "Iteration 8123, loss = 0.65708807\n",
      "Iteration 8124, loss = 0.65708784\n",
      "Iteration 8125, loss = 0.65706678\n",
      "Iteration 8126, loss = 0.65706202\n",
      "Iteration 8127, loss = 0.65705757\n",
      "Iteration 8128, loss = 0.65706106\n",
      "Iteration 8129, loss = 0.65705090\n",
      "Iteration 8130, loss = 0.65705127\n",
      "Iteration 8131, loss = 0.65704503\n",
      "Iteration 8132, loss = 0.65704313\n",
      "Iteration 8133, loss = 0.65703313\n",
      "Iteration 8134, loss = 0.65703790\n",
      "Iteration 8135, loss = 0.65703367\n",
      "Iteration 8136, loss = 0.65702500\n",
      "Iteration 8137, loss = 0.65701481\n",
      "Iteration 8138, loss = 0.65701560\n",
      "Iteration 8139, loss = 0.65701968\n",
      "Iteration 8140, loss = 0.65700763\n",
      "Iteration 8141, loss = 0.65699994\n",
      "Iteration 8142, loss = 0.65700260\n",
      "Iteration 8143, loss = 0.65699921\n",
      "Iteration 8144, loss = 0.65698430\n",
      "Iteration 8145, loss = 0.65698346\n",
      "Iteration 8146, loss = 0.65699280\n",
      "Iteration 8147, loss = 0.65697666\n",
      "Iteration 8148, loss = 0.65697044\n",
      "Iteration 8149, loss = 0.65696699\n",
      "Iteration 8150, loss = 0.65696025\n",
      "Iteration 8151, loss = 0.65695742\n",
      "Iteration 8152, loss = 0.65696032\n",
      "Iteration 8153, loss = 0.65694262\n",
      "Iteration 8154, loss = 0.65694029\n",
      "Iteration 8155, loss = 0.65694278\n",
      "Iteration 8156, loss = 0.65693043\n",
      "Iteration 8157, loss = 0.65693108\n",
      "Iteration 8158, loss = 0.65692876\n",
      "Iteration 8159, loss = 0.65692251\n",
      "Iteration 8160, loss = 0.65691409\n",
      "Iteration 8161, loss = 0.65691720\n",
      "Iteration 8162, loss = 0.65691702\n",
      "Iteration 8163, loss = 0.65692498\n",
      "Iteration 8164, loss = 0.65690146\n",
      "Iteration 8165, loss = 0.65690066\n",
      "Iteration 8166, loss = 0.65689137\n",
      "Iteration 8167, loss = 0.65689028\n",
      "Iteration 8168, loss = 0.65688354\n",
      "Iteration 8169, loss = 0.65687927\n",
      "Iteration 8170, loss = 0.65687730\n",
      "Iteration 8171, loss = 0.65686829\n",
      "Iteration 8172, loss = 0.65688338\n",
      "Iteration 8173, loss = 0.65686752\n",
      "Iteration 8174, loss = 0.65686181\n",
      "Iteration 8175, loss = 0.65685641\n",
      "Iteration 8176, loss = 0.65684708\n",
      "Iteration 8177, loss = 0.65684973\n",
      "Iteration 8178, loss = 0.65684036\n",
      "Iteration 8179, loss = 0.65684447\n",
      "Iteration 8180, loss = 0.65683732\n",
      "Iteration 8181, loss = 0.65683936\n",
      "Iteration 8182, loss = 0.65683023\n",
      "Iteration 8183, loss = 0.65683369\n",
      "Iteration 8184, loss = 0.65682152\n",
      "Iteration 8185, loss = 0.65681777\n",
      "Iteration 8186, loss = 0.65681107\n",
      "Iteration 8187, loss = 0.65680823\n",
      "Iteration 8188, loss = 0.65682057\n",
      "Iteration 8189, loss = 0.65679310\n",
      "Iteration 8190, loss = 0.65679971\n",
      "Iteration 8191, loss = 0.65679188\n",
      "Iteration 8192, loss = 0.65679926\n",
      "Iteration 8193, loss = 0.65678344\n",
      "Iteration 8194, loss = 0.65677424\n",
      "Iteration 8195, loss = 0.65678106\n",
      "Iteration 8196, loss = 0.65677250\n",
      "Iteration 8197, loss = 0.65676598\n",
      "Iteration 8198, loss = 0.65676768\n",
      "Iteration 8199, loss = 0.65676142\n",
      "Iteration 8200, loss = 0.65675095\n",
      "Iteration 8201, loss = 0.65674876\n",
      "Iteration 8202, loss = 0.65674228\n",
      "Iteration 8203, loss = 0.65674430\n",
      "Iteration 8204, loss = 0.65673939\n",
      "Iteration 8205, loss = 0.65673177\n",
      "Iteration 8206, loss = 0.65672971\n",
      "Iteration 8207, loss = 0.65672156\n",
      "Iteration 8208, loss = 0.65672620\n",
      "Iteration 8209, loss = 0.65671392\n",
      "Iteration 8210, loss = 0.65673237\n",
      "Iteration 8211, loss = 0.65671551\n",
      "Iteration 8212, loss = 0.65670685\n",
      "Iteration 8213, loss = 0.65670001\n",
      "Iteration 8214, loss = 0.65670134\n",
      "Iteration 8215, loss = 0.65672161\n",
      "Iteration 8216, loss = 0.65669105\n",
      "Iteration 8217, loss = 0.65669460\n",
      "Iteration 8218, loss = 0.65667874\n",
      "Iteration 8219, loss = 0.65669580\n",
      "Iteration 8220, loss = 0.65667385\n",
      "Iteration 8221, loss = 0.65667267\n",
      "Iteration 8222, loss = 0.65666988\n",
      "Iteration 8223, loss = 0.65668237\n",
      "Iteration 8224, loss = 0.65665945\n",
      "Iteration 8225, loss = 0.65666779\n",
      "Iteration 8226, loss = 0.65666920\n",
      "Iteration 8227, loss = 0.65664744\n",
      "Iteration 8228, loss = 0.65665228\n",
      "Iteration 8229, loss = 0.65663757\n",
      "Iteration 8230, loss = 0.65663352\n",
      "Iteration 8231, loss = 0.65663716\n",
      "Iteration 8232, loss = 0.65662878\n",
      "Iteration 8233, loss = 0.65663059\n",
      "Iteration 8234, loss = 0.65662969\n",
      "Iteration 8235, loss = 0.65664393\n",
      "Iteration 8236, loss = 0.65660966\n",
      "Iteration 8237, loss = 0.65660873\n",
      "Iteration 8238, loss = 0.65660953\n",
      "Iteration 8239, loss = 0.65660728\n",
      "Iteration 8240, loss = 0.65658373\n",
      "Iteration 8241, loss = 0.65657699\n",
      "Iteration 8242, loss = 0.65656841\n",
      "Iteration 8243, loss = 0.65655768\n",
      "Iteration 8244, loss = 0.65654656\n",
      "Iteration 8245, loss = 0.65655280\n",
      "Iteration 8246, loss = 0.65654083\n",
      "Iteration 8247, loss = 0.65653818\n",
      "Iteration 8248, loss = 0.65651580\n",
      "Iteration 8249, loss = 0.65651614\n",
      "Iteration 8250, loss = 0.65650989\n",
      "Iteration 8251, loss = 0.65648878\n",
      "Iteration 8252, loss = 0.65648522\n",
      "Iteration 8253, loss = 0.65647716\n",
      "Iteration 8254, loss = 0.65647075\n",
      "Iteration 8255, loss = 0.65646075\n",
      "Iteration 8256, loss = 0.65645157\n",
      "Iteration 8257, loss = 0.65644250\n",
      "Iteration 8258, loss = 0.65644524\n",
      "Iteration 8259, loss = 0.65643195\n",
      "Iteration 8260, loss = 0.65644225\n",
      "Iteration 8261, loss = 0.65641781\n",
      "Iteration 8262, loss = 0.65642790\n",
      "Iteration 8263, loss = 0.65640137\n",
      "Iteration 8264, loss = 0.65640176\n",
      "Iteration 8265, loss = 0.65640775\n",
      "Iteration 8266, loss = 0.65637208\n",
      "Iteration 8267, loss = 0.65637398\n",
      "Iteration 8268, loss = 0.65639918\n",
      "Iteration 8269, loss = 0.65635000\n",
      "Iteration 8270, loss = 0.65634677\n",
      "Iteration 8271, loss = 0.65634367\n",
      "Iteration 8272, loss = 0.65632338\n",
      "Iteration 8273, loss = 0.65631318\n",
      "Iteration 8274, loss = 0.65632350\n",
      "Iteration 8275, loss = 0.65631542\n",
      "Iteration 8276, loss = 0.65629832\n",
      "Iteration 8277, loss = 0.65629112\n",
      "Iteration 8278, loss = 0.65627563\n",
      "Iteration 8279, loss = 0.65627587\n",
      "Iteration 8280, loss = 0.65627021\n",
      "Iteration 8281, loss = 0.65626799\n",
      "Iteration 8282, loss = 0.65626307\n",
      "Iteration 8283, loss = 0.65625067\n",
      "Iteration 8284, loss = 0.65625102\n",
      "Iteration 8285, loss = 0.65623399\n",
      "Iteration 8286, loss = 0.65622420\n",
      "Iteration 8287, loss = 0.65622526\n",
      "Iteration 8288, loss = 0.65621133\n",
      "Iteration 8289, loss = 0.65620473\n",
      "Iteration 8290, loss = 0.65621567\n",
      "Iteration 8291, loss = 0.65618847\n",
      "Iteration 8292, loss = 0.65619485\n",
      "Iteration 8293, loss = 0.65617954\n",
      "Iteration 8294, loss = 0.65618149\n",
      "Iteration 8295, loss = 0.65616727\n",
      "Iteration 8296, loss = 0.65615877\n",
      "Iteration 8297, loss = 0.65616679\n",
      "Iteration 8298, loss = 0.65615181\n",
      "Iteration 8299, loss = 0.65614326\n",
      "Iteration 8300, loss = 0.65613957\n",
      "Iteration 8301, loss = 0.65615424\n",
      "Iteration 8302, loss = 0.65611644\n",
      "Iteration 8303, loss = 0.65611870\n",
      "Iteration 8304, loss = 0.65612725\n",
      "Iteration 8305, loss = 0.65610099\n",
      "Iteration 8306, loss = 0.65608922\n",
      "Iteration 8307, loss = 0.65608875\n",
      "Iteration 8308, loss = 0.65608009\n",
      "Iteration 8309, loss = 0.65607184\n",
      "Iteration 8310, loss = 0.65606957\n",
      "Iteration 8311, loss = 0.65606250\n",
      "Iteration 8312, loss = 0.65605309\n",
      "Iteration 8313, loss = 0.65604935\n",
      "Iteration 8314, loss = 0.65606493\n",
      "Iteration 8315, loss = 0.65603512\n",
      "Iteration 8316, loss = 0.65604479\n",
      "Iteration 8317, loss = 0.65604329\n",
      "Iteration 8318, loss = 0.65601271\n",
      "Iteration 8319, loss = 0.65601115\n",
      "Iteration 8320, loss = 0.65600185\n",
      "Iteration 8321, loss = 0.65599294\n",
      "Iteration 8322, loss = 0.65599352\n",
      "Iteration 8323, loss = 0.65598244\n",
      "Iteration 8324, loss = 0.65598148\n",
      "Iteration 8325, loss = 0.65597202\n",
      "Iteration 8326, loss = 0.65596553\n",
      "Iteration 8327, loss = 0.65595696\n",
      "Iteration 8328, loss = 0.65595438\n",
      "Iteration 8329, loss = 0.65594666\n",
      "Iteration 8330, loss = 0.65594243\n",
      "Iteration 8331, loss = 0.65595968\n",
      "Iteration 8332, loss = 0.65593553\n",
      "Iteration 8333, loss = 0.65593586\n",
      "Iteration 8334, loss = 0.65591668\n",
      "Iteration 8335, loss = 0.65591641\n",
      "Iteration 8336, loss = 0.65590915\n",
      "Iteration 8337, loss = 0.65590741\n",
      "Iteration 8338, loss = 0.65590498\n",
      "Iteration 8339, loss = 0.65589401\n",
      "Iteration 8340, loss = 0.65588846\n",
      "Iteration 8341, loss = 0.65588970\n",
      "Iteration 8342, loss = 0.65588133\n",
      "Iteration 8343, loss = 0.65587945\n",
      "Iteration 8344, loss = 0.65587740\n",
      "Iteration 8345, loss = 0.65587047\n",
      "Iteration 8346, loss = 0.65586430\n",
      "Iteration 8347, loss = 0.65586103\n",
      "Iteration 8348, loss = 0.65585802\n",
      "Iteration 8349, loss = 0.65585315\n",
      "Iteration 8350, loss = 0.65583141\n",
      "Iteration 8351, loss = 0.65581710\n",
      "Iteration 8352, loss = 0.65581591\n",
      "Iteration 8353, loss = 0.65579851\n",
      "Iteration 8354, loss = 0.65578282\n",
      "Iteration 8355, loss = 0.65576507\n",
      "Iteration 8356, loss = 0.65574771\n",
      "Iteration 8357, loss = 0.65574702\n",
      "Iteration 8358, loss = 0.65572316\n",
      "Iteration 8359, loss = 0.65570599\n",
      "Iteration 8360, loss = 0.65569140\n",
      "Iteration 8361, loss = 0.65568175\n",
      "Iteration 8362, loss = 0.65567292\n",
      "Iteration 8363, loss = 0.65564608\n",
      "Iteration 8364, loss = 0.65566072\n",
      "Iteration 8365, loss = 0.65563629\n",
      "Iteration 8366, loss = 0.65561533\n",
      "Iteration 8367, loss = 0.65559761\n",
      "Iteration 8368, loss = 0.65560782\n",
      "Iteration 8369, loss = 0.65558996\n",
      "Iteration 8370, loss = 0.65556672\n",
      "Iteration 8371, loss = 0.65555738\n",
      "Iteration 8372, loss = 0.65556680\n",
      "Iteration 8373, loss = 0.65553647\n",
      "Iteration 8374, loss = 0.65554240\n",
      "Iteration 8375, loss = 0.65552313\n",
      "Iteration 8376, loss = 0.65550848\n",
      "Iteration 8377, loss = 0.65549380\n",
      "Iteration 8378, loss = 0.65547686\n",
      "Iteration 8379, loss = 0.65546922\n",
      "Iteration 8380, loss = 0.65546168\n",
      "Iteration 8381, loss = 0.65544912\n",
      "Iteration 8382, loss = 0.65543779\n",
      "Iteration 8383, loss = 0.65544639\n",
      "Iteration 8384, loss = 0.65541326\n",
      "Iteration 8385, loss = 0.65540859\n",
      "Iteration 8386, loss = 0.65539799\n",
      "Iteration 8387, loss = 0.65539620\n",
      "Iteration 8388, loss = 0.65537760\n",
      "Iteration 8389, loss = 0.65539585\n",
      "Iteration 8390, loss = 0.65535961\n",
      "Iteration 8391, loss = 0.65534592\n",
      "Iteration 8392, loss = 0.65533796\n",
      "Iteration 8393, loss = 0.65532810\n",
      "Iteration 8394, loss = 0.65530671\n",
      "Iteration 8395, loss = 0.65530067\n",
      "Iteration 8396, loss = 0.65529504\n",
      "Iteration 8397, loss = 0.65527819\n",
      "Iteration 8398, loss = 0.65527504\n",
      "Iteration 8399, loss = 0.65526676\n",
      "Iteration 8400, loss = 0.65524698\n",
      "Iteration 8401, loss = 0.65524390\n",
      "Iteration 8402, loss = 0.65524056\n",
      "Iteration 8403, loss = 0.65522383\n",
      "Iteration 8404, loss = 0.65520602\n",
      "Iteration 8405, loss = 0.65520206\n",
      "Iteration 8406, loss = 0.65519047\n",
      "Iteration 8407, loss = 0.65517655\n",
      "Iteration 8408, loss = 0.65517369\n",
      "Iteration 8409, loss = 0.65517322\n",
      "Iteration 8410, loss = 0.65515603\n",
      "Iteration 8411, loss = 0.65514948\n",
      "Iteration 8412, loss = 0.65513593\n",
      "Iteration 8413, loss = 0.65512404\n",
      "Iteration 8414, loss = 0.65512075\n",
      "Iteration 8415, loss = 0.65510656\n",
      "Iteration 8416, loss = 0.65511269\n",
      "Iteration 8417, loss = 0.65508432\n",
      "Iteration 8418, loss = 0.65508059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8419, loss = 0.65506623\n",
      "Iteration 8420, loss = 0.65505902\n",
      "Iteration 8421, loss = 0.65505232\n",
      "Iteration 8422, loss = 0.65504292\n",
      "Iteration 8423, loss = 0.65505191\n",
      "Iteration 8424, loss = 0.65501687\n",
      "Iteration 8425, loss = 0.65501117\n",
      "Iteration 8426, loss = 0.65500128\n",
      "Iteration 8427, loss = 0.65499305\n",
      "Iteration 8428, loss = 0.65498093\n",
      "Iteration 8429, loss = 0.65497108\n",
      "Iteration 8430, loss = 0.65496448\n",
      "Iteration 8431, loss = 0.65496999\n",
      "Iteration 8432, loss = 0.65495099\n",
      "Iteration 8433, loss = 0.65493974\n",
      "Iteration 8434, loss = 0.65493018\n",
      "Iteration 8435, loss = 0.65492065\n",
      "Iteration 8436, loss = 0.65491421\n",
      "Iteration 8437, loss = 0.65489794\n",
      "Iteration 8438, loss = 0.65489349\n",
      "Iteration 8439, loss = 0.65488618\n",
      "Iteration 8440, loss = 0.65488087\n",
      "Iteration 8441, loss = 0.65486723\n",
      "Iteration 8442, loss = 0.65487581\n",
      "Iteration 8443, loss = 0.65487126\n",
      "Iteration 8444, loss = 0.65484849\n",
      "Iteration 8445, loss = 0.65483456\n",
      "Iteration 8446, loss = 0.65482892\n",
      "Iteration 8447, loss = 0.65482607\n",
      "Iteration 8448, loss = 0.65481233\n",
      "Iteration 8449, loss = 0.65480147\n",
      "Iteration 8450, loss = 0.65480947\n",
      "Iteration 8451, loss = 0.65478706\n",
      "Iteration 8452, loss = 0.65478151\n",
      "Iteration 8453, loss = 0.65477471\n",
      "Iteration 8454, loss = 0.65476709\n",
      "Iteration 8455, loss = 0.65476157\n",
      "Iteration 8456, loss = 0.65474910\n",
      "Iteration 8457, loss = 0.65474834\n",
      "Iteration 8458, loss = 0.65474712\n",
      "Iteration 8459, loss = 0.65473777\n",
      "Iteration 8460, loss = 0.65472153\n",
      "Iteration 8461, loss = 0.65472871\n",
      "Iteration 8462, loss = 0.65471489\n",
      "Iteration 8463, loss = 0.65471062\n",
      "Iteration 8464, loss = 0.65470309\n",
      "Iteration 8465, loss = 0.65469928\n",
      "Iteration 8466, loss = 0.65467239\n",
      "Iteration 8467, loss = 0.65466779\n",
      "Iteration 8468, loss = 0.65465963\n",
      "Iteration 8469, loss = 0.65468614\n",
      "Iteration 8470, loss = 0.65464936\n",
      "Iteration 8471, loss = 0.65463750\n",
      "Iteration 8472, loss = 0.65463239\n",
      "Iteration 8473, loss = 0.65461924\n",
      "Iteration 8474, loss = 0.65462342\n",
      "Iteration 8475, loss = 0.65462055\n",
      "Iteration 8476, loss = 0.65459533\n",
      "Iteration 8477, loss = 0.65460262\n",
      "Iteration 8478, loss = 0.65459332\n",
      "Iteration 8479, loss = 0.65458222\n",
      "Iteration 8480, loss = 0.65456818\n",
      "Iteration 8481, loss = 0.65456113\n",
      "Iteration 8482, loss = 0.65455974\n",
      "Iteration 8483, loss = 0.65455484\n",
      "Iteration 8484, loss = 0.65458745\n",
      "Iteration 8485, loss = 0.65454378\n",
      "Iteration 8486, loss = 0.65453728\n",
      "Iteration 8487, loss = 0.65453018\n",
      "Iteration 8488, loss = 0.65451250\n",
      "Iteration 8489, loss = 0.65451152\n",
      "Iteration 8490, loss = 0.65451210\n",
      "Iteration 8491, loss = 0.65449843\n",
      "Iteration 8492, loss = 0.65449432\n",
      "Iteration 8493, loss = 0.65448395\n",
      "Iteration 8494, loss = 0.65448879\n",
      "Iteration 8495, loss = 0.65447532\n",
      "Iteration 8496, loss = 0.65446442\n",
      "Iteration 8497, loss = 0.65445406\n",
      "Iteration 8498, loss = 0.65444780\n",
      "Iteration 8499, loss = 0.65444055\n",
      "Iteration 8500, loss = 0.65443830\n",
      "Iteration 8501, loss = 0.65442986\n",
      "Iteration 8502, loss = 0.65442373\n",
      "Iteration 8503, loss = 0.65441722\n",
      "Iteration 8504, loss = 0.65440883\n",
      "Iteration 8505, loss = 0.65440610\n",
      "Iteration 8506, loss = 0.65439995\n",
      "Iteration 8507, loss = 0.65439472\n",
      "Iteration 8508, loss = 0.65438390\n",
      "Iteration 8509, loss = 0.65439639\n",
      "Iteration 8510, loss = 0.65438307\n",
      "Iteration 8511, loss = 0.65437253\n",
      "Iteration 8512, loss = 0.65436169\n",
      "Iteration 8513, loss = 0.65435496\n",
      "Iteration 8514, loss = 0.65435019\n",
      "Iteration 8515, loss = 0.65433824\n",
      "Iteration 8516, loss = 0.65433674\n",
      "Iteration 8517, loss = 0.65433774\n",
      "Iteration 8518, loss = 0.65431898\n",
      "Iteration 8519, loss = 0.65432241\n",
      "Iteration 8520, loss = 0.65431947\n",
      "Iteration 8521, loss = 0.65432716\n",
      "Iteration 8522, loss = 0.65429113\n",
      "Iteration 8523, loss = 0.65428126\n",
      "Iteration 8524, loss = 0.65429676\n",
      "Iteration 8525, loss = 0.65427627\n",
      "Iteration 8526, loss = 0.65426507\n",
      "Iteration 8527, loss = 0.65427260\n",
      "Iteration 8528, loss = 0.65426208\n",
      "Iteration 8529, loss = 0.65425948\n",
      "Iteration 8530, loss = 0.65424879\n",
      "Iteration 8531, loss = 0.65423211\n",
      "Iteration 8532, loss = 0.65424840\n",
      "Iteration 8533, loss = 0.65423125\n",
      "Iteration 8534, loss = 0.65425039\n",
      "Iteration 8535, loss = 0.65421008\n",
      "Iteration 8536, loss = 0.65421790\n",
      "Iteration 8537, loss = 0.65419566\n",
      "Iteration 8538, loss = 0.65419968\n",
      "Iteration 8539, loss = 0.65418703\n",
      "Iteration 8540, loss = 0.65419103\n",
      "Iteration 8541, loss = 0.65417335\n",
      "Iteration 8542, loss = 0.65416968\n",
      "Iteration 8543, loss = 0.65416910\n",
      "Iteration 8544, loss = 0.65415583\n",
      "Iteration 8545, loss = 0.65416832\n",
      "Iteration 8546, loss = 0.65414659\n",
      "Iteration 8547, loss = 0.65414850\n",
      "Iteration 8548, loss = 0.65413922\n",
      "Iteration 8549, loss = 0.65413987\n",
      "Iteration 8550, loss = 0.65411999\n",
      "Iteration 8551, loss = 0.65411473\n",
      "Iteration 8552, loss = 0.65412033\n",
      "Iteration 8553, loss = 0.65409616\n",
      "Iteration 8554, loss = 0.65411026\n",
      "Iteration 8555, loss = 0.65410321\n",
      "Iteration 8556, loss = 0.65408395\n",
      "Iteration 8557, loss = 0.65407746\n",
      "Iteration 8558, loss = 0.65408355\n",
      "Iteration 8559, loss = 0.65408279\n",
      "Iteration 8560, loss = 0.65405868\n",
      "Iteration 8561, loss = 0.65405884\n",
      "Iteration 8562, loss = 0.65404567\n",
      "Iteration 8563, loss = 0.65404149\n",
      "Iteration 8564, loss = 0.65403447\n",
      "Iteration 8565, loss = 0.65402385\n",
      "Iteration 8566, loss = 0.65401841\n",
      "Iteration 8567, loss = 0.65400891\n",
      "Iteration 8568, loss = 0.65399748\n",
      "Iteration 8569, loss = 0.65399911\n",
      "Iteration 8570, loss = 0.65399710\n",
      "Iteration 8571, loss = 0.65398164\n",
      "Iteration 8572, loss = 0.65397230\n",
      "Iteration 8573, loss = 0.65396082\n",
      "Iteration 8574, loss = 0.65395056\n",
      "Iteration 8575, loss = 0.65394690\n",
      "Iteration 8576, loss = 0.65395066\n",
      "Iteration 8577, loss = 0.65395663\n",
      "Iteration 8578, loss = 0.65392181\n",
      "Iteration 8579, loss = 0.65390986\n",
      "Iteration 8580, loss = 0.65390660\n",
      "Iteration 8581, loss = 0.65390513\n",
      "Iteration 8582, loss = 0.65389029\n",
      "Iteration 8583, loss = 0.65388780\n",
      "Iteration 8584, loss = 0.65389303\n",
      "Iteration 8585, loss = 0.65386578\n",
      "Iteration 8586, loss = 0.65386789\n",
      "Iteration 8587, loss = 0.65385377\n",
      "Iteration 8588, loss = 0.65386006\n",
      "Iteration 8589, loss = 0.65383604\n",
      "Iteration 8590, loss = 0.65384023\n",
      "Iteration 8591, loss = 0.65383293\n",
      "Iteration 8592, loss = 0.65383105\n",
      "Iteration 8593, loss = 0.65382215\n",
      "Iteration 8594, loss = 0.65380894\n",
      "Iteration 8595, loss = 0.65380302\n",
      "Iteration 8596, loss = 0.65379479\n",
      "Iteration 8597, loss = 0.65378418\n",
      "Iteration 8598, loss = 0.65378540\n",
      "Iteration 8599, loss = 0.65377403\n",
      "Iteration 8600, loss = 0.65376138\n",
      "Iteration 8601, loss = 0.65376002\n",
      "Iteration 8602, loss = 0.65376051\n",
      "Iteration 8603, loss = 0.65374274\n",
      "Iteration 8604, loss = 0.65372249\n",
      "Iteration 8605, loss = 0.65370939\n",
      "Iteration 8606, loss = 0.65370637\n",
      "Iteration 8607, loss = 0.65368850\n",
      "Iteration 8608, loss = 0.65368581\n",
      "Iteration 8609, loss = 0.65367462\n",
      "Iteration 8610, loss = 0.65366611\n",
      "Iteration 8611, loss = 0.65365873\n",
      "Iteration 8612, loss = 0.65366711\n",
      "Iteration 8613, loss = 0.65364271\n",
      "Iteration 8614, loss = 0.65364399\n",
      "Iteration 8615, loss = 0.65362737\n",
      "Iteration 8616, loss = 0.65363185\n",
      "Iteration 8617, loss = 0.65362233\n",
      "Iteration 8618, loss = 0.65361010\n",
      "Iteration 8619, loss = 0.65361030\n",
      "Iteration 8620, loss = 0.65360965\n",
      "Iteration 8621, loss = 0.65359012\n",
      "Iteration 8622, loss = 0.65357666\n",
      "Iteration 8623, loss = 0.65356988\n",
      "Iteration 8624, loss = 0.65356098\n",
      "Iteration 8625, loss = 0.65355872\n",
      "Iteration 8626, loss = 0.65354532\n",
      "Iteration 8627, loss = 0.65353670\n",
      "Iteration 8628, loss = 0.65354474\n",
      "Iteration 8629, loss = 0.65352642\n",
      "Iteration 8630, loss = 0.65351376\n",
      "Iteration 8631, loss = 0.65349883\n",
      "Iteration 8632, loss = 0.65350081\n",
      "Iteration 8633, loss = 0.65349195\n",
      "Iteration 8634, loss = 0.65349715\n",
      "Iteration 8635, loss = 0.65347042\n",
      "Iteration 8636, loss = 0.65347126\n",
      "Iteration 8637, loss = 0.65345104\n",
      "Iteration 8638, loss = 0.65345297\n",
      "Iteration 8639, loss = 0.65343916\n",
      "Iteration 8640, loss = 0.65344209\n",
      "Iteration 8641, loss = 0.65342580\n",
      "Iteration 8642, loss = 0.65342224\n",
      "Iteration 8643, loss = 0.65340918\n",
      "Iteration 8644, loss = 0.65341083\n",
      "Iteration 8645, loss = 0.65339641\n",
      "Iteration 8646, loss = 0.65339319\n",
      "Iteration 8647, loss = 0.65339117\n",
      "Iteration 8648, loss = 0.65337219\n",
      "Iteration 8649, loss = 0.65336401\n",
      "Iteration 8650, loss = 0.65334826\n",
      "Iteration 8651, loss = 0.65336093\n",
      "Iteration 8652, loss = 0.65334014\n",
      "Iteration 8653, loss = 0.65333331\n",
      "Iteration 8654, loss = 0.65333819\n",
      "Iteration 8655, loss = 0.65331467\n",
      "Iteration 8656, loss = 0.65330502\n",
      "Iteration 8657, loss = 0.65330416\n",
      "Iteration 8658, loss = 0.65328902\n",
      "Iteration 8659, loss = 0.65328608\n",
      "Iteration 8660, loss = 0.65327425\n",
      "Iteration 8661, loss = 0.65328168\n",
      "Iteration 8662, loss = 0.65326270\n",
      "Iteration 8663, loss = 0.65325502\n",
      "Iteration 8664, loss = 0.65325135\n",
      "Iteration 8665, loss = 0.65323836\n",
      "Iteration 8666, loss = 0.65323499\n",
      "Iteration 8667, loss = 0.65322364\n",
      "Iteration 8668, loss = 0.65321240\n",
      "Iteration 8669, loss = 0.65321436\n",
      "Iteration 8670, loss = 0.65320692\n",
      "Iteration 8671, loss = 0.65318503\n",
      "Iteration 8672, loss = 0.65318071\n",
      "Iteration 8673, loss = 0.65317458\n",
      "Iteration 8674, loss = 0.65316753\n",
      "Iteration 8675, loss = 0.65316606\n",
      "Iteration 8676, loss = 0.65316808\n",
      "Iteration 8677, loss = 0.65314072\n",
      "Iteration 8678, loss = 0.65313607\n",
      "Iteration 8679, loss = 0.65313535\n",
      "Iteration 8680, loss = 0.65311863\n",
      "Iteration 8681, loss = 0.65310859\n",
      "Iteration 8682, loss = 0.65309178\n",
      "Iteration 8683, loss = 0.65308608\n",
      "Iteration 8684, loss = 0.65308970\n",
      "Iteration 8685, loss = 0.65307340\n",
      "Iteration 8686, loss = 0.65307202\n",
      "Iteration 8687, loss = 0.65306406\n",
      "Iteration 8688, loss = 0.65304824\n",
      "Iteration 8689, loss = 0.65307155\n",
      "Iteration 8690, loss = 0.65305274\n",
      "Iteration 8691, loss = 0.65303180\n",
      "Iteration 8692, loss = 0.65301562\n",
      "Iteration 8693, loss = 0.65301660\n",
      "Iteration 8694, loss = 0.65300769\n",
      "Iteration 8695, loss = 0.65299588\n",
      "Iteration 8696, loss = 0.65298385\n",
      "Iteration 8697, loss = 0.65297634\n",
      "Iteration 8698, loss = 0.65298086\n",
      "Iteration 8699, loss = 0.65295959\n",
      "Iteration 8700, loss = 0.65295325\n",
      "Iteration 8701, loss = 0.65295347\n",
      "Iteration 8702, loss = 0.65293895\n",
      "Iteration 8703, loss = 0.65293535\n",
      "Iteration 8704, loss = 0.65293404\n",
      "Iteration 8705, loss = 0.65293573\n",
      "Iteration 8706, loss = 0.65290269\n",
      "Iteration 8707, loss = 0.65290581\n",
      "Iteration 8708, loss = 0.65288666\n",
      "Iteration 8709, loss = 0.65288444\n",
      "Iteration 8710, loss = 0.65287457\n",
      "Iteration 8711, loss = 0.65286730\n",
      "Iteration 8712, loss = 0.65287082\n",
      "Iteration 8713, loss = 0.65286145\n",
      "Iteration 8714, loss = 0.65284976\n",
      "Iteration 8715, loss = 0.65284120\n",
      "Iteration 8716, loss = 0.65282566\n",
      "Iteration 8717, loss = 0.65282117\n",
      "Iteration 8718, loss = 0.65281638\n",
      "Iteration 8719, loss = 0.65280901\n",
      "Iteration 8720, loss = 0.65278997\n",
      "Iteration 8721, loss = 0.65279381\n",
      "Iteration 8722, loss = 0.65277987\n",
      "Iteration 8723, loss = 0.65278272\n",
      "Iteration 8724, loss = 0.65277764\n",
      "Iteration 8725, loss = 0.65275606\n",
      "Iteration 8726, loss = 0.65275973\n",
      "Iteration 8727, loss = 0.65275786\n",
      "Iteration 8728, loss = 0.65273332\n",
      "Iteration 8729, loss = 0.65273007\n",
      "Iteration 8730, loss = 0.65272487\n",
      "Iteration 8731, loss = 0.65271979\n",
      "Iteration 8732, loss = 0.65270328\n",
      "Iteration 8733, loss = 0.65269660\n",
      "Iteration 8734, loss = 0.65268803\n",
      "Iteration 8735, loss = 0.65267919\n",
      "Iteration 8736, loss = 0.65267222\n",
      "Iteration 8737, loss = 0.65267044\n",
      "Iteration 8738, loss = 0.65266492\n",
      "Iteration 8739, loss = 0.65265043\n",
      "Iteration 8740, loss = 0.65263760\n",
      "Iteration 8741, loss = 0.65263425\n",
      "Iteration 8742, loss = 0.65262405\n",
      "Iteration 8743, loss = 0.65263020\n",
      "Iteration 8744, loss = 0.65261429\n",
      "Iteration 8745, loss = 0.65259970\n",
      "Iteration 8746, loss = 0.65259349\n",
      "Iteration 8747, loss = 0.65258885\n",
      "Iteration 8748, loss = 0.65257577\n",
      "Iteration 8749, loss = 0.65257403\n",
      "Iteration 8750, loss = 0.65256396\n",
      "Iteration 8751, loss = 0.65255764\n",
      "Iteration 8752, loss = 0.65255725\n",
      "Iteration 8753, loss = 0.65253837\n",
      "Iteration 8754, loss = 0.65252499\n",
      "Iteration 8755, loss = 0.65251429\n",
      "Iteration 8756, loss = 0.65250012\n",
      "Iteration 8757, loss = 0.65248644\n",
      "Iteration 8758, loss = 0.65249711\n",
      "Iteration 8759, loss = 0.65246188\n",
      "Iteration 8760, loss = 0.65245356\n",
      "Iteration 8761, loss = 0.65245987\n",
      "Iteration 8762, loss = 0.65243690\n",
      "Iteration 8763, loss = 0.65241378\n",
      "Iteration 8764, loss = 0.65240253\n",
      "Iteration 8765, loss = 0.65238964\n",
      "Iteration 8766, loss = 0.65238352\n",
      "Iteration 8767, loss = 0.65236293\n",
      "Iteration 8768, loss = 0.65235735\n",
      "Iteration 8769, loss = 0.65235477\n",
      "Iteration 8770, loss = 0.65233397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 8771, loss = 0.65234088\n",
      "Iteration 8772, loss = 0.65231329\n",
      "Iteration 8773, loss = 0.65229389\n",
      "Iteration 8774, loss = 0.65227352\n",
      "Iteration 8775, loss = 0.65226129\n",
      "Iteration 8776, loss = 0.65225185\n",
      "Iteration 8777, loss = 0.65224488\n",
      "Iteration 8778, loss = 0.65224578\n",
      "Iteration 8779, loss = 0.65221611\n",
      "Iteration 8780, loss = 0.65220319\n",
      "Iteration 8781, loss = 0.65219576\n",
      "Iteration 8782, loss = 0.65220555\n",
      "Iteration 8783, loss = 0.65218179\n",
      "Iteration 8784, loss = 0.65215682\n",
      "Iteration 8785, loss = 0.65214511\n",
      "Iteration 8786, loss = 0.65213248\n",
      "Iteration 8787, loss = 0.65212556\n",
      "Iteration 8788, loss = 0.65210659\n",
      "Iteration 8789, loss = 0.65210063\n",
      "Iteration 8790, loss = 0.65210219\n",
      "Iteration 8791, loss = 0.65207469\n",
      "Iteration 8792, loss = 0.65207112\n",
      "Iteration 8793, loss = 0.65204560\n",
      "Iteration 8794, loss = 0.65204421\n",
      "Iteration 8795, loss = 0.65202723\n",
      "Iteration 8796, loss = 0.65201747\n",
      "Iteration 8797, loss = 0.65201370\n",
      "Iteration 8798, loss = 0.65200401\n",
      "Iteration 8799, loss = 0.65198412\n",
      "Iteration 8800, loss = 0.65197763\n",
      "Iteration 8801, loss = 0.65195995\n",
      "Iteration 8802, loss = 0.65195527\n",
      "Iteration 8803, loss = 0.65193500\n",
      "Iteration 8804, loss = 0.65192527\n",
      "Iteration 8805, loss = 0.65192541\n",
      "Iteration 8806, loss = 0.65190402\n",
      "Iteration 8807, loss = 0.65188651\n",
      "Iteration 8808, loss = 0.65189079\n",
      "Iteration 8809, loss = 0.65186847\n",
      "Iteration 8810, loss = 0.65185402\n",
      "Iteration 8811, loss = 0.65185103\n",
      "Iteration 8812, loss = 0.65183889\n",
      "Iteration 8813, loss = 0.65181511\n",
      "Iteration 8814, loss = 0.65180346\n",
      "Iteration 8815, loss = 0.65178340\n",
      "Iteration 8816, loss = 0.65178216\n",
      "Iteration 8817, loss = 0.65176891\n",
      "Iteration 8818, loss = 0.65174811\n",
      "Iteration 8819, loss = 0.65173516\n",
      "Iteration 8820, loss = 0.65171813\n",
      "Iteration 8821, loss = 0.65171440\n",
      "Iteration 8822, loss = 0.65169437\n",
      "Iteration 8823, loss = 0.65167265\n",
      "Iteration 8824, loss = 0.65165856\n",
      "Iteration 8825, loss = 0.65166428\n",
      "Iteration 8826, loss = 0.65163398\n",
      "Iteration 8827, loss = 0.65163608\n",
      "Iteration 8828, loss = 0.65160499\n",
      "Iteration 8829, loss = 0.65160069\n",
      "Iteration 8830, loss = 0.65158722\n",
      "Iteration 8831, loss = 0.65156773\n",
      "Iteration 8832, loss = 0.65154836\n",
      "Iteration 8833, loss = 0.65153659\n",
      "Iteration 8834, loss = 0.65153734\n",
      "Iteration 8835, loss = 0.65150998\n",
      "Iteration 8836, loss = 0.65149965\n",
      "Iteration 8837, loss = 0.65148349\n",
      "Iteration 8838, loss = 0.65146822\n",
      "Iteration 8839, loss = 0.65147422\n",
      "Iteration 8840, loss = 0.65145123\n",
      "Iteration 8841, loss = 0.65145418\n",
      "Iteration 8842, loss = 0.65143016\n",
      "Iteration 8843, loss = 0.65141338\n",
      "Iteration 8844, loss = 0.65140005\n",
      "Iteration 8845, loss = 0.65139653\n",
      "Iteration 8846, loss = 0.65136758\n",
      "Iteration 8847, loss = 0.65135787\n",
      "Iteration 8848, loss = 0.65136732\n",
      "Iteration 8849, loss = 0.65133496\n",
      "Iteration 8850, loss = 0.65131923\n",
      "Iteration 8851, loss = 0.65131965\n",
      "Iteration 8852, loss = 0.65129920\n",
      "Iteration 8853, loss = 0.65130211\n",
      "Iteration 8854, loss = 0.65127497\n",
      "Iteration 8855, loss = 0.65126313\n",
      "Iteration 8856, loss = 0.65126032\n",
      "Iteration 8857, loss = 0.65123509\n",
      "Iteration 8858, loss = 0.65123207\n",
      "Iteration 8859, loss = 0.65122471\n",
      "Iteration 8860, loss = 0.65119632\n",
      "Iteration 8861, loss = 0.65119482\n",
      "Iteration 8862, loss = 0.65118303\n",
      "Iteration 8863, loss = 0.65120008\n",
      "Iteration 8864, loss = 0.65114928\n",
      "Iteration 8865, loss = 0.65115447\n",
      "Iteration 8866, loss = 0.65113989\n",
      "Iteration 8867, loss = 0.65112188\n",
      "Iteration 8868, loss = 0.65110996\n",
      "Iteration 8869, loss = 0.65110875\n",
      "Iteration 8870, loss = 0.65108890\n",
      "Iteration 8871, loss = 0.65108396\n",
      "Iteration 8872, loss = 0.65105901\n",
      "Iteration 8873, loss = 0.65106206\n",
      "Iteration 8874, loss = 0.65103950\n",
      "Iteration 8875, loss = 0.65102812\n",
      "Iteration 8876, loss = 0.65103162\n",
      "Iteration 8877, loss = 0.65101735\n",
      "Iteration 8878, loss = 0.65100248\n",
      "Iteration 8879, loss = 0.65098977\n",
      "Iteration 8880, loss = 0.65097216\n",
      "Iteration 8881, loss = 0.65095881\n",
      "Iteration 8882, loss = 0.65094873\n",
      "Iteration 8883, loss = 0.65097578\n",
      "Iteration 8884, loss = 0.65092532\n",
      "Iteration 8885, loss = 0.65091627\n",
      "Iteration 8886, loss = 0.65091846\n",
      "Iteration 8887, loss = 0.65089080\n",
      "Iteration 8888, loss = 0.65088664\n",
      "Iteration 8889, loss = 0.65088200\n",
      "Iteration 8890, loss = 0.65086736\n",
      "Iteration 8891, loss = 0.65086642\n",
      "Iteration 8892, loss = 0.65084590\n",
      "Iteration 8893, loss = 0.65084662\n",
      "Iteration 8894, loss = 0.65081824\n",
      "Iteration 8895, loss = 0.65082749\n",
      "Iteration 8896, loss = 0.65079886\n",
      "Iteration 8897, loss = 0.65080233\n",
      "Iteration 8898, loss = 0.65077619\n",
      "Iteration 8899, loss = 0.65077223\n",
      "Iteration 8900, loss = 0.65075253\n",
      "Iteration 8901, loss = 0.65074816\n",
      "Iteration 8902, loss = 0.65073214\n",
      "Iteration 8903, loss = 0.65073439\n",
      "Iteration 8904, loss = 0.65071103\n",
      "Iteration 8905, loss = 0.65073649\n",
      "Iteration 8906, loss = 0.65069750\n",
      "Iteration 8907, loss = 0.65069604\n",
      "Iteration 8908, loss = 0.65067237\n",
      "Iteration 8909, loss = 0.65066046\n",
      "Iteration 8910, loss = 0.65065488\n",
      "Iteration 8911, loss = 0.65064537\n",
      "Iteration 8912, loss = 0.65064394\n",
      "Iteration 8913, loss = 0.65062436\n",
      "Iteration 8914, loss = 0.65062693\n",
      "Iteration 8915, loss = 0.65059941\n",
      "Iteration 8916, loss = 0.65059716\n",
      "Iteration 8917, loss = 0.65058108\n",
      "Iteration 8918, loss = 0.65056927\n",
      "Iteration 8919, loss = 0.65056255\n",
      "Iteration 8920, loss = 0.65054970\n",
      "Iteration 8921, loss = 0.65054604\n",
      "Iteration 8922, loss = 0.65051952\n",
      "Iteration 8923, loss = 0.65051212\n",
      "Iteration 8924, loss = 0.65050666\n",
      "Iteration 8925, loss = 0.65049095\n",
      "Iteration 8926, loss = 0.65049343\n",
      "Iteration 8927, loss = 0.65046909\n",
      "Iteration 8928, loss = 0.65045880\n",
      "Iteration 8929, loss = 0.65044718\n",
      "Iteration 8930, loss = 0.65045671\n",
      "Iteration 8931, loss = 0.65043632\n",
      "Iteration 8932, loss = 0.65041442\n",
      "Iteration 8933, loss = 0.65040298\n",
      "Iteration 8934, loss = 0.65044325\n",
      "Iteration 8935, loss = 0.65038338\n",
      "Iteration 8936, loss = 0.65041050\n",
      "Iteration 8937, loss = 0.65037418\n",
      "Iteration 8938, loss = 0.65035735\n",
      "Iteration 8939, loss = 0.65034727\n",
      "Iteration 8940, loss = 0.65034536\n",
      "Iteration 8941, loss = 0.65032936\n",
      "Iteration 8942, loss = 0.65031023\n",
      "Iteration 8943, loss = 0.65030576\n",
      "Iteration 8944, loss = 0.65030185\n",
      "Iteration 8945, loss = 0.65028982\n",
      "Iteration 8946, loss = 0.65027986\n",
      "Iteration 8947, loss = 0.65027617\n",
      "Iteration 8948, loss = 0.65027441\n",
      "Iteration 8949, loss = 0.65025697\n",
      "Iteration 8950, loss = 0.65024019\n",
      "Iteration 8951, loss = 0.65023375\n",
      "Iteration 8952, loss = 0.65024166\n",
      "Iteration 8953, loss = 0.65021740\n",
      "Iteration 8954, loss = 0.65020144\n",
      "Iteration 8955, loss = 0.65018978\n",
      "Iteration 8956, loss = 0.65021062\n",
      "Iteration 8957, loss = 0.65017618\n",
      "Iteration 8958, loss = 0.65018426\n",
      "Iteration 8959, loss = 0.65015930\n",
      "Iteration 8960, loss = 0.65015151\n",
      "Iteration 8961, loss = 0.65015083\n",
      "Iteration 8962, loss = 0.65013504\n",
      "Iteration 8963, loss = 0.65011988\n",
      "Iteration 8964, loss = 0.65012012\n",
      "Iteration 8965, loss = 0.65012147\n",
      "Iteration 8966, loss = 0.65011081\n",
      "Iteration 8967, loss = 0.65010099\n",
      "Iteration 8968, loss = 0.65009309\n",
      "Iteration 8969, loss = 0.65008127\n",
      "Iteration 8970, loss = 0.65007916\n",
      "Iteration 8971, loss = 0.65006365\n",
      "Iteration 8972, loss = 0.65007205\n",
      "Iteration 8973, loss = 0.65004444\n",
      "Iteration 8974, loss = 0.65003928\n",
      "Iteration 8975, loss = 0.65002976\n",
      "Iteration 8976, loss = 0.65002498\n",
      "Iteration 8977, loss = 0.65001096\n",
      "Iteration 8978, loss = 0.65000462\n",
      "Iteration 8979, loss = 0.64999997\n",
      "Iteration 8980, loss = 0.64999836\n",
      "Iteration 8981, loss = 0.65001881\n",
      "Iteration 8982, loss = 0.64997016\n",
      "Iteration 8983, loss = 0.64997378\n",
      "Iteration 8984, loss = 0.64996105\n",
      "Iteration 8985, loss = 0.64996035\n",
      "Iteration 8986, loss = 0.64994621\n",
      "Iteration 8987, loss = 0.64993421\n",
      "Iteration 8988, loss = 0.64993278\n",
      "Iteration 8989, loss = 0.64992021\n",
      "Iteration 8990, loss = 0.64992416\n",
      "Iteration 8991, loss = 0.64991150\n",
      "Iteration 8992, loss = 0.64991219\n",
      "Iteration 8993, loss = 0.64989229\n",
      "Iteration 8994, loss = 0.64990498\n",
      "Iteration 8995, loss = 0.64987975\n",
      "Iteration 8996, loss = 0.64987248\n",
      "Iteration 8997, loss = 0.64985854\n",
      "Iteration 8998, loss = 0.64985786\n",
      "Iteration 8999, loss = 0.64985111\n",
      "Iteration 9000, loss = 0.64984497\n",
      "Iteration 9001, loss = 0.64984561\n",
      "Iteration 9002, loss = 0.64981938\n",
      "Iteration 9003, loss = 0.64981771\n",
      "Iteration 9004, loss = 0.64981240\n",
      "Iteration 9005, loss = 0.64980437\n",
      "Iteration 9006, loss = 0.64979842\n",
      "Iteration 9007, loss = 0.64978568\n",
      "Iteration 9008, loss = 0.64977612\n",
      "Iteration 9009, loss = 0.64977669\n",
      "Iteration 9010, loss = 0.64976235\n",
      "Iteration 9011, loss = 0.64975271\n",
      "Iteration 9012, loss = 0.64973859\n",
      "Iteration 9013, loss = 0.64975302\n",
      "Iteration 9014, loss = 0.64972058\n",
      "Iteration 9015, loss = 0.64971669\n",
      "Iteration 9016, loss = 0.64970901\n",
      "Iteration 9017, loss = 0.64970194\n",
      "Iteration 9018, loss = 0.64969427\n",
      "Iteration 9019, loss = 0.64968520\n",
      "Iteration 9020, loss = 0.64968168\n",
      "Iteration 9021, loss = 0.64967300\n",
      "Iteration 9022, loss = 0.64965702\n",
      "Iteration 9023, loss = 0.64964487\n",
      "Iteration 9024, loss = 0.64964287\n",
      "Iteration 9025, loss = 0.64965299\n",
      "Iteration 9026, loss = 0.64962277\n",
      "Iteration 9027, loss = 0.64962860\n",
      "Iteration 9028, loss = 0.64962238\n",
      "Iteration 9029, loss = 0.64959770\n",
      "Iteration 9030, loss = 0.64959967\n",
      "Iteration 9031, loss = 0.64958823\n",
      "Iteration 9032, loss = 0.64958542\n",
      "Iteration 9033, loss = 0.64956397\n",
      "Iteration 9034, loss = 0.64955324\n",
      "Iteration 9035, loss = 0.64955604\n",
      "Iteration 9036, loss = 0.64954729\n",
      "Iteration 9037, loss = 0.64954218\n",
      "Iteration 9038, loss = 0.64953059\n",
      "Iteration 9039, loss = 0.64953112\n",
      "Iteration 9040, loss = 0.64951392\n",
      "Iteration 9041, loss = 0.64954169\n",
      "Iteration 9042, loss = 0.64948979\n",
      "Iteration 9043, loss = 0.64951915\n",
      "Iteration 9044, loss = 0.64947524\n",
      "Iteration 9045, loss = 0.64947620\n",
      "Iteration 9046, loss = 0.64946577\n",
      "Iteration 9047, loss = 0.64948478\n",
      "Iteration 9048, loss = 0.64944906\n",
      "Iteration 9049, loss = 0.64945411\n",
      "Iteration 9050, loss = 0.64943219\n",
      "Iteration 9051, loss = 0.64943086\n",
      "Iteration 9052, loss = 0.64941457\n",
      "Iteration 9053, loss = 0.64941923\n",
      "Iteration 9054, loss = 0.64940375\n",
      "Iteration 9055, loss = 0.64940507\n",
      "Iteration 9056, loss = 0.64938075\n",
      "Iteration 9057, loss = 0.64937730\n",
      "Iteration 9058, loss = 0.64937614\n",
      "Iteration 9059, loss = 0.64936445\n",
      "Iteration 9060, loss = 0.64937631\n",
      "Iteration 9061, loss = 0.64934845\n",
      "Iteration 9062, loss = 0.64933362\n",
      "Iteration 9063, loss = 0.64932812\n",
      "Iteration 9064, loss = 0.64932701\n",
      "Iteration 9065, loss = 0.64931657\n",
      "Iteration 9066, loss = 0.64930389\n",
      "Iteration 9067, loss = 0.64930261\n",
      "Iteration 9068, loss = 0.64931459\n",
      "Iteration 9069, loss = 0.64927261\n",
      "Iteration 9070, loss = 0.64931404\n",
      "Iteration 9071, loss = 0.64926609\n",
      "Iteration 9072, loss = 0.64927915\n",
      "Iteration 9073, loss = 0.64924835\n",
      "Iteration 9074, loss = 0.64924528\n",
      "Iteration 9075, loss = 0.64924924\n",
      "Iteration 9076, loss = 0.64923966\n",
      "Iteration 9077, loss = 0.64923532\n",
      "Iteration 9078, loss = 0.64922042\n",
      "Iteration 9079, loss = 0.64922159\n",
      "Iteration 9080, loss = 0.64920364\n",
      "Iteration 9081, loss = 0.64919484\n",
      "Iteration 9082, loss = 0.64919422\n",
      "Iteration 9083, loss = 0.64918846\n",
      "Iteration 9084, loss = 0.64918802\n",
      "Iteration 9085, loss = 0.64918595\n",
      "Iteration 9086, loss = 0.64916533\n",
      "Iteration 9087, loss = 0.64915856\n",
      "Iteration 9088, loss = 0.64914797\n",
      "Iteration 9089, loss = 0.64914511\n",
      "Iteration 9090, loss = 0.64914710\n",
      "Iteration 9091, loss = 0.64912838\n",
      "Iteration 9092, loss = 0.64913796\n",
      "Iteration 9093, loss = 0.64911854\n",
      "Iteration 9094, loss = 0.64911026\n",
      "Iteration 9095, loss = 0.64910689\n",
      "Iteration 9096, loss = 0.64910527\n",
      "Iteration 9097, loss = 0.64909926\n",
      "Iteration 9098, loss = 0.64908128\n",
      "Iteration 9099, loss = 0.64908204\n",
      "Iteration 9100, loss = 0.64908470\n",
      "Iteration 9101, loss = 0.64907704\n",
      "Iteration 9102, loss = 0.64906806\n",
      "Iteration 9103, loss = 0.64904321\n",
      "Iteration 9104, loss = 0.64905333\n",
      "Iteration 9105, loss = 0.64904072\n",
      "Iteration 9106, loss = 0.64903561\n",
      "Iteration 9107, loss = 0.64902482\n",
      "Iteration 9108, loss = 0.64900842\n",
      "Iteration 9109, loss = 0.64901596\n",
      "Iteration 9110, loss = 0.64899467\n",
      "Iteration 9111, loss = 0.64899605\n",
      "Iteration 9112, loss = 0.64898517\n",
      "Iteration 9113, loss = 0.64899611\n",
      "Iteration 9114, loss = 0.64898210\n",
      "Iteration 9115, loss = 0.64897751\n",
      "Iteration 9116, loss = 0.64896459\n",
      "Iteration 9117, loss = 0.64895555\n",
      "Iteration 9118, loss = 0.64893993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9119, loss = 0.64893729\n",
      "Iteration 9120, loss = 0.64892947\n",
      "Iteration 9121, loss = 0.64893954\n",
      "Iteration 9122, loss = 0.64891749\n",
      "Iteration 9123, loss = 0.64892055\n",
      "Iteration 9124, loss = 0.64892203\n",
      "Iteration 9125, loss = 0.64889790\n",
      "Iteration 9126, loss = 0.64891777\n",
      "Iteration 9127, loss = 0.64888754\n",
      "Iteration 9128, loss = 0.64888860\n",
      "Iteration 9129, loss = 0.64887624\n",
      "Iteration 9130, loss = 0.64886635\n",
      "Iteration 9131, loss = 0.64886309\n",
      "Iteration 9132, loss = 0.64885048\n",
      "Iteration 9133, loss = 0.64885895\n",
      "Iteration 9134, loss = 0.64884398\n",
      "Iteration 9135, loss = 0.64884303\n",
      "Iteration 9136, loss = 0.64884210\n",
      "Iteration 9137, loss = 0.64884408\n",
      "Iteration 9138, loss = 0.64882187\n",
      "Iteration 9139, loss = 0.64882474\n",
      "Iteration 9140, loss = 0.64881031\n",
      "Iteration 9141, loss = 0.64879741\n",
      "Iteration 9142, loss = 0.64879833\n",
      "Iteration 9143, loss = 0.64880616\n",
      "Iteration 9144, loss = 0.64877759\n",
      "Iteration 9145, loss = 0.64878703\n",
      "Iteration 9146, loss = 0.64876991\n",
      "Iteration 9147, loss = 0.64878378\n",
      "Iteration 9148, loss = 0.64876301\n",
      "Iteration 9149, loss = 0.64875062\n",
      "Iteration 9150, loss = 0.64874821\n",
      "Iteration 9151, loss = 0.64873998\n",
      "Iteration 9152, loss = 0.64873266\n",
      "Iteration 9153, loss = 0.64873062\n",
      "Iteration 9154, loss = 0.64871618\n",
      "Iteration 9155, loss = 0.64871389\n",
      "Iteration 9156, loss = 0.64870738\n",
      "Iteration 9157, loss = 0.64870634\n",
      "Iteration 9158, loss = 0.64869671\n",
      "Iteration 9159, loss = 0.64869931\n",
      "Iteration 9160, loss = 0.64869221\n",
      "Iteration 9161, loss = 0.64867105\n",
      "Iteration 9162, loss = 0.64867474\n",
      "Iteration 9163, loss = 0.64867408\n",
      "Iteration 9164, loss = 0.64866787\n",
      "Iteration 9165, loss = 0.64865815\n",
      "Iteration 9166, loss = 0.64866595\n",
      "Iteration 9167, loss = 0.64863811\n",
      "Iteration 9168, loss = 0.64863407\n",
      "Iteration 9169, loss = 0.64862544\n",
      "Iteration 9170, loss = 0.64862360\n",
      "Iteration 9171, loss = 0.64860572\n",
      "Iteration 9172, loss = 0.64860910\n",
      "Iteration 9173, loss = 0.64860562\n",
      "Iteration 9174, loss = 0.64858768\n",
      "Iteration 9175, loss = 0.64859031\n",
      "Iteration 9176, loss = 0.64857728\n",
      "Iteration 9177, loss = 0.64857251\n",
      "Iteration 9178, loss = 0.64858315\n",
      "Iteration 9179, loss = 0.64857897\n",
      "Iteration 9180, loss = 0.64855927\n",
      "Iteration 9181, loss = 0.64855172\n",
      "Iteration 9182, loss = 0.64854522\n",
      "Iteration 9183, loss = 0.64853368\n",
      "Iteration 9184, loss = 0.64852984\n",
      "Iteration 9185, loss = 0.64852439\n",
      "Iteration 9186, loss = 0.64851597\n",
      "Iteration 9187, loss = 0.64850616\n",
      "Iteration 9188, loss = 0.64850862\n",
      "Iteration 9189, loss = 0.64849064\n",
      "Iteration 9190, loss = 0.64849871\n",
      "Iteration 9191, loss = 0.64849730\n",
      "Iteration 9192, loss = 0.64849933\n",
      "Iteration 9193, loss = 0.64847604\n",
      "Iteration 9194, loss = 0.64847037\n",
      "Iteration 9195, loss = 0.64846013\n",
      "Iteration 9196, loss = 0.64845797\n",
      "Iteration 9197, loss = 0.64844397\n",
      "Iteration 9198, loss = 0.64845019\n",
      "Iteration 9199, loss = 0.64844177\n",
      "Iteration 9200, loss = 0.64844858\n",
      "Iteration 9201, loss = 0.64842920\n",
      "Iteration 9202, loss = 0.64841399\n",
      "Iteration 9203, loss = 0.64841193\n",
      "Iteration 9204, loss = 0.64839991\n",
      "Iteration 9205, loss = 0.64840485\n",
      "Iteration 9206, loss = 0.64839696\n",
      "Iteration 9207, loss = 0.64838595\n",
      "Iteration 9208, loss = 0.64836889\n",
      "Iteration 9209, loss = 0.64837186\n",
      "Iteration 9210, loss = 0.64835162\n",
      "Iteration 9211, loss = 0.64835872\n",
      "Iteration 9212, loss = 0.64833480\n",
      "Iteration 9213, loss = 0.64833918\n",
      "Iteration 9214, loss = 0.64833309\n",
      "Iteration 9215, loss = 0.64831904\n",
      "Iteration 9216, loss = 0.64830634\n",
      "Iteration 9217, loss = 0.64829456\n",
      "Iteration 9218, loss = 0.64831198\n",
      "Iteration 9219, loss = 0.64828460\n",
      "Iteration 9220, loss = 0.64827663\n",
      "Iteration 9221, loss = 0.64827477\n",
      "Iteration 9222, loss = 0.64827154\n",
      "Iteration 9223, loss = 0.64827623\n",
      "Iteration 9224, loss = 0.64825506\n",
      "Iteration 9225, loss = 0.64823725\n",
      "Iteration 9226, loss = 0.64823616\n",
      "Iteration 9227, loss = 0.64822494\n",
      "Iteration 9228, loss = 0.64821758\n",
      "Iteration 9229, loss = 0.64820763\n",
      "Iteration 9230, loss = 0.64819483\n",
      "Iteration 9231, loss = 0.64819967\n",
      "Iteration 9232, loss = 0.64819218\n",
      "Iteration 9233, loss = 0.64818529\n",
      "Iteration 9234, loss = 0.64817238\n",
      "Iteration 9235, loss = 0.64818333\n",
      "Iteration 9236, loss = 0.64815516\n",
      "Iteration 9237, loss = 0.64815168\n",
      "Iteration 9238, loss = 0.64814344\n",
      "Iteration 9239, loss = 0.64814344\n",
      "Iteration 9240, loss = 0.64813870\n",
      "Iteration 9241, loss = 0.64814817\n",
      "Iteration 9242, loss = 0.64810992\n",
      "Iteration 9243, loss = 0.64810198\n",
      "Iteration 9244, loss = 0.64811449\n",
      "Iteration 9245, loss = 0.64809552\n",
      "Iteration 9246, loss = 0.64809589\n",
      "Iteration 9247, loss = 0.64808707\n",
      "Iteration 9248, loss = 0.64806903\n",
      "Iteration 9249, loss = 0.64805607\n",
      "Iteration 9250, loss = 0.64806070\n",
      "Iteration 9251, loss = 0.64804105\n",
      "Iteration 9252, loss = 0.64804198\n",
      "Iteration 9253, loss = 0.64806392\n",
      "Iteration 9254, loss = 0.64802361\n",
      "Iteration 9255, loss = 0.64801766\n",
      "Iteration 9256, loss = 0.64800908\n",
      "Iteration 9257, loss = 0.64800898\n",
      "Iteration 9258, loss = 0.64799456\n",
      "Iteration 9259, loss = 0.64798681\n",
      "Iteration 9260, loss = 0.64797376\n",
      "Iteration 9261, loss = 0.64796864\n",
      "Iteration 9262, loss = 0.64796686\n",
      "Iteration 9263, loss = 0.64795138\n",
      "Iteration 9264, loss = 0.64795839\n",
      "Iteration 9265, loss = 0.64793906\n",
      "Iteration 9266, loss = 0.64794239\n",
      "Iteration 9267, loss = 0.64795116\n",
      "Iteration 9268, loss = 0.64794047\n",
      "Iteration 9269, loss = 0.64793904\n",
      "Iteration 9270, loss = 0.64791856\n",
      "Iteration 9271, loss = 0.64790060\n",
      "Iteration 9272, loss = 0.64791679\n",
      "Iteration 9273, loss = 0.64788533\n",
      "Iteration 9274, loss = 0.64789101\n",
      "Iteration 9275, loss = 0.64787944\n",
      "Iteration 9276, loss = 0.64786930\n",
      "Iteration 9277, loss = 0.64792104\n",
      "Iteration 9278, loss = 0.64787927\n",
      "Iteration 9279, loss = 0.64784526\n",
      "Iteration 9280, loss = 0.64787333\n",
      "Iteration 9281, loss = 0.64783544\n",
      "Iteration 9282, loss = 0.64782928\n",
      "Iteration 9283, loss = 0.64781514\n",
      "Iteration 9284, loss = 0.64780968\n",
      "Iteration 9285, loss = 0.64780739\n",
      "Iteration 9286, loss = 0.64780773\n",
      "Iteration 9287, loss = 0.64778233\n",
      "Iteration 9288, loss = 0.64779484\n",
      "Iteration 9289, loss = 0.64780464\n",
      "Iteration 9290, loss = 0.64777306\n",
      "Iteration 9291, loss = 0.64777212\n",
      "Iteration 9292, loss = 0.64781565\n",
      "Iteration 9293, loss = 0.64778446\n",
      "Iteration 9294, loss = 0.64774949\n",
      "Iteration 9295, loss = 0.64773167\n",
      "Iteration 9296, loss = 0.64776228\n",
      "Iteration 9297, loss = 0.64774191\n",
      "Iteration 9298, loss = 0.64771344\n",
      "Iteration 9299, loss = 0.64770995\n",
      "Iteration 9300, loss = 0.64770667\n",
      "Iteration 9301, loss = 0.64772053\n",
      "Iteration 9302, loss = 0.64770293\n",
      "Iteration 9303, loss = 0.64768449\n",
      "Iteration 9304, loss = 0.64767980\n",
      "Iteration 9305, loss = 0.64766596\n",
      "Iteration 9306, loss = 0.64766884\n",
      "Iteration 9307, loss = 0.64766306\n",
      "Iteration 9308, loss = 0.64765487\n",
      "Iteration 9309, loss = 0.64765349\n",
      "Iteration 9310, loss = 0.64766150\n",
      "Iteration 9311, loss = 0.64763528\n",
      "Iteration 9312, loss = 0.64762370\n",
      "Iteration 9313, loss = 0.64761966\n",
      "Iteration 9314, loss = 0.64762690\n",
      "Iteration 9315, loss = 0.64761039\n",
      "Iteration 9316, loss = 0.64762227\n",
      "Iteration 9317, loss = 0.64759201\n",
      "Iteration 9318, loss = 0.64758892\n",
      "Iteration 9319, loss = 0.64757161\n",
      "Iteration 9320, loss = 0.64757316\n",
      "Iteration 9321, loss = 0.64756829\n",
      "Iteration 9322, loss = 0.64756694\n",
      "Iteration 9323, loss = 0.64756374\n",
      "Iteration 9324, loss = 0.64754655\n",
      "Iteration 9325, loss = 0.64755126\n",
      "Iteration 9326, loss = 0.64753097\n",
      "Iteration 9327, loss = 0.64751791\n",
      "Iteration 9328, loss = 0.64754188\n",
      "Iteration 9329, loss = 0.64751169\n",
      "Iteration 9330, loss = 0.64750650\n",
      "Iteration 9331, loss = 0.64749933\n",
      "Iteration 9332, loss = 0.64750742\n",
      "Iteration 9333, loss = 0.64752538\n",
      "Iteration 9334, loss = 0.64752115\n",
      "Iteration 9335, loss = 0.64747230\n",
      "Iteration 9336, loss = 0.64746063\n",
      "Iteration 9337, loss = 0.64745994\n",
      "Iteration 9338, loss = 0.64747301\n",
      "Iteration 9339, loss = 0.64745583\n",
      "Iteration 9340, loss = 0.64744245\n",
      "Iteration 9341, loss = 0.64742581\n",
      "Iteration 9342, loss = 0.64741697\n",
      "Iteration 9343, loss = 0.64742138\n",
      "Iteration 9344, loss = 0.64742162\n",
      "Iteration 9345, loss = 0.64740543\n",
      "Iteration 9346, loss = 0.64741023\n",
      "Iteration 9347, loss = 0.64741758\n",
      "Iteration 9348, loss = 0.64739204\n",
      "Iteration 9349, loss = 0.64737276\n",
      "Iteration 9350, loss = 0.64737228\n",
      "Iteration 9351, loss = 0.64737436\n",
      "Iteration 9352, loss = 0.64736896\n",
      "Iteration 9353, loss = 0.64736366\n",
      "Iteration 9354, loss = 0.64735371\n",
      "Iteration 9355, loss = 0.64735547\n",
      "Iteration 9356, loss = 0.64734805\n",
      "Iteration 9357, loss = 0.64734255\n",
      "Iteration 9358, loss = 0.64732383\n",
      "Iteration 9359, loss = 0.64732426\n",
      "Iteration 9360, loss = 0.64731835\n",
      "Iteration 9361, loss = 0.64730411\n",
      "Iteration 9362, loss = 0.64729820\n",
      "Iteration 9363, loss = 0.64729905\n",
      "Iteration 9364, loss = 0.64731342\n",
      "Iteration 9365, loss = 0.64728332\n",
      "Iteration 9366, loss = 0.64728170\n",
      "Iteration 9367, loss = 0.64731187\n",
      "Iteration 9368, loss = 0.64727168\n",
      "Iteration 9369, loss = 0.64726443\n",
      "Iteration 9370, loss = 0.64726695\n",
      "Iteration 9371, loss = 0.64726146\n",
      "Iteration 9372, loss = 0.64724853\n",
      "Iteration 9373, loss = 0.64723924\n",
      "Iteration 9374, loss = 0.64722636\n",
      "Iteration 9375, loss = 0.64723187\n",
      "Iteration 9376, loss = 0.64722016\n",
      "Iteration 9377, loss = 0.64721230\n",
      "Iteration 9378, loss = 0.64720879\n",
      "Iteration 9379, loss = 0.64720919\n",
      "Iteration 9380, loss = 0.64719524\n",
      "Iteration 9381, loss = 0.64722090\n",
      "Iteration 9382, loss = 0.64719444\n",
      "Iteration 9383, loss = 0.64718057\n",
      "Iteration 9384, loss = 0.64719580\n",
      "Iteration 9385, loss = 0.64716030\n",
      "Iteration 9386, loss = 0.64716150\n",
      "Iteration 9387, loss = 0.64715967\n",
      "Iteration 9388, loss = 0.64716710\n",
      "Iteration 9389, loss = 0.64715813\n",
      "Iteration 9390, loss = 0.64714602\n",
      "Iteration 9391, loss = 0.64715216\n",
      "Iteration 9392, loss = 0.64712728\n",
      "Iteration 9393, loss = 0.64711907\n",
      "Iteration 9394, loss = 0.64710964\n",
      "Iteration 9395, loss = 0.64711132\n",
      "Iteration 9396, loss = 0.64710043\n",
      "Iteration 9397, loss = 0.64710375\n",
      "Iteration 9398, loss = 0.64709289\n",
      "Iteration 9399, loss = 0.64709269\n",
      "Iteration 9400, loss = 0.64707829\n",
      "Iteration 9401, loss = 0.64708025\n",
      "Iteration 9402, loss = 0.64709034\n",
      "Iteration 9403, loss = 0.64706286\n",
      "Iteration 9404, loss = 0.64707192\n",
      "Iteration 9405, loss = 0.64704744\n",
      "Iteration 9406, loss = 0.64704882\n",
      "Iteration 9407, loss = 0.64703884\n",
      "Iteration 9408, loss = 0.64703774\n",
      "Iteration 9409, loss = 0.64702044\n",
      "Iteration 9410, loss = 0.64702803\n",
      "Iteration 9411, loss = 0.64701977\n",
      "Iteration 9412, loss = 0.64700491\n",
      "Iteration 9413, loss = 0.64701535\n",
      "Iteration 9414, loss = 0.64699275\n",
      "Iteration 9415, loss = 0.64699036\n",
      "Iteration 9416, loss = 0.64699010\n",
      "Iteration 9417, loss = 0.64700122\n",
      "Iteration 9418, loss = 0.64697013\n",
      "Iteration 9419, loss = 0.64698903\n",
      "Iteration 9420, loss = 0.64696565\n",
      "Iteration 9421, loss = 0.64696635\n",
      "Iteration 9422, loss = 0.64694795\n",
      "Iteration 9423, loss = 0.64693759\n",
      "Iteration 9424, loss = 0.64694166\n",
      "Iteration 9425, loss = 0.64693049\n",
      "Iteration 9426, loss = 0.64693034\n",
      "Iteration 9427, loss = 0.64692816\n",
      "Iteration 9428, loss = 0.64691929\n",
      "Iteration 9429, loss = 0.64690199\n",
      "Iteration 9430, loss = 0.64689614\n",
      "Iteration 9431, loss = 0.64691164\n",
      "Iteration 9432, loss = 0.64689051\n",
      "Iteration 9433, loss = 0.64688453\n",
      "Iteration 9434, loss = 0.64687439\n",
      "Iteration 9435, loss = 0.64688125\n",
      "Iteration 9436, loss = 0.64687151\n",
      "Iteration 9437, loss = 0.64685336\n",
      "Iteration 9438, loss = 0.64685831\n",
      "Iteration 9439, loss = 0.64683510\n",
      "Iteration 9440, loss = 0.64683653\n",
      "Iteration 9441, loss = 0.64681775\n",
      "Iteration 9442, loss = 0.64682169\n",
      "Iteration 9443, loss = 0.64680258\n",
      "Iteration 9444, loss = 0.64680440\n",
      "Iteration 9445, loss = 0.64680106\n",
      "Iteration 9446, loss = 0.64678902\n",
      "Iteration 9447, loss = 0.64678006\n",
      "Iteration 9448, loss = 0.64679132\n",
      "Iteration 9449, loss = 0.64676483\n",
      "Iteration 9450, loss = 0.64675666\n",
      "Iteration 9451, loss = 0.64676836\n",
      "Iteration 9452, loss = 0.64676023\n",
      "Iteration 9453, loss = 0.64671961\n",
      "Iteration 9454, loss = 0.64672329\n",
      "Iteration 9455, loss = 0.64671154\n",
      "Iteration 9456, loss = 0.64670955\n",
      "Iteration 9457, loss = 0.64670341\n",
      "Iteration 9458, loss = 0.64669238\n",
      "Iteration 9459, loss = 0.64668901\n",
      "Iteration 9460, loss = 0.64669028\n",
      "Iteration 9461, loss = 0.64668291\n",
      "Iteration 9462, loss = 0.64666282\n",
      "Iteration 9463, loss = 0.64665998\n",
      "Iteration 9464, loss = 0.64664460\n",
      "Iteration 9465, loss = 0.64663682\n",
      "Iteration 9466, loss = 0.64662701\n",
      "Iteration 9467, loss = 0.64662088\n",
      "Iteration 9468, loss = 0.64666087\n",
      "Iteration 9469, loss = 0.64660666\n",
      "Iteration 9470, loss = 0.64660734\n",
      "Iteration 9471, loss = 0.64661425\n",
      "Iteration 9472, loss = 0.64660808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9473, loss = 0.64658087\n",
      "Iteration 9474, loss = 0.64658343\n",
      "Iteration 9475, loss = 0.64657013\n",
      "Iteration 9476, loss = 0.64656236\n",
      "Iteration 9477, loss = 0.64656553\n",
      "Iteration 9478, loss = 0.64654587\n",
      "Iteration 9479, loss = 0.64654782\n",
      "Iteration 9480, loss = 0.64654382\n",
      "Iteration 9481, loss = 0.64652414\n",
      "Iteration 9482, loss = 0.64653156\n",
      "Iteration 9483, loss = 0.64653193\n",
      "Iteration 9484, loss = 0.64652828\n",
      "Iteration 9485, loss = 0.64650902\n",
      "Iteration 9486, loss = 0.64650478\n",
      "Iteration 9487, loss = 0.64650456\n",
      "Iteration 9488, loss = 0.64648632\n",
      "Iteration 9489, loss = 0.64648474\n",
      "Iteration 9490, loss = 0.64647673\n",
      "Iteration 9491, loss = 0.64646633\n",
      "Iteration 9492, loss = 0.64645395\n",
      "Iteration 9493, loss = 0.64647683\n",
      "Iteration 9494, loss = 0.64645956\n",
      "Iteration 9495, loss = 0.64643988\n",
      "Iteration 9496, loss = 0.64642062\n",
      "Iteration 9497, loss = 0.64641697\n",
      "Iteration 9498, loss = 0.64645220\n",
      "Iteration 9499, loss = 0.64641201\n",
      "Iteration 9500, loss = 0.64643077\n",
      "Iteration 9501, loss = 0.64639318\n",
      "Iteration 9502, loss = 0.64640339\n",
      "Iteration 9503, loss = 0.64638998\n",
      "Iteration 9504, loss = 0.64640019\n",
      "Iteration 9505, loss = 0.64637045\n",
      "Iteration 9506, loss = 0.64636227\n",
      "Iteration 9507, loss = 0.64635630\n",
      "Iteration 9508, loss = 0.64636393\n",
      "Iteration 9509, loss = 0.64635348\n",
      "Iteration 9510, loss = 0.64636940\n",
      "Iteration 9511, loss = 0.64632766\n",
      "Iteration 9512, loss = 0.64633554\n",
      "Iteration 9513, loss = 0.64631883\n",
      "Iteration 9514, loss = 0.64631974\n",
      "Iteration 9515, loss = 0.64630830\n",
      "Iteration 9516, loss = 0.64630669\n",
      "Iteration 9517, loss = 0.64629369\n",
      "Iteration 9518, loss = 0.64628219\n",
      "Iteration 9519, loss = 0.64627965\n",
      "Iteration 9520, loss = 0.64628003\n",
      "Iteration 9521, loss = 0.64627612\n",
      "Iteration 9522, loss = 0.64626403\n",
      "Iteration 9523, loss = 0.64628180\n",
      "Iteration 9524, loss = 0.64627302\n",
      "Iteration 9525, loss = 0.64624140\n",
      "Iteration 9526, loss = 0.64624831\n",
      "Iteration 9527, loss = 0.64623266\n",
      "Iteration 9528, loss = 0.64622712\n",
      "Iteration 9529, loss = 0.64621540\n",
      "Iteration 9530, loss = 0.64620775\n",
      "Iteration 9531, loss = 0.64620246\n",
      "Iteration 9532, loss = 0.64619506\n",
      "Iteration 9533, loss = 0.64619505\n",
      "Iteration 9534, loss = 0.64620321\n",
      "Iteration 9535, loss = 0.64620128\n",
      "Iteration 9536, loss = 0.64617388\n",
      "Iteration 9537, loss = 0.64621087\n",
      "Iteration 9538, loss = 0.64616737\n",
      "Iteration 9539, loss = 0.64617913\n",
      "Iteration 9540, loss = 0.64617204\n",
      "Iteration 9541, loss = 0.64617239\n",
      "Iteration 9542, loss = 0.64615772\n",
      "Iteration 9543, loss = 0.64614658\n",
      "Iteration 9544, loss = 0.64612758\n",
      "Iteration 9545, loss = 0.64612838\n",
      "Iteration 9546, loss = 0.64610696\n",
      "Iteration 9547, loss = 0.64611658\n",
      "Iteration 9548, loss = 0.64610246\n",
      "Iteration 9549, loss = 0.64610704\n",
      "Iteration 9550, loss = 0.64609638\n",
      "Iteration 9551, loss = 0.64609219\n",
      "Iteration 9552, loss = 0.64609490\n",
      "Iteration 9553, loss = 0.64607665\n",
      "Iteration 9554, loss = 0.64606360\n",
      "Iteration 9555, loss = 0.64608017\n",
      "Iteration 9556, loss = 0.64605024\n",
      "Iteration 9557, loss = 0.64604849\n",
      "Iteration 9558, loss = 0.64605482\n",
      "Iteration 9559, loss = 0.64605295\n",
      "Iteration 9560, loss = 0.64603550\n",
      "Iteration 9561, loss = 0.64601290\n",
      "Iteration 9562, loss = 0.64602868\n",
      "Iteration 9563, loss = 0.64600201\n",
      "Iteration 9564, loss = 0.64600370\n",
      "Iteration 9565, loss = 0.64602141\n",
      "Iteration 9566, loss = 0.64599984\n",
      "Iteration 9567, loss = 0.64600097\n",
      "Iteration 9568, loss = 0.64598419\n",
      "Iteration 9569, loss = 0.64601531\n",
      "Iteration 9570, loss = 0.64599137\n",
      "Iteration 9571, loss = 0.64596209\n",
      "Iteration 9572, loss = 0.64595244\n",
      "Iteration 9573, loss = 0.64595719\n",
      "Iteration 9574, loss = 0.64595098\n",
      "Iteration 9575, loss = 0.64593653\n",
      "Iteration 9576, loss = 0.64593310\n",
      "Iteration 9577, loss = 0.64594293\n",
      "Iteration 9578, loss = 0.64594119\n",
      "Iteration 9579, loss = 0.64591182\n",
      "Iteration 9580, loss = 0.64590704\n",
      "Iteration 9581, loss = 0.64591191\n",
      "Iteration 9582, loss = 0.64591293\n",
      "Iteration 9583, loss = 0.64588487\n",
      "Iteration 9584, loss = 0.64588972\n",
      "Iteration 9585, loss = 0.64587349\n",
      "Iteration 9586, loss = 0.64586509\n",
      "Iteration 9587, loss = 0.64589882\n",
      "Iteration 9588, loss = 0.64587453\n",
      "Iteration 9589, loss = 0.64587554\n",
      "Iteration 9590, loss = 0.64585027\n",
      "Iteration 9591, loss = 0.64584799\n",
      "Iteration 9592, loss = 0.64583653\n",
      "Iteration 9593, loss = 0.64583664\n",
      "Iteration 9594, loss = 0.64582522\n",
      "Iteration 9595, loss = 0.64582248\n",
      "Iteration 9596, loss = 0.64580966\n",
      "Iteration 9597, loss = 0.64580758\n",
      "Iteration 9598, loss = 0.64579919\n",
      "Iteration 9599, loss = 0.64582824\n",
      "Iteration 9600, loss = 0.64579707\n",
      "Iteration 9601, loss = 0.64577904\n",
      "Iteration 9602, loss = 0.64578125\n",
      "Iteration 9603, loss = 0.64578958\n",
      "Iteration 9604, loss = 0.64577346\n",
      "Iteration 9605, loss = 0.64575349\n",
      "Iteration 9606, loss = 0.64576068\n",
      "Iteration 9607, loss = 0.64576054\n",
      "Iteration 9608, loss = 0.64574584\n",
      "Iteration 9609, loss = 0.64574539\n",
      "Iteration 9610, loss = 0.64576667\n",
      "Iteration 9611, loss = 0.64575555\n",
      "Iteration 9612, loss = 0.64572434\n",
      "Iteration 9613, loss = 0.64572849\n",
      "Iteration 9614, loss = 0.64574693\n",
      "Iteration 9615, loss = 0.64571360\n",
      "Iteration 9616, loss = 0.64570810\n",
      "Iteration 9617, loss = 0.64569594\n",
      "Iteration 9618, loss = 0.64569611\n",
      "Iteration 9619, loss = 0.64568959\n",
      "Iteration 9620, loss = 0.64569066\n",
      "Iteration 9621, loss = 0.64567326\n",
      "Iteration 9622, loss = 0.64566962\n",
      "Iteration 9623, loss = 0.64566197\n",
      "Iteration 9624, loss = 0.64565411\n",
      "Iteration 9625, loss = 0.64566132\n",
      "Iteration 9626, loss = 0.64564605\n",
      "Iteration 9627, loss = 0.64566001\n",
      "Iteration 9628, loss = 0.64563695\n",
      "Iteration 9629, loss = 0.64562722\n",
      "Iteration 9630, loss = 0.64562229\n",
      "Iteration 9631, loss = 0.64561583\n",
      "Iteration 9632, loss = 0.64561309\n",
      "Iteration 9633, loss = 0.64560347\n",
      "Iteration 9634, loss = 0.64560846\n",
      "Iteration 9635, loss = 0.64559279\n",
      "Iteration 9636, loss = 0.64558512\n",
      "Iteration 9637, loss = 0.64557236\n",
      "Iteration 9638, loss = 0.64559285\n",
      "Iteration 9639, loss = 0.64557616\n",
      "Iteration 9640, loss = 0.64557027\n",
      "Iteration 9641, loss = 0.64554988\n",
      "Iteration 9642, loss = 0.64554811\n",
      "Iteration 9643, loss = 0.64555020\n",
      "Iteration 9644, loss = 0.64553985\n",
      "Iteration 9645, loss = 0.64553591\n",
      "Iteration 9646, loss = 0.64553002\n",
      "Iteration 9647, loss = 0.64552156\n",
      "Iteration 9648, loss = 0.64551837\n",
      "Iteration 9649, loss = 0.64553187\n",
      "Iteration 9650, loss = 0.64551146\n",
      "Iteration 9651, loss = 0.64551282\n",
      "Iteration 9652, loss = 0.64549704\n",
      "Iteration 9653, loss = 0.64548956\n",
      "Iteration 9654, loss = 0.64551779\n",
      "Iteration 9655, loss = 0.64547019\n",
      "Iteration 9656, loss = 0.64548563\n",
      "Iteration 9657, loss = 0.64546289\n",
      "Iteration 9658, loss = 0.64546732\n",
      "Iteration 9659, loss = 0.64545772\n",
      "Iteration 9660, loss = 0.64545235\n",
      "Iteration 9661, loss = 0.64545270\n",
      "Iteration 9662, loss = 0.64546154\n",
      "Iteration 9663, loss = 0.64544597\n",
      "Iteration 9664, loss = 0.64543388\n",
      "Iteration 9665, loss = 0.64542951\n",
      "Iteration 9666, loss = 0.64542384\n",
      "Iteration 9667, loss = 0.64540664\n",
      "Iteration 9668, loss = 0.64539925\n",
      "Iteration 9669, loss = 0.64540541\n",
      "Iteration 9670, loss = 0.64539391\n",
      "Iteration 9671, loss = 0.64538977\n",
      "Iteration 9672, loss = 0.64538713\n",
      "Iteration 9673, loss = 0.64538255\n",
      "Iteration 9674, loss = 0.64537298\n",
      "Iteration 9675, loss = 0.64538416\n",
      "Iteration 9676, loss = 0.64535310\n",
      "Iteration 9677, loss = 0.64537995\n",
      "Iteration 9678, loss = 0.64535275\n",
      "Iteration 9679, loss = 0.64535745\n",
      "Iteration 9680, loss = 0.64536165\n",
      "Iteration 9681, loss = 0.64534250\n",
      "Iteration 9682, loss = 0.64532608\n",
      "Iteration 9683, loss = 0.64532336\n",
      "Iteration 9684, loss = 0.64530695\n",
      "Iteration 9685, loss = 0.64530654\n",
      "Iteration 9686, loss = 0.64529897\n",
      "Iteration 9687, loss = 0.64527870\n",
      "Iteration 9688, loss = 0.64529220\n",
      "Iteration 9689, loss = 0.64527467\n",
      "Iteration 9690, loss = 0.64527470\n",
      "Iteration 9691, loss = 0.64524150\n",
      "Iteration 9692, loss = 0.64525632\n",
      "Iteration 9693, loss = 0.64523940\n",
      "Iteration 9694, loss = 0.64522665\n",
      "Iteration 9695, loss = 0.64522152\n",
      "Iteration 9696, loss = 0.64521880\n",
      "Iteration 9697, loss = 0.64521025\n",
      "Iteration 9698, loss = 0.64519971\n",
      "Iteration 9699, loss = 0.64518971\n",
      "Iteration 9700, loss = 0.64518249\n",
      "Iteration 9701, loss = 0.64517918\n",
      "Iteration 9702, loss = 0.64517211\n",
      "Iteration 9703, loss = 0.64517449\n",
      "Iteration 9704, loss = 0.64514761\n",
      "Iteration 9705, loss = 0.64515245\n",
      "Iteration 9706, loss = 0.64513601\n",
      "Iteration 9707, loss = 0.64514049\n",
      "Iteration 9708, loss = 0.64512166\n",
      "Iteration 9709, loss = 0.64512888\n",
      "Iteration 9710, loss = 0.64510920\n",
      "Iteration 9711, loss = 0.64510351\n",
      "Iteration 9712, loss = 0.64510943\n",
      "Iteration 9713, loss = 0.64508377\n",
      "Iteration 9714, loss = 0.64507308\n",
      "Iteration 9715, loss = 0.64507722\n",
      "Iteration 9716, loss = 0.64507205\n",
      "Iteration 9717, loss = 0.64506729\n",
      "Iteration 9718, loss = 0.64505039\n",
      "Iteration 9719, loss = 0.64504495\n",
      "Iteration 9720, loss = 0.64504022\n",
      "Iteration 9721, loss = 0.64503648\n",
      "Iteration 9722, loss = 0.64505851\n",
      "Iteration 9723, loss = 0.64501339\n",
      "Iteration 9724, loss = 0.64501047\n",
      "Iteration 9725, loss = 0.64499624\n",
      "Iteration 9726, loss = 0.64500540\n",
      "Iteration 9727, loss = 0.64501292\n",
      "Iteration 9728, loss = 0.64500413\n",
      "Iteration 9729, loss = 0.64497458\n",
      "Iteration 9730, loss = 0.64496664\n",
      "Iteration 9731, loss = 0.64496914\n",
      "Iteration 9732, loss = 0.64495602\n",
      "Iteration 9733, loss = 0.64499638\n",
      "Iteration 9734, loss = 0.64494483\n",
      "Iteration 9735, loss = 0.64494009\n",
      "Iteration 9736, loss = 0.64492137\n",
      "Iteration 9737, loss = 0.64491903\n",
      "Iteration 9738, loss = 0.64492603\n",
      "Iteration 9739, loss = 0.64491102\n",
      "Iteration 9740, loss = 0.64492675\n",
      "Iteration 9741, loss = 0.64491489\n",
      "Iteration 9742, loss = 0.64488631\n",
      "Iteration 9743, loss = 0.64489111\n",
      "Iteration 9744, loss = 0.64487403\n",
      "Iteration 9745, loss = 0.64487271\n",
      "Iteration 9746, loss = 0.64486946\n",
      "Iteration 9747, loss = 0.64486862\n",
      "Iteration 9748, loss = 0.64485910\n",
      "Iteration 9749, loss = 0.64484075\n",
      "Iteration 9750, loss = 0.64483762\n",
      "Iteration 9751, loss = 0.64482597\n",
      "Iteration 9752, loss = 0.64481379\n",
      "Iteration 9753, loss = 0.64481729\n",
      "Iteration 9754, loss = 0.64480655\n",
      "Iteration 9755, loss = 0.64479862\n",
      "Iteration 9756, loss = 0.64479915\n",
      "Iteration 9757, loss = 0.64480850\n",
      "Iteration 9758, loss = 0.64478532\n",
      "Iteration 9759, loss = 0.64477201\n",
      "Iteration 9760, loss = 0.64478098\n",
      "Iteration 9761, loss = 0.64476460\n",
      "Iteration 9762, loss = 0.64475255\n",
      "Iteration 9763, loss = 0.64475120\n",
      "Iteration 9764, loss = 0.64475627\n",
      "Iteration 9765, loss = 0.64474257\n",
      "Iteration 9766, loss = 0.64474033\n",
      "Iteration 9767, loss = 0.64472304\n",
      "Iteration 9768, loss = 0.64472101\n",
      "Iteration 9769, loss = 0.64471325\n",
      "Iteration 9770, loss = 0.64473856\n",
      "Iteration 9771, loss = 0.64469721\n",
      "Iteration 9772, loss = 0.64470430\n",
      "Iteration 9773, loss = 0.64468971\n",
      "Iteration 9774, loss = 0.64468008\n",
      "Iteration 9775, loss = 0.64469816\n",
      "Iteration 9776, loss = 0.64467451\n",
      "Iteration 9777, loss = 0.64467024\n",
      "Iteration 9778, loss = 0.64464890\n",
      "Iteration 9779, loss = 0.64465472\n",
      "Iteration 9780, loss = 0.64464081\n",
      "Iteration 9781, loss = 0.64463244\n",
      "Iteration 9782, loss = 0.64464171\n",
      "Iteration 9783, loss = 0.64462256\n",
      "Iteration 9784, loss = 0.64462702\n",
      "Iteration 9785, loss = 0.64461473\n",
      "Iteration 9786, loss = 0.64460738\n",
      "Iteration 9787, loss = 0.64461234\n",
      "Iteration 9788, loss = 0.64458591\n",
      "Iteration 9789, loss = 0.64458042\n",
      "Iteration 9790, loss = 0.64457269\n",
      "Iteration 9791, loss = 0.64458024\n",
      "Iteration 9792, loss = 0.64456671\n",
      "Iteration 9793, loss = 0.64456826\n",
      "Iteration 9794, loss = 0.64457116\n",
      "Iteration 9795, loss = 0.64454913\n",
      "Iteration 9796, loss = 0.64454324\n",
      "Iteration 9797, loss = 0.64452934\n",
      "Iteration 9798, loss = 0.64454900\n",
      "Iteration 9799, loss = 0.64451529\n",
      "Iteration 9800, loss = 0.64452663\n",
      "Iteration 9801, loss = 0.64453406\n",
      "Iteration 9802, loss = 0.64450303\n",
      "Iteration 9803, loss = 0.64451597\n",
      "Iteration 9804, loss = 0.64448523\n",
      "Iteration 9805, loss = 0.64448731\n",
      "Iteration 9806, loss = 0.64448092\n",
      "Iteration 9807, loss = 0.64448408\n",
      "Iteration 9808, loss = 0.64446220\n",
      "Iteration 9809, loss = 0.64446930\n",
      "Iteration 9810, loss = 0.64444790\n",
      "Iteration 9811, loss = 0.64446557\n",
      "Iteration 9812, loss = 0.64442860\n",
      "Iteration 9813, loss = 0.64443418\n",
      "Iteration 9814, loss = 0.64443505\n",
      "Iteration 9815, loss = 0.64443179\n",
      "Iteration 9816, loss = 0.64441970\n",
      "Iteration 9817, loss = 0.64440929\n",
      "Iteration 9818, loss = 0.64440498\n",
      "Iteration 9819, loss = 0.64439947\n",
      "Iteration 9820, loss = 0.64439601\n",
      "Iteration 9821, loss = 0.64438951\n",
      "Iteration 9822, loss = 0.64438853\n",
      "Iteration 9823, loss = 0.64438067\n",
      "Iteration 9824, loss = 0.64437182\n",
      "Iteration 9825, loss = 0.64438389\n",
      "Iteration 9826, loss = 0.64434878\n",
      "Iteration 9827, loss = 0.64438513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 9828, loss = 0.64435156\n",
      "Iteration 9829, loss = 0.64434332\n",
      "Iteration 9830, loss = 0.64432509\n",
      "Iteration 9831, loss = 0.64434307\n",
      "Iteration 9832, loss = 0.64432417\n",
      "Iteration 9833, loss = 0.64432449\n",
      "Iteration 9834, loss = 0.64431331\n",
      "Iteration 9835, loss = 0.64430602\n",
      "Iteration 9836, loss = 0.64429972\n",
      "Iteration 9837, loss = 0.64428726\n",
      "Iteration 9838, loss = 0.64427874\n",
      "Iteration 9839, loss = 0.64427562\n",
      "Iteration 9840, loss = 0.64427197\n",
      "Iteration 9841, loss = 0.64427841\n",
      "Iteration 9842, loss = 0.64427623\n",
      "Iteration 9843, loss = 0.64425760\n",
      "Iteration 9844, loss = 0.64426610\n",
      "Iteration 9845, loss = 0.64426140\n",
      "Iteration 9846, loss = 0.64423165\n",
      "Iteration 9847, loss = 0.64425292\n",
      "Iteration 9848, loss = 0.64422271\n",
      "Iteration 9849, loss = 0.64422578\n",
      "Iteration 9850, loss = 0.64421250\n",
      "Iteration 9851, loss = 0.64422140\n",
      "Iteration 9852, loss = 0.64420575\n",
      "Iteration 9853, loss = 0.64419503\n",
      "Iteration 9854, loss = 0.64418879\n",
      "Iteration 9855, loss = 0.64418973\n",
      "Iteration 9856, loss = 0.64417814\n",
      "Iteration 9857, loss = 0.64417935\n",
      "Iteration 9858, loss = 0.64417743\n",
      "Iteration 9859, loss = 0.64415443\n",
      "Iteration 9860, loss = 0.64416338\n",
      "Iteration 9861, loss = 0.64416147\n",
      "Iteration 9862, loss = 0.64417878\n",
      "Iteration 9863, loss = 0.64414127\n",
      "Iteration 9864, loss = 0.64413662\n",
      "Iteration 9865, loss = 0.64417865\n",
      "Iteration 9866, loss = 0.64412824\n",
      "Iteration 9867, loss = 0.64411539\n",
      "Iteration 9868, loss = 0.64410761\n",
      "Iteration 9869, loss = 0.64412634\n",
      "Iteration 9870, loss = 0.64409153\n",
      "Iteration 9871, loss = 0.64409074\n",
      "Iteration 9872, loss = 0.64409473\n",
      "Iteration 9873, loss = 0.64410361\n",
      "Iteration 9874, loss = 0.64407625\n",
      "Iteration 9875, loss = 0.64409258\n",
      "Iteration 9876, loss = 0.64406221\n",
      "Iteration 9877, loss = 0.64406087\n",
      "Iteration 9878, loss = 0.64404589\n",
      "Iteration 9879, loss = 0.64406668\n",
      "Iteration 9880, loss = 0.64404184\n",
      "Iteration 9881, loss = 0.64403771\n",
      "Iteration 9882, loss = 0.64404710\n",
      "Iteration 9883, loss = 0.64401621\n",
      "Iteration 9884, loss = 0.64404041\n",
      "Iteration 9885, loss = 0.64402518\n",
      "Iteration 9886, loss = 0.64399745\n",
      "Iteration 9887, loss = 0.64400562\n",
      "Iteration 9888, loss = 0.64398794\n",
      "Iteration 9889, loss = 0.64398779\n",
      "Iteration 9890, loss = 0.64397980\n",
      "Iteration 9891, loss = 0.64398621\n",
      "Iteration 9892, loss = 0.64401452\n",
      "Iteration 9893, loss = 0.64397653\n",
      "Iteration 9894, loss = 0.64397177\n",
      "Iteration 9895, loss = 0.64395863\n",
      "Iteration 9896, loss = 0.64394667\n",
      "Iteration 9897, loss = 0.64395564\n",
      "Iteration 9898, loss = 0.64393718\n",
      "Iteration 9899, loss = 0.64395285\n",
      "Iteration 9900, loss = 0.64392642\n",
      "Iteration 9901, loss = 0.64393032\n",
      "Iteration 9902, loss = 0.64392254\n",
      "Iteration 9903, loss = 0.64391388\n",
      "Iteration 9904, loss = 0.64391507\n",
      "Iteration 9905, loss = 0.64390115\n",
      "Iteration 9906, loss = 0.64388688\n",
      "Iteration 9907, loss = 0.64388378\n",
      "Iteration 9908, loss = 0.64389775\n",
      "Iteration 9909, loss = 0.64387760\n",
      "Iteration 9910, loss = 0.64387357\n",
      "Iteration 9911, loss = 0.64388208\n",
      "Iteration 9912, loss = 0.64386150\n",
      "Iteration 9913, loss = 0.64385642\n",
      "Iteration 9914, loss = 0.64384166\n",
      "Iteration 9915, loss = 0.64383684\n",
      "Iteration 9916, loss = 0.64384874\n",
      "Iteration 9917, loss = 0.64382618\n",
      "Iteration 9918, loss = 0.64384244\n",
      "Iteration 9919, loss = 0.64383527\n",
      "Iteration 9920, loss = 0.64382934\n",
      "Iteration 9921, loss = 0.64381411\n",
      "Iteration 9922, loss = 0.64379551\n",
      "Iteration 9923, loss = 0.64380049\n",
      "Iteration 9924, loss = 0.64379146\n",
      "Iteration 9925, loss = 0.64377797\n",
      "Iteration 9926, loss = 0.64378210\n",
      "Iteration 9927, loss = 0.64377255\n",
      "Iteration 9928, loss = 0.64376030\n",
      "Iteration 9929, loss = 0.64376529\n",
      "Iteration 9930, loss = 0.64377743\n",
      "Iteration 9931, loss = 0.64377474\n",
      "Iteration 9932, loss = 0.64375301\n",
      "Iteration 9933, loss = 0.64373964\n",
      "Iteration 9934, loss = 0.64373911\n",
      "Iteration 9935, loss = 0.64374151\n",
      "Iteration 9936, loss = 0.64372581\n",
      "Iteration 9937, loss = 0.64374027\n",
      "Iteration 9938, loss = 0.64373318\n",
      "Iteration 9939, loss = 0.64370965\n",
      "Iteration 9940, loss = 0.64370403\n",
      "Iteration 9941, loss = 0.64370571\n",
      "Iteration 9942, loss = 0.64368541\n",
      "Iteration 9943, loss = 0.64368351\n",
      "Iteration 9944, loss = 0.64367526\n",
      "Iteration 9945, loss = 0.64368041\n",
      "Iteration 9946, loss = 0.64367129\n",
      "Iteration 9947, loss = 0.64367758\n",
      "Iteration 9948, loss = 0.64369791\n",
      "Iteration 9949, loss = 0.64365007\n",
      "Iteration 9950, loss = 0.64365383\n",
      "Iteration 9951, loss = 0.64363839\n",
      "Iteration 9952, loss = 0.64363190\n",
      "Iteration 9953, loss = 0.64363812\n",
      "Iteration 9954, loss = 0.64362408\n",
      "Iteration 9955, loss = 0.64361411\n",
      "Iteration 9956, loss = 0.64361253\n",
      "Iteration 9957, loss = 0.64360148\n",
      "Iteration 9958, loss = 0.64361926\n",
      "Iteration 9959, loss = 0.64362474\n",
      "Iteration 9960, loss = 0.64361177\n",
      "Iteration 9961, loss = 0.64358590\n",
      "Iteration 9962, loss = 0.64358532\n",
      "Iteration 9963, loss = 0.64359745\n",
      "Iteration 9964, loss = 0.64358400\n",
      "Iteration 9965, loss = 0.64356201\n",
      "Iteration 9966, loss = 0.64356365\n",
      "Iteration 9967, loss = 0.64355418\n",
      "Iteration 9968, loss = 0.64354395\n",
      "Iteration 9969, loss = 0.64355643\n",
      "Iteration 9970, loss = 0.64353542\n",
      "Iteration 9971, loss = 0.64352771\n",
      "Iteration 9972, loss = 0.64351197\n",
      "Iteration 9973, loss = 0.64352285\n",
      "Iteration 9974, loss = 0.64349681\n",
      "Iteration 9975, loss = 0.64349195\n",
      "Iteration 9976, loss = 0.64349572\n",
      "Iteration 9977, loss = 0.64347933\n",
      "Iteration 9978, loss = 0.64347700\n",
      "Iteration 9979, loss = 0.64346641\n",
      "Iteration 9980, loss = 0.64350019\n",
      "Iteration 9981, loss = 0.64344882\n",
      "Iteration 9982, loss = 0.64345234\n",
      "Iteration 9983, loss = 0.64344329\n",
      "Iteration 9984, loss = 0.64344629\n",
      "Iteration 9985, loss = 0.64344212\n",
      "Iteration 9986, loss = 0.64341228\n",
      "Iteration 9987, loss = 0.64341227\n",
      "Iteration 9988, loss = 0.64341579\n",
      "Iteration 9989, loss = 0.64338847\n",
      "Iteration 9990, loss = 0.64339515\n",
      "Iteration 9991, loss = 0.64341061\n",
      "Iteration 9992, loss = 0.64337609\n",
      "Iteration 9993, loss = 0.64336809\n",
      "Iteration 9994, loss = 0.64335799\n",
      "Iteration 9995, loss = 0.64336353\n",
      "Iteration 9996, loss = 0.64334547\n",
      "Iteration 9997, loss = 0.64335618\n",
      "Iteration 9998, loss = 0.64335318\n",
      "Iteration 9999, loss = 0.64334349\n",
      "Iteration 10000, loss = 0.64332125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(3, 2, 1), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=10000,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=21, shuffle=True, solver='sgd',\n",
       "              tol=1e-09, validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_295 (Dense)            (None, 11)                121       \n",
      "_________________________________________________________________\n",
      "dense_296 (Dense)            (None, 5)                 60        \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 3)                 18        \n",
      "_________________________________________________________________\n",
      "dense_298 (Dense)            (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_299 (Dense)            (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 210\n",
      "Trainable params: 210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "538/538 [==============================] - 1s 948us/step - loss: 0.8256 - accuracy: 0.4796\n",
      "Epoch 2/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.7817 - accuracy: 0.4796\n",
      "Epoch 3/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.7443 - accuracy: 0.4796\n",
      "Epoch 4/500\n",
      "538/538 [==============================] - 0s 630us/step - loss: 0.7205 - accuracy: 0.4796\n",
      "Epoch 5/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.7072 - accuracy: 0.4796\n",
      "Epoch 6/500\n",
      "538/538 [==============================] - 0s 623us/step - loss: 0.7000 - accuracy: 0.4796\n",
      "Epoch 7/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6962 - accuracy: 0.4796\n",
      "Epoch 8/500\n",
      "538/538 [==============================] - 0s 692us/step - loss: 0.6947 - accuracy: 0.4796\n",
      "Epoch 9/500\n",
      "538/538 [==============================] - 0s 686us/step - loss: 0.6937 - accuracy: 0.4833\n",
      "Epoch 10/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6931 - accuracy: 0.5019\n",
      "Epoch 11/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 12/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 13/500\n",
      "538/538 [==============================] - 0s 626us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 14/500\n",
      "538/538 [==============================] - 0s 703us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 15/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 16/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 17/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 18/500\n",
      "538/538 [==============================] - 0s 725us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 19/500\n",
      "538/538 [==============================] - 0s 701us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 20/500\n",
      "538/538 [==============================] - 0s 626us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 21/500\n",
      "538/538 [==============================] - 0s 623us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 22/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 23/500\n",
      "538/538 [==============================] - 0s 624us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 24/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 25/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 26/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 27/500\n",
      "538/538 [==============================] - 0s 760us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 28/500\n",
      "538/538 [==============================] - 0s 751us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 29/500\n",
      "538/538 [==============================] - 0s 756us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 30/500\n",
      "538/538 [==============================] - 0s 686us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 31/500\n",
      "538/538 [==============================] - 0s 742us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 32/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 33/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 34/500\n",
      "538/538 [==============================] - 0s 623us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 35/500\n",
      "538/538 [==============================] - 0s 613us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 36/500\n",
      "538/538 [==============================] - 0s 640us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 37/500\n",
      "538/538 [==============================] - 0s 625us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 38/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 39/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 40/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 41/500\n",
      "538/538 [==============================] - 0s 626us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 42/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 43/500\n",
      "538/538 [==============================] - 0s 623us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 44/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6929 - accuracy: 0.5204\n",
      "Epoch 45/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 46/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 47/500\n",
      "538/538 [==============================] - 0s 626us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 48/500\n",
      "538/538 [==============================] - 0s 642us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 49/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 50/500\n",
      "538/538 [==============================] - 0s 628us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 51/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 52/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 53/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 54/500\n",
      "538/538 [==============================] - 0s 669us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 55/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 56/500\n",
      "538/538 [==============================] - 0s 664us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 57/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 58/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 59/500\n",
      "538/538 [==============================] - 0s 777us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 60/500\n",
      "538/538 [==============================] - 0s 770us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 61/500\n",
      "538/538 [==============================] - 0s 762us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 62/500\n",
      "538/538 [==============================] - 0s 697us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 63/500\n",
      "538/538 [==============================] - 0s 706us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 64/500\n",
      "538/538 [==============================] - 0s 760us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 65/500\n",
      "538/538 [==============================] - 0s 684us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 66/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 67/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 68/500\n",
      "538/538 [==============================] - 0s 732us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 69/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 70/500\n",
      "538/538 [==============================] - 0s 657us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 71/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 72/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 73/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 74/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 75/500\n",
      "538/538 [==============================] - 0s 660us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 76/500\n",
      "538/538 [==============================] - 0s 640us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 77/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 78/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 79/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 80/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 81/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 82/500\n",
      "538/538 [==============================] - 0s 637us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 83/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 84/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 85/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 86/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 87/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 88/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 89/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 90/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 91/500\n",
      "538/538 [==============================] - 0s 680us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 92/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 93/500\n",
      "538/538 [==============================] - 0s 660us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 94/500\n",
      "538/538 [==============================] - 0s 630us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 95/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 96/500\n",
      "538/538 [==============================] - 0s 661us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 97/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 98/500\n",
      "538/538 [==============================] - 0s 664us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 99/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 100/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 101/500\n",
      "538/538 [==============================] - 0s 662us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 102/500\n",
      "538/538 [==============================] - 0s 660us/step - loss: 0.6930 - accuracy: 0.5204\n",
      "Epoch 103/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 104/500\n",
      "538/538 [==============================] - 0s 671us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 105/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 106/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 107/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 108/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 109/500\n",
      "538/538 [==============================] - 0s 644us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 110/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 111/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 112/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 113/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 114/500\n",
      "538/538 [==============================] - 0s 640us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 115/500\n",
      "538/538 [==============================] - 0s 662us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 116/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 117/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 118/500\n",
      "538/538 [==============================] - 0s 671us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 119/500\n",
      "538/538 [==============================] - 0s 655us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 120/500\n",
      "538/538 [==============================] - 0s 675us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 121/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 122/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 123/500\n",
      "538/538 [==============================] - 0s 675us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 124/500\n",
      "538/538 [==============================] - 0s 697us/step - loss: 0.6930 - accuracy: 0.5204\n",
      "Epoch 125/500\n",
      "538/538 [==============================] - 0s 741us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 126/500\n",
      "538/538 [==============================] - 0s 665us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 127/500\n",
      "538/538 [==============================] - 0s 623us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 128/500\n",
      "538/538 [==============================] - 0s 693us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 129/500\n",
      "538/538 [==============================] - 0s 690us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 130/500\n",
      "538/538 [==============================] - 0s 751us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 131/500\n",
      "538/538 [==============================] - 0s 721us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 132/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 133/500\n",
      "538/538 [==============================] - 0s 627us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 134/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 135/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 136/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 137/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 138/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 139/500\n",
      "538/538 [==============================] - 0s 670us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 140/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 141/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 142/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 143/500\n",
      "538/538 [==============================] - 0s 660us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 144/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 145/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 146/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 147/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 148/500\n",
      "538/538 [==============================] - 0s 820us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 149/500\n",
      "538/538 [==============================] - 0s 664us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 150/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 151/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6933 - accuracy: 0.5204\n",
      "Epoch 152/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 153/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 154/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 155/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 156/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 157/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 158/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 159/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 160/500\n",
      "538/538 [==============================] - 0s 664us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 161/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 162/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 163/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 164/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 165/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 166/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 167/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 168/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 169/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 170/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 171/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 172/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 173/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 174/500\n",
      "538/538 [==============================] - 0s 642us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 175/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 176/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 177/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 178/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6929 - accuracy: 0.5204\n",
      "Epoch 179/500\n",
      "538/538 [==============================] - 0s 642us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 180/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 181/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 182/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 183/500\n",
      "538/538 [==============================] - 0s 749us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 184/500\n",
      "538/538 [==============================] - 0s 717us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 185/500\n",
      "538/538 [==============================] - 0s 719us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 186/500\n",
      "538/538 [==============================] - 0s 690us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 187/500\n",
      "538/538 [==============================] - 0s 725us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 188/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 189/500\n",
      "538/538 [==============================] - 0s 675us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 190/500\n",
      "538/538 [==============================] - 0s 691us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 191/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 192/500\n",
      "538/538 [==============================] - 0s 623us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 193/500\n",
      "538/538 [==============================] - 0s 642us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 194/500\n",
      "538/538 [==============================] - 0s 736us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 195/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 196/500\n",
      "538/538 [==============================] - 0s 743us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 197/500\n",
      "538/538 [==============================] - 0s 697us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 198/500\n",
      "538/538 [==============================] - 0s 662us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 199/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 200/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 201/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 202/500\n",
      "538/538 [==============================] - 0s 790us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 203/500\n",
      "538/538 [==============================] - 0s 773us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 204/500\n",
      "538/538 [==============================] - 0s 762us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 205/500\n",
      "538/538 [==============================] - 0s 749us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 206/500\n",
      "538/538 [==============================] - 0s 695us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 207/500\n",
      "538/538 [==============================] - 0s 628us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 208/500\n",
      "538/538 [==============================] - 0s 624us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 209/500\n",
      "538/538 [==============================] - 0s 604us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 210/500\n",
      "538/538 [==============================] - 0s 640us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 211/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 212/500\n",
      "538/538 [==============================] - 0s 626us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 213/500\n",
      "538/538 [==============================] - 0s 671us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 214/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 215/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 216/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 217/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 218/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 219/500\n",
      "538/538 [==============================] - 0s 644us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 220/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 221/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 222/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 223/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 224/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 225/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 226/500\n",
      "538/538 [==============================] - 0s 630us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 227/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 228/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 229/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 230/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 231/500\n",
      "538/538 [==============================] - 0s 653us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 232/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 233/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 234/500\n",
      "538/538 [==============================] - 0s 758us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 235/500\n",
      "538/538 [==============================] - 0s 667us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 236/500\n",
      "538/538 [==============================] - 0s 760us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 237/500\n",
      "538/538 [==============================] - 0s 623us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 238/500\n",
      "538/538 [==============================] - 0s 635us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 239/500\n",
      "538/538 [==============================] - 0s 736us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 240/500\n",
      "538/538 [==============================] - 0s 673us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 241/500\n",
      "538/538 [==============================] - 0s 742us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 242/500\n",
      "538/538 [==============================] - 0s 731us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 243/500\n",
      "538/538 [==============================] - 0s 712us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 244/500\n",
      "538/538 [==============================] - 0s 752us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 245/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 246/500\n",
      "538/538 [==============================] - 0s 775us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 247/500\n",
      "538/538 [==============================] - 0s 773us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 248/500\n",
      "538/538 [==============================] - 0s 667us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 249/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 250/500\n",
      "538/538 [==============================] - 0s 619us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 251/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 252/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 253/500\n",
      "538/538 [==============================] - 0s 626us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 254/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 255/500\n",
      "538/538 [==============================] - 0s 665us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 256/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6931 - accuracy: 0.5204\n",
      "Epoch 257/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 258/500\n",
      "538/538 [==============================] - 0s 635us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 259/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 260/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 261/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 262/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 263/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 264/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 265/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 266/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 267/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 268/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 269/500\n",
      "538/538 [==============================] - 0s 642us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 270/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 271/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 272/500\n",
      "538/538 [==============================] - 0s 664us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 273/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6929 - accuracy: 0.5204\n",
      "Epoch 274/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 275/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 276/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 277/500\n",
      "538/538 [==============================] - 0s 667us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 278/500\n",
      "538/538 [==============================] - 0s 801us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 279/500\n",
      "538/538 [==============================] - 0s 736us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 280/500\n",
      "538/538 [==============================] - 0s 833us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 281/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 282/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 283/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 284/500\n",
      "538/538 [==============================] - 0s 626us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 285/500\n",
      "538/538 [==============================] - 0s 631us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 286/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 287/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 288/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 289/500\n",
      "538/538 [==============================] - 0s 650us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 290/500\n",
      "538/538 [==============================] - 0s 693us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 291/500\n",
      "538/538 [==============================] - 0s 705us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 292/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 293/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 294/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 295/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 296/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 297/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 298/500\n",
      "538/538 [==============================] - 0s 662us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 299/500\n",
      "538/538 [==============================] - 0s 646us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 300/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 301/500\n",
      "538/538 [==============================] - 0s 644us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 302/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 303/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 304/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 305/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 306/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 307/500\n",
      "538/538 [==============================] - 0s 640us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 308/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 309/500\n",
      "538/538 [==============================] - 0s 663us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 310/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 311/500\n",
      "538/538 [==============================] - 0s 662us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 312/500\n",
      "538/538 [==============================] - 0s 660us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 313/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538/538 [==============================] - 0s 664us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 314/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 315/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 316/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 317/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 318/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 319/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 320/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6929 - accuracy: 0.5204\n",
      "Epoch 321/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 322/500\n",
      "538/538 [==============================] - 0s 704us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 323/500\n",
      "538/538 [==============================] - 0s 784us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 324/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 325/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 326/500\n",
      "538/538 [==============================] - 0s 628us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 327/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 328/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 329/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 330/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 331/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 332/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 333/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 334/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 335/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 336/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 337/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 338/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 339/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 340/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 341/500\n",
      "538/538 [==============================] - 0s 646us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 342/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 343/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 344/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 345/500\n",
      "538/538 [==============================] - 0s 669us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 346/500\n",
      "538/538 [==============================] - 0s 682us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 347/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 348/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 349/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 350/500\n",
      "538/538 [==============================] - 0s 642us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 351/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 352/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 353/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 354/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 355/500\n",
      "538/538 [==============================] - 0s 650us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 356/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 357/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 358/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 359/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 360/500\n",
      "538/538 [==============================] - 0s 690us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 361/500\n",
      "538/538 [==============================] - 0s 805us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 362/500\n",
      "538/538 [==============================] - 0s 703us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 363/500\n",
      "538/538 [==============================] - 0s 606us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 364/500\n",
      "538/538 [==============================] - 0s 599us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 365/500\n",
      "538/538 [==============================] - 0s 584us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 366/500\n",
      "538/538 [==============================] - 0s 587us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 367/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 368/500\n",
      "538/538 [==============================] - 0s 594us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 369/500\n",
      "538/538 [==============================] - 0s 602us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 370/500\n",
      "538/538 [==============================] - 0s 589us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 371/500\n",
      "538/538 [==============================] - 0s 606us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 372/500\n",
      "538/538 [==============================] - 0s 599us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 373/500\n",
      "538/538 [==============================] - 0s 586us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 374/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 375/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 376/500\n",
      "538/538 [==============================] - 0s 585us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 377/500\n",
      "538/538 [==============================] - 0s 612us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 378/500\n",
      "538/538 [==============================] - 0s 589us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 379/500\n",
      "538/538 [==============================] - 0s 587us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 380/500\n",
      "538/538 [==============================] - 0s 585us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 381/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 382/500\n",
      "538/538 [==============================] - 0s 726us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 383/500\n",
      "538/538 [==============================] - 0s 606us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 384/500\n",
      "538/538 [==============================] - 0s 591us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 385/500\n",
      "538/538 [==============================] - 0s 584us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 386/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 387/500\n",
      "538/538 [==============================] - 0s 662us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 388/500\n",
      "538/538 [==============================] - 0s 584us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 389/500\n",
      "538/538 [==============================] - 0s 586us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 390/500\n",
      "538/538 [==============================] - 0s 593us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 391/500\n",
      "538/538 [==============================] - 0s 576us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 392/500\n",
      "538/538 [==============================] - 0s 586us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 393/500\n",
      "538/538 [==============================] - 0s 587us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 394/500\n",
      "538/538 [==============================] - 0s 602us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 395/500\n",
      "538/538 [==============================] - 0s 608us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 396/500\n",
      "538/538 [==============================] - 0s 695us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 397/500\n",
      "538/538 [==============================] - 0s 727us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 398/500\n",
      "538/538 [==============================] - 0s 699us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 399/500\n",
      "538/538 [==============================] - 0s 725us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 400/500\n",
      "538/538 [==============================] - 0s 591us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 401/500\n",
      "538/538 [==============================] - 0s 595us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 402/500\n",
      "538/538 [==============================] - 0s 580us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 403/500\n",
      "538/538 [==============================] - 0s 589us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 404/500\n",
      "538/538 [==============================] - 0s 605us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 405/500\n",
      "538/538 [==============================] - 0s 604us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 406/500\n",
      "538/538 [==============================] - 0s 592us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 407/500\n",
      "538/538 [==============================] - 0s 595us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 408/500\n",
      "538/538 [==============================] - 0s 606us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 409/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 410/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 411/500\n",
      "538/538 [==============================] - 0s 599us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 412/500\n",
      "538/538 [==============================] - 0s 601us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 413/500\n",
      "538/538 [==============================] - 0s 612us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 414/500\n",
      "538/538 [==============================] - 0s 593us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 415/500\n",
      "538/538 [==============================] - 0s 608us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 416/500\n",
      "538/538 [==============================] - 0s 621us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 417/500\n",
      "538/538 [==============================] - 0s 599us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 418/500\n",
      "538/538 [==============================] - 0s 598us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 419/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 420/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 421/500\n",
      "538/538 [==============================] - 0s 612us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 422/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 423/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 424/500\n",
      "538/538 [==============================] - 0s 598us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 425/500\n",
      "538/538 [==============================] - 0s 599us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 426/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 427/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 428/500\n",
      "538/538 [==============================] - 0s 593us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 429/500\n",
      "538/538 [==============================] - 0s 602us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 430/500\n",
      "538/538 [==============================] - 0s 693us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 431/500\n",
      "538/538 [==============================] - 0s 671us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 432/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 433/500\n",
      "538/538 [==============================] - 0s 612us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 434/500\n",
      "538/538 [==============================] - 0s 591us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 435/500\n",
      "538/538 [==============================] - 0s 576us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 436/500\n",
      "538/538 [==============================] - 0s 586us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 437/500\n",
      "538/538 [==============================] - 0s 593us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 438/500\n",
      "538/538 [==============================] - 0s 602us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 439/500\n",
      "538/538 [==============================] - 0s 608us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 440/500\n",
      "538/538 [==============================] - 0s 599us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 441/500\n",
      "538/538 [==============================] - 0s 595us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 442/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 443/500\n",
      "538/538 [==============================] - 0s 600us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 444/500\n",
      "538/538 [==============================] - 0s 599us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 445/500\n",
      "538/538 [==============================] - 0s 602us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 446/500\n",
      "538/538 [==============================] - 0s 597us/step - loss: 0.6931 - accuracy: 0.5204\n",
      "Epoch 447/500\n",
      "538/538 [==============================] - 0s 699us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 448/500\n",
      "538/538 [==============================] - 0s 712us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 449/500\n",
      "538/538 [==============================] - 0s 656us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 450/500\n",
      "538/538 [==============================] - 0s 640us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 451/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 452/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 453/500\n",
      "538/538 [==============================] - 0s 740us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 454/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 455/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 456/500\n",
      "538/538 [==============================] - 0s 638us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 457/500\n",
      "538/538 [==============================] - 0s 712us/step - loss: 0.6929 - accuracy: 0.5204\n",
      "Epoch 458/500\n",
      "538/538 [==============================] - 0s 691us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 459/500\n",
      "538/538 [==============================] - 0s 703us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 460/500\n",
      "538/538 [==============================] - 0s 790us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 461/500\n",
      "538/538 [==============================] - 0s 699us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 462/500\n",
      "538/538 [==============================] - 0s 680us/step - loss: 0.6928 - accuracy: 0.5204\n",
      "Epoch 463/500\n",
      "538/538 [==============================] - 0s 772us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 464/500\n",
      "538/538 [==============================] - 0s 755us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 465/500\n",
      "538/538 [==============================] - 0s 662us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 466/500\n",
      "538/538 [==============================] - 0s 665us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 467/500\n",
      "538/538 [==============================] - 0s 634us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 468/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 469/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538/538 [==============================] - 0s 664us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 470/500\n",
      "538/538 [==============================] - 0s 738us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 471/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 472/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 473/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 474/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 475/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 476/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 477/500\n",
      "538/538 [==============================] - 0s 652us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 478/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 479/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 480/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 481/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 482/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 483/500\n",
      "538/538 [==============================] - 0s 664us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 484/500\n",
      "538/538 [==============================] - 0s 645us/step - loss: 0.6929 - accuracy: 0.5204\n",
      "Epoch 485/500\n",
      "538/538 [==============================] - 0s 663us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 486/500\n",
      "538/538 [==============================] - 0s 636us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 487/500\n",
      "538/538 [==============================] - 0s 647us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 488/500\n",
      "538/538 [==============================] - 0s 632us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 489/500\n",
      "538/538 [==============================] - 0s 628us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 490/500\n",
      "538/538 [==============================] - 0s 641us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 491/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 492/500\n",
      "538/538 [==============================] - 0s 658us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 493/500\n",
      "538/538 [==============================] - 0s 651us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 494/500\n",
      "538/538 [==============================] - 0s 643us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 495/500\n",
      "538/538 [==============================] - 0s 680us/step - loss: 0.6926 - accuracy: 0.5204\n",
      "Epoch 496/500\n",
      "538/538 [==============================] - 0s 649us/step - loss: 0.6925 - accuracy: 0.5204\n",
      "Epoch 497/500\n",
      "538/538 [==============================] - 0s 654us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 498/500\n",
      "538/538 [==============================] - 0s 639us/step - loss: 0.6927 - accuracy: 0.5204\n",
      "Epoch 499/500\n",
      "538/538 [==============================] - 0s 740us/step - loss: 0.6924 - accuracy: 0.5204\n",
      "Epoch 500/500\n",
      "538/538 [==============================] - 0s 723us/step - loss: 0.6925 - accuracy: 0.5204\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,  epochs=500, batch_size=5,  verbose=1) # validation_split=0.1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 520 samples, validate on 58 samples\n",
      "Epoch 1/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6930 - accuracy: 0.5231 - val_loss: 0.6984 - val_accuracy: 0.3966\n",
      "Epoch 2/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6927 - accuracy: 0.5231 - val_loss: 0.7002 - val_accuracy: 0.3966\n",
      "Epoch 3/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6926 - accuracy: 0.5231 - val_loss: 0.7018 - val_accuracy: 0.3966\n",
      "Epoch 4/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.5231 - val_loss: 0.7036 - val_accuracy: 0.3966\n",
      "Epoch 5/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.5231 - val_loss: 0.7045 - val_accuracy: 0.3966\n",
      "Epoch 6/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6925 - accuracy: 0.5231 - val_loss: 0.7051 - val_accuracy: 0.3966\n",
      "Epoch 7/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.5231 - val_loss: 0.7042 - val_accuracy: 0.3966\n",
      "Epoch 8/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6926 - accuracy: 0.5231 - val_loss: 0.7028 - val_accuracy: 0.3966\n",
      "Epoch 9/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6923 - accuracy: 0.5231 - val_loss: 0.7027 - val_accuracy: 0.3966\n",
      "Epoch 10/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6923 - accuracy: 0.5231 - val_loss: 0.7041 - val_accuracy: 0.3966\n",
      "Epoch 11/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6924 - accuracy: 0.5231 - val_loss: 0.7037 - val_accuracy: 0.3966\n",
      "Epoch 12/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6921 - accuracy: 0.5231 - val_loss: 0.7034 - val_accuracy: 0.3966\n",
      "Epoch 13/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6918 - accuracy: 0.5231 - val_loss: 0.7035 - val_accuracy: 0.3966\n",
      "Epoch 14/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6915 - accuracy: 0.5231 - val_loss: 0.7022 - val_accuracy: 0.3966\n",
      "Epoch 15/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6914 - accuracy: 0.5231 - val_loss: 0.7000 - val_accuracy: 0.3966\n",
      "Epoch 16/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6907 - accuracy: 0.5250 - val_loss: 0.7005 - val_accuracy: 0.3966\n",
      "Epoch 17/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6901 - accuracy: 0.5192 - val_loss: 0.6983 - val_accuracy: 0.5000\n",
      "Epoch 18/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6887 - accuracy: 0.5269 - val_loss: 0.6980 - val_accuracy: 0.4828\n",
      "Epoch 19/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6877 - accuracy: 0.5423 - val_loss: 0.6939 - val_accuracy: 0.5345\n",
      "Epoch 20/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6865 - accuracy: 0.5558 - val_loss: 0.6914 - val_accuracy: 0.6034\n",
      "Epoch 21/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6854 - accuracy: 0.5692 - val_loss: 0.6918 - val_accuracy: 0.5862\n",
      "Epoch 22/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6840 - accuracy: 0.5596 - val_loss: 0.6909 - val_accuracy: 0.6207\n",
      "Epoch 23/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6836 - accuracy: 0.5865 - val_loss: 0.6947 - val_accuracy: 0.5000\n",
      "Epoch 24/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6833 - accuracy: 0.5635 - val_loss: 0.6902 - val_accuracy: 0.5862\n",
      "Epoch 25/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6826 - accuracy: 0.5692 - val_loss: 0.6950 - val_accuracy: 0.5690\n",
      "Epoch 26/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6821 - accuracy: 0.5827 - val_loss: 0.6922 - val_accuracy: 0.5000\n",
      "Epoch 27/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6818 - accuracy: 0.5731 - val_loss: 0.6882 - val_accuracy: 0.6034\n",
      "Epoch 28/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6814 - accuracy: 0.5731 - val_loss: 0.6905 - val_accuracy: 0.6034\n",
      "Epoch 29/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6811 - accuracy: 0.5846 - val_loss: 0.6964 - val_accuracy: 0.5690\n",
      "Epoch 30/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6814 - accuracy: 0.5769 - val_loss: 0.6915 - val_accuracy: 0.6034\n",
      "Epoch 31/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6806 - accuracy: 0.5673 - val_loss: 0.6922 - val_accuracy: 0.6034\n",
      "Epoch 32/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6803 - accuracy: 0.5846 - val_loss: 0.6921 - val_accuracy: 0.5862\n",
      "Epoch 33/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6798 - accuracy: 0.5827 - val_loss: 0.6887 - val_accuracy: 0.6207\n",
      "Epoch 34/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6807 - accuracy: 0.5731 - val_loss: 0.6900 - val_accuracy: 0.6034\n",
      "Epoch 35/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6800 - accuracy: 0.5827 - val_loss: 0.6896 - val_accuracy: 0.6034\n",
      "Epoch 36/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6795 - accuracy: 0.5827 - val_loss: 0.6912 - val_accuracy: 0.6207\n",
      "Epoch 37/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6793 - accuracy: 0.5827 - val_loss: 0.6920 - val_accuracy: 0.6034\n",
      "Epoch 38/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6795 - accuracy: 0.5846 - val_loss: 0.6861 - val_accuracy: 0.6207\n",
      "Epoch 39/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6788 - accuracy: 0.5923 - val_loss: 0.6843 - val_accuracy: 0.6034\n",
      "Epoch 40/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6787 - accuracy: 0.5827 - val_loss: 0.6864 - val_accuracy: 0.6034\n",
      "Epoch 41/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6786 - accuracy: 0.5865 - val_loss: 0.6876 - val_accuracy: 0.6034\n",
      "Epoch 42/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6781 - accuracy: 0.5962 - val_loss: 0.6852 - val_accuracy: 0.5862\n",
      "Epoch 43/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6788 - accuracy: 0.5769 - val_loss: 0.6893 - val_accuracy: 0.5690\n",
      "Epoch 44/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6779 - accuracy: 0.5904 - val_loss: 0.6907 - val_accuracy: 0.5862\n",
      "Epoch 45/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6775 - accuracy: 0.5808 - val_loss: 0.6885 - val_accuracy: 0.6034\n",
      "Epoch 46/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6777 - accuracy: 0.5865 - val_loss: 0.6848 - val_accuracy: 0.6034\n",
      "Epoch 47/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6773 - accuracy: 0.5827 - val_loss: 0.6902 - val_accuracy: 0.5345\n",
      "Epoch 48/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6771 - accuracy: 0.5962 - val_loss: 0.6879 - val_accuracy: 0.5862\n",
      "Epoch 49/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6765 - accuracy: 0.5923 - val_loss: 0.6869 - val_accuracy: 0.6034\n",
      "Epoch 50/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6774 - accuracy: 0.5865 - val_loss: 0.6872 - val_accuracy: 0.5862\n",
      "Epoch 51/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6762 - accuracy: 0.5923 - val_loss: 0.6870 - val_accuracy: 0.5862\n",
      "Epoch 52/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6762 - accuracy: 0.5923 - val_loss: 0.6890 - val_accuracy: 0.5862\n",
      "Epoch 53/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6763 - accuracy: 0.5981 - val_loss: 0.6875 - val_accuracy: 0.5862\n",
      "Epoch 54/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6758 - accuracy: 0.5904 - val_loss: 0.6841 - val_accuracy: 0.6034\n",
      "Epoch 55/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6752 - accuracy: 0.6058 - val_loss: 0.6858 - val_accuracy: 0.5690\n",
      "Epoch 56/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6752 - accuracy: 0.5904 - val_loss: 0.6906 - val_accuracy: 0.5345\n",
      "Epoch 57/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6752 - accuracy: 0.5885 - val_loss: 0.6863 - val_accuracy: 0.5517\n",
      "Epoch 58/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6750 - accuracy: 0.5904 - val_loss: 0.6865 - val_accuracy: 0.5690\n",
      "Epoch 59/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6743 - accuracy: 0.5904 - val_loss: 0.6830 - val_accuracy: 0.6034\n",
      "Epoch 60/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6745 - accuracy: 0.6000 - val_loss: 0.6851 - val_accuracy: 0.6034\n",
      "Epoch 61/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6742 - accuracy: 0.5942 - val_loss: 0.6830 - val_accuracy: 0.6034\n",
      "Epoch 62/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6737 - accuracy: 0.5942 - val_loss: 0.6877 - val_accuracy: 0.5517\n",
      "Epoch 63/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6738 - accuracy: 0.5942 - val_loss: 0.6792 - val_accuracy: 0.6034\n",
      "Epoch 64/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6729 - accuracy: 0.5942 - val_loss: 0.6912 - val_accuracy: 0.5345\n",
      "Epoch 65/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6727 - accuracy: 0.6000 - val_loss: 0.6874 - val_accuracy: 0.5690\n",
      "Epoch 66/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6728 - accuracy: 0.5942 - val_loss: 0.6899 - val_accuracy: 0.5690\n",
      "Epoch 67/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6726 - accuracy: 0.5942 - val_loss: 0.6816 - val_accuracy: 0.6034\n",
      "Epoch 68/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6717 - accuracy: 0.6000 - val_loss: 0.6897 - val_accuracy: 0.5345\n",
      "Epoch 69/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6721 - accuracy: 0.6096 - val_loss: 0.6833 - val_accuracy: 0.5862\n",
      "Epoch 70/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6718 - accuracy: 0.5981 - val_loss: 0.6965 - val_accuracy: 0.5345\n",
      "Epoch 71/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6704 - accuracy: 0.6077 - val_loss: 0.6897 - val_accuracy: 0.5690\n",
      "Epoch 72/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6703 - accuracy: 0.6000 - val_loss: 0.6911 - val_accuracy: 0.5690\n",
      "Epoch 73/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6705 - accuracy: 0.6192 - val_loss: 0.6907 - val_accuracy: 0.5345\n",
      "Epoch 74/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6705 - accuracy: 0.6077 - val_loss: 0.6868 - val_accuracy: 0.5690\n",
      "Epoch 75/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6699 - accuracy: 0.6000 - val_loss: 0.6925 - val_accuracy: 0.5690\n",
      "Epoch 76/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6697 - accuracy: 0.6135 - val_loss: 0.6934 - val_accuracy: 0.5517\n",
      "Epoch 77/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6692 - accuracy: 0.6173 - val_loss: 0.6851 - val_accuracy: 0.5690\n",
      "Epoch 78/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6685 - accuracy: 0.6135 - val_loss: 0.6960 - val_accuracy: 0.5345\n",
      "Epoch 79/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6683 - accuracy: 0.6135 - val_loss: 0.6986 - val_accuracy: 0.5345\n",
      "Epoch 80/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6690 - accuracy: 0.6019 - val_loss: 0.6879 - val_accuracy: 0.5862\n",
      "Epoch 81/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6687 - accuracy: 0.6000 - val_loss: 0.6886 - val_accuracy: 0.5862\n",
      "Epoch 82/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6684 - accuracy: 0.6173 - val_loss: 0.6928 - val_accuracy: 0.5345\n",
      "Epoch 83/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6679 - accuracy: 0.6077 - val_loss: 0.6951 - val_accuracy: 0.5345\n",
      "Epoch 84/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6686 - accuracy: 0.6000 - val_loss: 0.6858 - val_accuracy: 0.5862\n",
      "Epoch 85/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6677 - accuracy: 0.6135 - val_loss: 0.6920 - val_accuracy: 0.5517\n",
      "Epoch 86/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6673 - accuracy: 0.6019 - val_loss: 0.6932 - val_accuracy: 0.5345\n",
      "Epoch 87/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6667 - accuracy: 0.6192 - val_loss: 0.6984 - val_accuracy: 0.5345\n",
      "Epoch 88/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6676 - accuracy: 0.6115 - val_loss: 0.6930 - val_accuracy: 0.5517\n",
      "Epoch 89/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6662 - accuracy: 0.6058 - val_loss: 0.6988 - val_accuracy: 0.5345\n",
      "Epoch 90/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6664 - accuracy: 0.6077 - val_loss: 0.6891 - val_accuracy: 0.5690\n",
      "Epoch 91/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6663 - accuracy: 0.6096 - val_loss: 0.6931 - val_accuracy: 0.5517\n",
      "Epoch 92/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6663 - accuracy: 0.6115 - val_loss: 0.6989 - val_accuracy: 0.5172\n",
      "Epoch 93/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6668 - accuracy: 0.6192 - val_loss: 0.6816 - val_accuracy: 0.6034\n",
      "Epoch 94/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6659 - accuracy: 0.6000 - val_loss: 0.6865 - val_accuracy: 0.5862\n",
      "Epoch 95/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6659 - accuracy: 0.6231 - val_loss: 0.6883 - val_accuracy: 0.5690\n",
      "Epoch 96/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6661 - accuracy: 0.6096 - val_loss: 0.6959 - val_accuracy: 0.5345\n",
      "Epoch 97/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6654 - accuracy: 0.6058 - val_loss: 0.6939 - val_accuracy: 0.5517\n",
      "Epoch 98/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6653 - accuracy: 0.6115 - val_loss: 0.6904 - val_accuracy: 0.5690\n",
      "Epoch 99/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6650 - accuracy: 0.6135 - val_loss: 0.6873 - val_accuracy: 0.5690\n",
      "Epoch 100/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6646 - accuracy: 0.6192 - val_loss: 0.6919 - val_accuracy: 0.5690\n",
      "Epoch 101/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6647 - accuracy: 0.6212 - val_loss: 0.6924 - val_accuracy: 0.5517\n",
      "Epoch 102/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6645 - accuracy: 0.6154 - val_loss: 0.6976 - val_accuracy: 0.5517\n",
      "Epoch 103/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6641 - accuracy: 0.6173 - val_loss: 0.6915 - val_accuracy: 0.5517\n",
      "Epoch 104/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6644 - accuracy: 0.6154 - val_loss: 0.6936 - val_accuracy: 0.5517\n",
      "Epoch 105/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6623 - accuracy: 0.6077 - val_loss: 0.6820 - val_accuracy: 0.5690\n",
      "Epoch 106/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6636 - accuracy: 0.6135 - val_loss: 0.6971 - val_accuracy: 0.5345\n",
      "Epoch 107/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6635 - accuracy: 0.6250 - val_loss: 0.7004 - val_accuracy: 0.5517\n",
      "Epoch 108/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6628 - accuracy: 0.6250 - val_loss: 0.6879 - val_accuracy: 0.5690\n",
      "Epoch 109/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6632 - accuracy: 0.6173 - val_loss: 0.6923 - val_accuracy: 0.5690\n",
      "Epoch 110/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6622 - accuracy: 0.6250 - val_loss: 0.6959 - val_accuracy: 0.5517\n",
      "Epoch 111/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6617 - accuracy: 0.6212 - val_loss: 0.6959 - val_accuracy: 0.5690\n",
      "Epoch 112/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6605 - accuracy: 0.6192 - val_loss: 0.6915 - val_accuracy: 0.5690\n",
      "Epoch 113/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6593 - accuracy: 0.6308 - val_loss: 0.7069 - val_accuracy: 0.5345\n",
      "Epoch 114/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6603 - accuracy: 0.6308 - val_loss: 0.7042 - val_accuracy: 0.5517\n",
      "Epoch 115/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6597 - accuracy: 0.6250 - val_loss: 0.6953 - val_accuracy: 0.5517\n",
      "Epoch 116/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6601 - accuracy: 0.6250 - val_loss: 0.7096 - val_accuracy: 0.5517\n",
      "Epoch 117/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6596 - accuracy: 0.6192 - val_loss: 0.7014 - val_accuracy: 0.5517\n",
      "Epoch 118/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6574 - accuracy: 0.6385 - val_loss: 0.7037 - val_accuracy: 0.5690\n",
      "Epoch 119/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6583 - accuracy: 0.6308 - val_loss: 0.6950 - val_accuracy: 0.5862\n",
      "Epoch 120/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6569 - accuracy: 0.6288 - val_loss: 0.7028 - val_accuracy: 0.5690\n",
      "Epoch 121/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6575 - accuracy: 0.6231 - val_loss: 0.7020 - val_accuracy: 0.5690\n",
      "Epoch 122/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6548 - accuracy: 0.6308 - val_loss: 0.6970 - val_accuracy: 0.5862\n",
      "Epoch 123/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6538 - accuracy: 0.6365 - val_loss: 0.6997 - val_accuracy: 0.5690\n",
      "Epoch 124/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6529 - accuracy: 0.6365 - val_loss: 0.7047 - val_accuracy: 0.5690\n",
      "Epoch 125/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6521 - accuracy: 0.6327 - val_loss: 0.6891 - val_accuracy: 0.5862\n",
      "Epoch 126/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6531 - accuracy: 0.6288 - val_loss: 0.6991 - val_accuracy: 0.5862\n",
      "Epoch 127/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6500 - accuracy: 0.6385 - val_loss: 0.6962 - val_accuracy: 0.5690\n",
      "Epoch 128/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6510 - accuracy: 0.6346 - val_loss: 0.6983 - val_accuracy: 0.5862\n",
      "Epoch 129/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6505 - accuracy: 0.6385 - val_loss: 0.7123 - val_accuracy: 0.5690\n",
      "Epoch 130/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6498 - accuracy: 0.6404 - val_loss: 0.7021 - val_accuracy: 0.5862\n",
      "Epoch 131/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6483 - accuracy: 0.6423 - val_loss: 0.6924 - val_accuracy: 0.5862\n",
      "Epoch 132/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6480 - accuracy: 0.6385 - val_loss: 0.6978 - val_accuracy: 0.5862\n",
      "Epoch 133/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6477 - accuracy: 0.6423 - val_loss: 0.7028 - val_accuracy: 0.5862\n",
      "Epoch 134/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6461 - accuracy: 0.6462 - val_loss: 0.7034 - val_accuracy: 0.5690\n",
      "Epoch 135/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6457 - accuracy: 0.6365 - val_loss: 0.7025 - val_accuracy: 0.5862\n",
      "Epoch 136/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6473 - accuracy: 0.6346 - val_loss: 0.6968 - val_accuracy: 0.5862\n",
      "Epoch 137/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6435 - accuracy: 0.6423 - val_loss: 0.7109 - val_accuracy: 0.5862\n",
      "Epoch 138/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6436 - accuracy: 0.6519 - val_loss: 0.7003 - val_accuracy: 0.5862\n",
      "Epoch 139/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6431 - accuracy: 0.6500 - val_loss: 0.7063 - val_accuracy: 0.5862\n",
      "Epoch 140/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6436 - accuracy: 0.6442 - val_loss: 0.7064 - val_accuracy: 0.5862\n",
      "Epoch 141/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6428 - accuracy: 0.6423 - val_loss: 0.7143 - val_accuracy: 0.5690\n",
      "Epoch 142/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6411 - accuracy: 0.6538 - val_loss: 0.6988 - val_accuracy: 0.5862\n",
      "Epoch 143/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6390 - accuracy: 0.6519 - val_loss: 0.7018 - val_accuracy: 0.5862\n",
      "Epoch 144/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6416 - accuracy: 0.6500 - val_loss: 0.7126 - val_accuracy: 0.5862\n",
      "Epoch 145/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6381 - accuracy: 0.6558 - val_loss: 0.7097 - val_accuracy: 0.5862\n",
      "Epoch 146/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6386 - accuracy: 0.6519 - val_loss: 0.6988 - val_accuracy: 0.5862\n",
      "Epoch 147/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6382 - accuracy: 0.6615 - val_loss: 0.7169 - val_accuracy: 0.5862\n",
      "Epoch 148/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6373 - accuracy: 0.6538 - val_loss: 0.7034 - val_accuracy: 0.5862\n",
      "Epoch 149/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6358 - accuracy: 0.6577 - val_loss: 0.7235 - val_accuracy: 0.5690\n",
      "Epoch 150/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6348 - accuracy: 0.6558 - val_loss: 0.7161 - val_accuracy: 0.5862\n",
      "Epoch 151/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6332 - accuracy: 0.6538 - val_loss: 0.7130 - val_accuracy: 0.5690\n",
      "Epoch 152/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6310 - accuracy: 0.6635 - val_loss: 0.7123 - val_accuracy: 0.5690\n",
      "Epoch 153/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6334 - accuracy: 0.6538 - val_loss: 0.7205 - val_accuracy: 0.5690\n",
      "Epoch 154/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6307 - accuracy: 0.6615 - val_loss: 0.7250 - val_accuracy: 0.5345\n",
      "Epoch 155/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6327 - accuracy: 0.6712 - val_loss: 0.7182 - val_accuracy: 0.5690\n",
      "Epoch 156/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6303 - accuracy: 0.6596 - val_loss: 0.7167 - val_accuracy: 0.5690\n",
      "Epoch 157/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6292 - accuracy: 0.6635 - val_loss: 0.7214 - val_accuracy: 0.5690\n",
      "Epoch 158/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6265 - accuracy: 0.6731 - val_loss: 0.7290 - val_accuracy: 0.5690\n",
      "Epoch 159/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6279 - accuracy: 0.6654 - val_loss: 0.7335 - val_accuracy: 0.5517\n",
      "Epoch 160/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6287 - accuracy: 0.6654 - val_loss: 0.7129 - val_accuracy: 0.5690\n",
      "Epoch 161/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6285 - accuracy: 0.6615 - val_loss: 0.7328 - val_accuracy: 0.5517\n",
      "Epoch 162/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6270 - accuracy: 0.6635 - val_loss: 0.7128 - val_accuracy: 0.5690\n",
      "Epoch 163/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6262 - accuracy: 0.6712 - val_loss: 0.7090 - val_accuracy: 0.5690\n",
      "Epoch 164/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6264 - accuracy: 0.6596 - val_loss: 0.7111 - val_accuracy: 0.5690\n",
      "Epoch 165/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6253 - accuracy: 0.6692 - val_loss: 0.7183 - val_accuracy: 0.5517\n",
      "Epoch 166/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6274 - accuracy: 0.6635 - val_loss: 0.7177 - val_accuracy: 0.5690\n",
      "Epoch 167/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6216 - accuracy: 0.6673 - val_loss: 0.7276 - val_accuracy: 0.5517\n",
      "Epoch 168/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6223 - accuracy: 0.6654 - val_loss: 0.7161 - val_accuracy: 0.5690\n",
      "Epoch 169/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6215 - accuracy: 0.6654 - val_loss: 0.7188 - val_accuracy: 0.5517\n",
      "Epoch 170/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6242 - accuracy: 0.6635 - val_loss: 0.7206 - val_accuracy: 0.5517\n",
      "Epoch 171/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6233 - accuracy: 0.6712 - val_loss: 0.7182 - val_accuracy: 0.5690\n",
      "Epoch 172/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6195 - accuracy: 0.6731 - val_loss: 0.7273 - val_accuracy: 0.5690\n",
      "Epoch 173/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6214 - accuracy: 0.6692 - val_loss: 0.7244 - val_accuracy: 0.5517\n",
      "Epoch 174/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6201 - accuracy: 0.6692 - val_loss: 0.7254 - val_accuracy: 0.5690\n",
      "Epoch 175/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6200 - accuracy: 0.6654 - val_loss: 0.7253 - val_accuracy: 0.5690\n",
      "Epoch 176/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6177 - accuracy: 0.6712 - val_loss: 0.7314 - val_accuracy: 0.5517\n",
      "Epoch 177/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6171 - accuracy: 0.6788 - val_loss: 0.7195 - val_accuracy: 0.5862\n",
      "Epoch 178/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6194 - accuracy: 0.6712 - val_loss: 0.7169 - val_accuracy: 0.5862\n",
      "Epoch 179/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6155 - accuracy: 0.6865 - val_loss: 0.7464 - val_accuracy: 0.5690\n",
      "Epoch 180/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6191 - accuracy: 0.6808 - val_loss: 0.7427 - val_accuracy: 0.5690\n",
      "Epoch 181/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6192 - accuracy: 0.6865 - val_loss: 0.7213 - val_accuracy: 0.5862\n",
      "Epoch 182/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6189 - accuracy: 0.6827 - val_loss: 0.7492 - val_accuracy: 0.5345\n",
      "Epoch 183/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6182 - accuracy: 0.6788 - val_loss: 0.7336 - val_accuracy: 0.5862\n",
      "Epoch 184/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6171 - accuracy: 0.6788 - val_loss: 0.7243 - val_accuracy: 0.5862\n",
      "Epoch 185/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6138 - accuracy: 0.6846 - val_loss: 0.7148 - val_accuracy: 0.5862\n",
      "Epoch 186/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6125 - accuracy: 0.6942 - val_loss: 0.7179 - val_accuracy: 0.5862\n",
      "Epoch 187/2000\n",
      "520/520 [==============================] - 2s 3ms/step - loss: 0.6125 - accuracy: 0.6885 - val_loss: 0.7182 - val_accuracy: 0.5862\n",
      "Epoch 188/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6145 - accuracy: 0.6885 - val_loss: 0.7197 - val_accuracy: 0.5862\n",
      "Epoch 189/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6111 - accuracy: 0.6942 - val_loss: 0.7149 - val_accuracy: 0.5862\n",
      "Epoch 190/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6117 - accuracy: 0.6981 - val_loss: 0.7257 - val_accuracy: 0.5690\n",
      "Epoch 191/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6127 - accuracy: 0.6904 - val_loss: 0.7255 - val_accuracy: 0.5862\n",
      "Epoch 192/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6122 - accuracy: 0.6942 - val_loss: 0.7306 - val_accuracy: 0.5690\n",
      "Epoch 193/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6102 - accuracy: 0.6962 - val_loss: 0.7320 - val_accuracy: 0.5862\n",
      "Epoch 194/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6080 - accuracy: 0.6923 - val_loss: 0.7321 - val_accuracy: 0.5690\n",
      "Epoch 195/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6099 - accuracy: 0.6885 - val_loss: 0.7254 - val_accuracy: 0.6034\n",
      "Epoch 196/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6070 - accuracy: 0.6962 - val_loss: 0.7165 - val_accuracy: 0.6034\n",
      "Epoch 197/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6060 - accuracy: 0.6981 - val_loss: 0.7250 - val_accuracy: 0.6034\n",
      "Epoch 198/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6073 - accuracy: 0.7000 - val_loss: 0.7223 - val_accuracy: 0.6034\n",
      "Epoch 199/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6078 - accuracy: 0.6942 - val_loss: 0.7248 - val_accuracy: 0.6034\n",
      "Epoch 200/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.6032 - accuracy: 0.6981 - val_loss: 0.7477 - val_accuracy: 0.5862\n",
      "Epoch 201/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6059 - accuracy: 0.6942 - val_loss: 0.7344 - val_accuracy: 0.6034\n",
      "Epoch 202/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6144 - accuracy: 0.6904 - val_loss: 0.7160 - val_accuracy: 0.6034\n",
      "Epoch 203/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6050 - accuracy: 0.6962 - val_loss: 0.7175 - val_accuracy: 0.6034\n",
      "Epoch 204/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6014 - accuracy: 0.7000 - val_loss: 0.7183 - val_accuracy: 0.6034\n",
      "Epoch 205/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6009 - accuracy: 0.7019 - val_loss: 0.7093 - val_accuracy: 0.6034\n",
      "Epoch 206/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5999 - accuracy: 0.7038 - val_loss: 0.7168 - val_accuracy: 0.5862\n",
      "Epoch 207/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6012 - accuracy: 0.7058 - val_loss: 0.7438 - val_accuracy: 0.5862\n",
      "Epoch 208/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6023 - accuracy: 0.7000 - val_loss: 0.7133 - val_accuracy: 0.6034\n",
      "Epoch 209/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5981 - accuracy: 0.7077 - val_loss: 0.7191 - val_accuracy: 0.6034\n",
      "Epoch 210/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6004 - accuracy: 0.7077 - val_loss: 0.7265 - val_accuracy: 0.5862\n",
      "Epoch 211/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6005 - accuracy: 0.7000 - val_loss: 0.7050 - val_accuracy: 0.6034\n",
      "Epoch 212/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5967 - accuracy: 0.7173 - val_loss: 0.7366 - val_accuracy: 0.5690\n",
      "Epoch 213/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6009 - accuracy: 0.7077 - val_loss: 0.7577 - val_accuracy: 0.5862\n",
      "Epoch 214/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6008 - accuracy: 0.7038 - val_loss: 0.7047 - val_accuracy: 0.6207\n",
      "Epoch 215/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5976 - accuracy: 0.7058 - val_loss: 0.7159 - val_accuracy: 0.6207\n",
      "Epoch 216/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5977 - accuracy: 0.7038 - val_loss: 0.7344 - val_accuracy: 0.5690\n",
      "Epoch 217/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5995 - accuracy: 0.6981 - val_loss: 0.7122 - val_accuracy: 0.6034\n",
      "Epoch 218/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5959 - accuracy: 0.7019 - val_loss: 0.7572 - val_accuracy: 0.6034\n",
      "Epoch 219/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5981 - accuracy: 0.7019 - val_loss: 0.7255 - val_accuracy: 0.6034\n",
      "Epoch 220/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5945 - accuracy: 0.7115 - val_loss: 0.6977 - val_accuracy: 0.6207\n",
      "Epoch 221/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5971 - accuracy: 0.7096 - val_loss: 0.6974 - val_accuracy: 0.5690\n",
      "Epoch 222/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5923 - accuracy: 0.7192 - val_loss: 0.7536 - val_accuracy: 0.6034\n",
      "Epoch 223/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5995 - accuracy: 0.6962 - val_loss: 0.7413 - val_accuracy: 0.5690\n",
      "Epoch 224/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5914 - accuracy: 0.7077 - val_loss: 0.7479 - val_accuracy: 0.5862\n",
      "Epoch 225/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5936 - accuracy: 0.7077 - val_loss: 0.7109 - val_accuracy: 0.6207\n",
      "Epoch 226/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5911 - accuracy: 0.7135 - val_loss: 0.6917 - val_accuracy: 0.6034\n",
      "Epoch 227/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5932 - accuracy: 0.7019 - val_loss: 0.7241 - val_accuracy: 0.6034\n",
      "Epoch 228/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5874 - accuracy: 0.7135 - val_loss: 0.7582 - val_accuracy: 0.5862\n",
      "Epoch 229/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5940 - accuracy: 0.7058 - val_loss: 0.7306 - val_accuracy: 0.5690\n",
      "Epoch 230/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5861 - accuracy: 0.7058 - val_loss: 0.7525 - val_accuracy: 0.5862\n",
      "Epoch 231/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5915 - accuracy: 0.7096 - val_loss: 0.7782 - val_accuracy: 0.5862\n",
      "Epoch 232/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5880 - accuracy: 0.7135 - val_loss: 0.7318 - val_accuracy: 0.5862\n",
      "Epoch 233/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5882 - accuracy: 0.7058 - val_loss: 0.7112 - val_accuracy: 0.5862\n",
      "Epoch 234/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5852 - accuracy: 0.7135 - val_loss: 0.7611 - val_accuracy: 0.5862\n",
      "Epoch 235/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5906 - accuracy: 0.7077 - val_loss: 0.7650 - val_accuracy: 0.5690\n",
      "Epoch 236/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6033 - accuracy: 0.6962 - val_loss: 0.7630 - val_accuracy: 0.5690\n",
      "Epoch 237/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.6066 - accuracy: 0.6827 - val_loss: 0.7214 - val_accuracy: 0.6034\n",
      "Epoch 238/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5913 - accuracy: 0.6942 - val_loss: 0.7112 - val_accuracy: 0.5862\n",
      "Epoch 239/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5849 - accuracy: 0.7038 - val_loss: 0.7005 - val_accuracy: 0.5862\n",
      "Epoch 240/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5800 - accuracy: 0.7231 - val_loss: 0.7492 - val_accuracy: 0.5862\n",
      "Epoch 241/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5848 - accuracy: 0.7058 - val_loss: 0.7481 - val_accuracy: 0.5690\n",
      "Epoch 242/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5907 - accuracy: 0.6981 - val_loss: 0.7241 - val_accuracy: 0.5690\n",
      "Epoch 243/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5837 - accuracy: 0.7038 - val_loss: 0.7083 - val_accuracy: 0.5862\n",
      "Epoch 244/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5834 - accuracy: 0.7077 - val_loss: 0.7310 - val_accuracy: 0.5690\n",
      "Epoch 245/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5810 - accuracy: 0.7096 - val_loss: 0.7732 - val_accuracy: 0.5862\n",
      "Epoch 246/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5840 - accuracy: 0.7096 - val_loss: 0.7102 - val_accuracy: 0.5862\n",
      "Epoch 247/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5828 - accuracy: 0.7135 - val_loss: 0.6908 - val_accuracy: 0.6034\n",
      "Epoch 248/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5850 - accuracy: 0.7058 - val_loss: 0.6979 - val_accuracy: 0.6034\n",
      "Epoch 249/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5827 - accuracy: 0.7077 - val_loss: 0.7011 - val_accuracy: 0.5862\n",
      "Epoch 250/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5809 - accuracy: 0.7135 - val_loss: 0.7165 - val_accuracy: 0.5862\n",
      "Epoch 251/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5805 - accuracy: 0.7038 - val_loss: 0.7131 - val_accuracy: 0.5862\n",
      "Epoch 252/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5807 - accuracy: 0.7038 - val_loss: 0.7200 - val_accuracy: 0.5862\n",
      "Epoch 253/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5797 - accuracy: 0.7154 - val_loss: 0.7227 - val_accuracy: 0.6034\n",
      "Epoch 254/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5822 - accuracy: 0.7135 - val_loss: 0.7105 - val_accuracy: 0.5862\n",
      "Epoch 255/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5828 - accuracy: 0.7019 - val_loss: 0.7069 - val_accuracy: 0.5862\n",
      "Epoch 256/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5759 - accuracy: 0.7154 - val_loss: 0.7171 - val_accuracy: 0.6207\n",
      "Epoch 257/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5773 - accuracy: 0.7077 - val_loss: 0.7434 - val_accuracy: 0.5862\n",
      "Epoch 258/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5784 - accuracy: 0.7096 - val_loss: 0.7107 - val_accuracy: 0.5862\n",
      "Epoch 259/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5761 - accuracy: 0.7154 - val_loss: 0.7369 - val_accuracy: 0.5690\n",
      "Epoch 260/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5772 - accuracy: 0.7192 - val_loss: 0.7415 - val_accuracy: 0.6034\n",
      "Epoch 261/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5791 - accuracy: 0.7077 - val_loss: 0.7115 - val_accuracy: 0.6034\n",
      "Epoch 262/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5836 - accuracy: 0.7058 - val_loss: 0.7006 - val_accuracy: 0.6034\n",
      "Epoch 263/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5798 - accuracy: 0.7096 - val_loss: 0.7305 - val_accuracy: 0.5690\n",
      "Epoch 264/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5784 - accuracy: 0.7077 - val_loss: 0.7199 - val_accuracy: 0.5862\n",
      "Epoch 265/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5763 - accuracy: 0.7135 - val_loss: 0.7322 - val_accuracy: 0.5690\n",
      "Epoch 266/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5780 - accuracy: 0.7135 - val_loss: 0.7294 - val_accuracy: 0.6034\n",
      "Epoch 267/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5740 - accuracy: 0.7135 - val_loss: 0.7201 - val_accuracy: 0.5862\n",
      "Epoch 268/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5774 - accuracy: 0.7038 - val_loss: 0.7084 - val_accuracy: 0.5862\n",
      "Epoch 269/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5698 - accuracy: 0.7192 - val_loss: 0.7272 - val_accuracy: 0.5690\n",
      "Epoch 270/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5775 - accuracy: 0.7115 - val_loss: 0.7128 - val_accuracy: 0.5862\n",
      "Epoch 271/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5721 - accuracy: 0.7192 - val_loss: 0.7257 - val_accuracy: 0.5862\n",
      "Epoch 272/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5741 - accuracy: 0.7038 - val_loss: 0.7269 - val_accuracy: 0.5862\n",
      "Epoch 273/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5712 - accuracy: 0.7154 - val_loss: 0.7522 - val_accuracy: 0.6034\n",
      "Epoch 274/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5835 - accuracy: 0.7115 - val_loss: 0.7484 - val_accuracy: 0.5862\n",
      "Epoch 275/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5734 - accuracy: 0.7135 - val_loss: 0.7480 - val_accuracy: 0.5690\n",
      "Epoch 276/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5703 - accuracy: 0.7135 - val_loss: 0.7211 - val_accuracy: 0.5862\n",
      "Epoch 277/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5734 - accuracy: 0.7135 - val_loss: 0.7200 - val_accuracy: 0.5862\n",
      "Epoch 278/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5708 - accuracy: 0.7231 - val_loss: 0.7184 - val_accuracy: 0.5862\n",
      "Epoch 279/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5751 - accuracy: 0.7077 - val_loss: 0.7302 - val_accuracy: 0.5690\n",
      "Epoch 280/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5694 - accuracy: 0.7212 - val_loss: 0.7559 - val_accuracy: 0.5862\n",
      "Epoch 281/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5675 - accuracy: 0.7135 - val_loss: 0.7345 - val_accuracy: 0.6034\n",
      "Epoch 282/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5701 - accuracy: 0.7154 - val_loss: 0.7698 - val_accuracy: 0.5862\n",
      "Epoch 283/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5713 - accuracy: 0.7173 - val_loss: 0.7563 - val_accuracy: 0.5690\n",
      "Epoch 284/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5679 - accuracy: 0.7173 - val_loss: 0.7155 - val_accuracy: 0.5690\n",
      "Epoch 285/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5686 - accuracy: 0.7135 - val_loss: 0.7054 - val_accuracy: 0.5862\n",
      "Epoch 286/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5671 - accuracy: 0.7115 - val_loss: 0.7118 - val_accuracy: 0.5690\n",
      "Epoch 287/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5711 - accuracy: 0.7135 - val_loss: 0.7074 - val_accuracy: 0.5862\n",
      "Epoch 288/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5681 - accuracy: 0.7154 - val_loss: 0.7161 - val_accuracy: 0.5862\n",
      "Epoch 289/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5720 - accuracy: 0.7135 - val_loss: 0.7044 - val_accuracy: 0.5690\n",
      "Epoch 290/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5707 - accuracy: 0.7058 - val_loss: 0.7331 - val_accuracy: 0.5862\n",
      "Epoch 291/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5677 - accuracy: 0.7135 - val_loss: 0.7195 - val_accuracy: 0.5862\n",
      "Epoch 292/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5656 - accuracy: 0.7135 - val_loss: 0.7000 - val_accuracy: 0.6034\n",
      "Epoch 293/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5640 - accuracy: 0.7231 - val_loss: 0.7216 - val_accuracy: 0.5862\n",
      "Epoch 294/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7192 - val_loss: 0.6893 - val_accuracy: 0.5862\n",
      "Epoch 295/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5645 - accuracy: 0.7115 - val_loss: 0.7210 - val_accuracy: 0.5862\n",
      "Epoch 296/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5643 - accuracy: 0.7173 - val_loss: 0.7212 - val_accuracy: 0.5862\n",
      "Epoch 297/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5634 - accuracy: 0.7154 - val_loss: 0.7378 - val_accuracy: 0.5862\n",
      "Epoch 298/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5695 - accuracy: 0.7154 - val_loss: 0.7338 - val_accuracy: 0.5862\n",
      "Epoch 299/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5662 - accuracy: 0.7154 - val_loss: 0.7297 - val_accuracy: 0.5862\n",
      "Epoch 300/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5642 - accuracy: 0.7154 - val_loss: 0.7111 - val_accuracy: 0.6034\n",
      "Epoch 301/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5623 - accuracy: 0.7154 - val_loss: 0.7164 - val_accuracy: 0.6034\n",
      "Epoch 302/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5607 - accuracy: 0.7115 - val_loss: 0.7158 - val_accuracy: 0.6034\n",
      "Epoch 303/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5630 - accuracy: 0.7192 - val_loss: 0.7123 - val_accuracy: 0.6034\n",
      "Epoch 304/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5711 - accuracy: 0.7058 - val_loss: 0.7077 - val_accuracy: 0.6034\n",
      "Epoch 305/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5609 - accuracy: 0.7212 - val_loss: 0.7018 - val_accuracy: 0.5862\n",
      "Epoch 306/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5627 - accuracy: 0.7115 - val_loss: 0.6845 - val_accuracy: 0.6034\n",
      "Epoch 307/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5612 - accuracy: 0.7173 - val_loss: 0.7199 - val_accuracy: 0.5862\n",
      "Epoch 308/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.7135 - val_loss: 0.7135 - val_accuracy: 0.5862\n",
      "Epoch 309/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5584 - accuracy: 0.7077 - val_loss: 0.7385 - val_accuracy: 0.5862\n",
      "Epoch 310/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5586 - accuracy: 0.7192 - val_loss: 0.7248 - val_accuracy: 0.5862\n",
      "Epoch 311/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5568 - accuracy: 0.7173 - val_loss: 0.7339 - val_accuracy: 0.5862\n",
      "Epoch 312/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5582 - accuracy: 0.7173 - val_loss: 0.7159 - val_accuracy: 0.6034\n",
      "Epoch 313/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5567 - accuracy: 0.7096 - val_loss: 0.7643 - val_accuracy: 0.5690\n",
      "Epoch 314/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5548 - accuracy: 0.7192 - val_loss: 0.7223 - val_accuracy: 0.6034\n",
      "Epoch 315/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5683 - accuracy: 0.7038 - val_loss: 0.7190 - val_accuracy: 0.6034\n",
      "Epoch 316/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5647 - accuracy: 0.7096 - val_loss: 0.7166 - val_accuracy: 0.5862\n",
      "Epoch 317/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5552 - accuracy: 0.7212 - val_loss: 0.7206 - val_accuracy: 0.5690\n",
      "Epoch 318/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5712 - accuracy: 0.7019 - val_loss: 0.7135 - val_accuracy: 0.5690\n",
      "Epoch 319/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5538 - accuracy: 0.7135 - val_loss: 0.7505 - val_accuracy: 0.5690\n",
      "Epoch 320/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5514 - accuracy: 0.7115 - val_loss: 0.7214 - val_accuracy: 0.6034\n",
      "Epoch 321/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5551 - accuracy: 0.7250 - val_loss: 0.7221 - val_accuracy: 0.5862\n",
      "Epoch 322/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5535 - accuracy: 0.7231 - val_loss: 0.7352 - val_accuracy: 0.5862\n",
      "Epoch 323/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5557 - accuracy: 0.7135 - val_loss: 0.7172 - val_accuracy: 0.5862\n",
      "Epoch 324/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5560 - accuracy: 0.7115 - val_loss: 0.7127 - val_accuracy: 0.5690\n",
      "Epoch 325/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5508 - accuracy: 0.7231 - val_loss: 0.7363 - val_accuracy: 0.5862\n",
      "Epoch 326/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5509 - accuracy: 0.7173 - val_loss: 0.7652 - val_accuracy: 0.5517\n",
      "Epoch 327/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5505 - accuracy: 0.7077 - val_loss: 0.7356 - val_accuracy: 0.5862\n",
      "Epoch 328/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5736 - accuracy: 0.7058 - val_loss: 0.7475 - val_accuracy: 0.5862\n",
      "Epoch 329/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5545 - accuracy: 0.7096 - val_loss: 0.7173 - val_accuracy: 0.5690\n",
      "Epoch 330/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5507 - accuracy: 0.7019 - val_loss: 0.7355 - val_accuracy: 0.5517\n",
      "Epoch 331/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5572 - accuracy: 0.7096 - val_loss: 0.7007 - val_accuracy: 0.5862\n",
      "Epoch 332/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5501 - accuracy: 0.7173 - val_loss: 0.7136 - val_accuracy: 0.5690\n",
      "Epoch 333/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5455 - accuracy: 0.7154 - val_loss: 0.7707 - val_accuracy: 0.5690\n",
      "Epoch 334/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5458 - accuracy: 0.7250 - val_loss: 0.7730 - val_accuracy: 0.5862\n",
      "Epoch 335/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5474 - accuracy: 0.7135 - val_loss: 0.7396 - val_accuracy: 0.5690\n",
      "Epoch 336/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5462 - accuracy: 0.7135 - val_loss: 0.7490 - val_accuracy: 0.5517\n",
      "Epoch 337/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5440 - accuracy: 0.7058 - val_loss: 0.7254 - val_accuracy: 0.6034\n",
      "Epoch 338/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5443 - accuracy: 0.7096 - val_loss: 0.7041 - val_accuracy: 0.5690\n",
      "Epoch 339/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5418 - accuracy: 0.7077 - val_loss: 0.7713 - val_accuracy: 0.5862\n",
      "Epoch 340/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5778 - accuracy: 0.6904 - val_loss: 0.7872 - val_accuracy: 0.5690\n",
      "Epoch 341/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5733 - accuracy: 0.7038 - val_loss: 0.7361 - val_accuracy: 0.5517\n",
      "Epoch 342/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5504 - accuracy: 0.7038 - val_loss: 0.7572 - val_accuracy: 0.5517\n",
      "Epoch 343/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5416 - accuracy: 0.7154 - val_loss: 0.7906 - val_accuracy: 0.5690\n",
      "Epoch 344/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5415 - accuracy: 0.7192 - val_loss: 0.7544 - val_accuracy: 0.5517\n",
      "Epoch 345/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5399 - accuracy: 0.7192 - val_loss: 0.7524 - val_accuracy: 0.5690\n",
      "Epoch 346/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5392 - accuracy: 0.7135 - val_loss: 0.7533 - val_accuracy: 0.5862\n",
      "Epoch 347/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5399 - accuracy: 0.7096 - val_loss: 0.7341 - val_accuracy: 0.5517\n",
      "Epoch 348/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5420 - accuracy: 0.7115 - val_loss: 0.7084 - val_accuracy: 0.5690\n",
      "Epoch 349/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5415 - accuracy: 0.7115 - val_loss: 0.7998 - val_accuracy: 0.5862\n",
      "Epoch 350/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5417 - accuracy: 0.7077 - val_loss: 0.7085 - val_accuracy: 0.6207\n",
      "Epoch 351/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7173 - val_loss: 0.7097 - val_accuracy: 0.5690\n",
      "Epoch 352/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5355 - accuracy: 0.7269 - val_loss: 0.7062 - val_accuracy: 0.5690\n",
      "Epoch 353/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5393 - accuracy: 0.7154 - val_loss: 0.6923 - val_accuracy: 0.5862\n",
      "Epoch 354/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5384 - accuracy: 0.7192 - val_loss: 0.7353 - val_accuracy: 0.5862\n",
      "Epoch 355/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5368 - accuracy: 0.7154 - val_loss: 0.7166 - val_accuracy: 0.5862\n",
      "Epoch 356/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5498 - accuracy: 0.7115 - val_loss: 0.7462 - val_accuracy: 0.5690\n",
      "Epoch 357/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5339 - accuracy: 0.7231 - val_loss: 0.7402 - val_accuracy: 0.5862\n",
      "Epoch 358/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5337 - accuracy: 0.7154 - val_loss: 0.7243 - val_accuracy: 0.5862\n",
      "Epoch 359/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5433 - accuracy: 0.7154 - val_loss: 0.7132 - val_accuracy: 0.5690\n",
      "Epoch 360/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5340 - accuracy: 0.7038 - val_loss: 0.7195 - val_accuracy: 0.5690\n",
      "Epoch 361/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5361 - accuracy: 0.7192 - val_loss: 0.7218 - val_accuracy: 0.5690\n",
      "Epoch 362/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5380 - accuracy: 0.7135 - val_loss: 0.7176 - val_accuracy: 0.5690\n",
      "Epoch 363/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5353 - accuracy: 0.7115 - val_loss: 0.7072 - val_accuracy: 0.5862\n",
      "Epoch 364/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7115 - val_loss: 0.7543 - val_accuracy: 0.5517\n",
      "Epoch 365/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7154 - val_loss: 0.7322 - val_accuracy: 0.6034\n",
      "Epoch 366/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5301 - accuracy: 0.7192 - val_loss: 0.7606 - val_accuracy: 0.6034\n",
      "Epoch 367/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5587 - accuracy: 0.7019 - val_loss: 0.6852 - val_accuracy: 0.6034\n",
      "Epoch 368/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5431 - accuracy: 0.7096 - val_loss: 0.7008 - val_accuracy: 0.6034\n",
      "Epoch 369/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5340 - accuracy: 0.7192 - val_loss: 0.7435 - val_accuracy: 0.5862\n",
      "Epoch 370/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5325 - accuracy: 0.7154 - val_loss: 0.7604 - val_accuracy: 0.5690\n",
      "Epoch 371/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5283 - accuracy: 0.7192 - val_loss: 0.7817 - val_accuracy: 0.5517\n",
      "Epoch 372/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5306 - accuracy: 0.7192 - val_loss: 0.7370 - val_accuracy: 0.5690\n",
      "Epoch 373/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5299 - accuracy: 0.7173 - val_loss: 0.7180 - val_accuracy: 0.5690\n",
      "Epoch 374/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5291 - accuracy: 0.7250 - val_loss: 0.7286 - val_accuracy: 0.5690\n",
      "Epoch 375/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5261 - accuracy: 0.7154 - val_loss: 0.7307 - val_accuracy: 0.5862\n",
      "Epoch 376/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5341 - accuracy: 0.7173 - val_loss: 0.7185 - val_accuracy: 0.6034\n",
      "Epoch 377/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5249 - accuracy: 0.7154 - val_loss: 0.7260 - val_accuracy: 0.5690\n",
      "Epoch 378/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5280 - accuracy: 0.7154 - val_loss: 0.7109 - val_accuracy: 0.5862\n",
      "Epoch 379/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5295 - accuracy: 0.7058 - val_loss: 0.7420 - val_accuracy: 0.5862\n",
      "Epoch 380/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5401 - accuracy: 0.7058 - val_loss: 0.7256 - val_accuracy: 0.5690\n",
      "Epoch 381/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5268 - accuracy: 0.7269 - val_loss: 0.7050 - val_accuracy: 0.5862\n",
      "Epoch 382/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5243 - accuracy: 0.7308 - val_loss: 0.7316 - val_accuracy: 0.6034\n",
      "Epoch 383/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5269 - accuracy: 0.7212 - val_loss: 0.7030 - val_accuracy: 0.5862\n",
      "Epoch 384/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5226 - accuracy: 0.7288 - val_loss: 0.7416 - val_accuracy: 0.6034\n",
      "Epoch 385/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5243 - accuracy: 0.7269 - val_loss: 0.7338 - val_accuracy: 0.6207\n",
      "Epoch 386/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5213 - accuracy: 0.7250 - val_loss: 0.7018 - val_accuracy: 0.6034\n",
      "Epoch 387/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5315 - accuracy: 0.7038 - val_loss: 0.7366 - val_accuracy: 0.6207\n",
      "Epoch 388/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5203 - accuracy: 0.7173 - val_loss: 0.7753 - val_accuracy: 0.6034\n",
      "Epoch 389/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5245 - accuracy: 0.7173 - val_loss: 0.7211 - val_accuracy: 0.6034\n",
      "Epoch 390/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5238 - accuracy: 0.7154 - val_loss: 0.7104 - val_accuracy: 0.5862\n",
      "Epoch 391/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5231 - accuracy: 0.7231 - val_loss: 0.7595 - val_accuracy: 0.5862\n",
      "Epoch 392/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5204 - accuracy: 0.7212 - val_loss: 0.7660 - val_accuracy: 0.5517\n",
      "Epoch 393/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5182 - accuracy: 0.7231 - val_loss: 0.7120 - val_accuracy: 0.5862\n",
      "Epoch 394/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5237 - accuracy: 0.7269 - val_loss: 0.7043 - val_accuracy: 0.5862\n",
      "Epoch 395/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5207 - accuracy: 0.7288 - val_loss: 0.7869 - val_accuracy: 0.5517\n",
      "Epoch 396/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5290 - accuracy: 0.7135 - val_loss: 0.7165 - val_accuracy: 0.5690\n",
      "Epoch 397/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5220 - accuracy: 0.7288 - val_loss: 0.7168 - val_accuracy: 0.5862\n",
      "Epoch 398/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5222 - accuracy: 0.7212 - val_loss: 0.7198 - val_accuracy: 0.6034\n",
      "Epoch 399/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5174 - accuracy: 0.7308 - val_loss: 0.7378 - val_accuracy: 0.6034\n",
      "Epoch 400/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5215 - accuracy: 0.7173 - val_loss: 0.7334 - val_accuracy: 0.5862\n",
      "Epoch 401/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5191 - accuracy: 0.7250 - val_loss: 0.7256 - val_accuracy: 0.6207\n",
      "Epoch 402/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5182 - accuracy: 0.7173 - val_loss: 0.7106 - val_accuracy: 0.5690\n",
      "Epoch 403/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5266 - accuracy: 0.7173 - val_loss: 0.7724 - val_accuracy: 0.5690\n",
      "Epoch 404/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5220 - accuracy: 0.7231 - val_loss: 0.7094 - val_accuracy: 0.6034\n",
      "Epoch 405/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5173 - accuracy: 0.7288 - val_loss: 0.7289 - val_accuracy: 0.6034\n",
      "Epoch 406/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5170 - accuracy: 0.7154 - val_loss: 0.7541 - val_accuracy: 0.5690\n",
      "Epoch 407/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5150 - accuracy: 0.7269 - val_loss: 0.7118 - val_accuracy: 0.6034\n",
      "Epoch 408/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5154 - accuracy: 0.7308 - val_loss: 0.7341 - val_accuracy: 0.5690\n",
      "Epoch 409/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5203 - accuracy: 0.7173 - val_loss: 0.7263 - val_accuracy: 0.5690\n",
      "Epoch 410/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5134 - accuracy: 0.7346 - val_loss: 0.7202 - val_accuracy: 0.5690\n",
      "Epoch 411/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5280 - accuracy: 0.7288 - val_loss: 0.7298 - val_accuracy: 0.6034\n",
      "Epoch 412/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5171 - accuracy: 0.7288 - val_loss: 0.7536 - val_accuracy: 0.5862\n",
      "Epoch 413/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5180 - accuracy: 0.7288 - val_loss: 0.7159 - val_accuracy: 0.6034\n",
      "Epoch 414/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5102 - accuracy: 0.7269 - val_loss: 0.7040 - val_accuracy: 0.6207\n",
      "Epoch 415/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5152 - accuracy: 0.7231 - val_loss: 0.7389 - val_accuracy: 0.5862\n",
      "Epoch 416/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5102 - accuracy: 0.7269 - val_loss: 0.7182 - val_accuracy: 0.5690\n",
      "Epoch 417/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5135 - accuracy: 0.7269 - val_loss: 0.7141 - val_accuracy: 0.5862\n",
      "Epoch 418/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5114 - accuracy: 0.7250 - val_loss: 0.7316 - val_accuracy: 0.5862\n",
      "Epoch 419/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5133 - accuracy: 0.7308 - val_loss: 0.7044 - val_accuracy: 0.5862\n",
      "Epoch 420/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5125 - accuracy: 0.7308 - val_loss: 0.7142 - val_accuracy: 0.5862\n",
      "Epoch 421/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5146 - accuracy: 0.7269 - val_loss: 0.7128 - val_accuracy: 0.5862\n",
      "Epoch 422/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5267 - accuracy: 0.7250 - val_loss: 0.6925 - val_accuracy: 0.6207\n",
      "Epoch 423/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5252 - accuracy: 0.7173 - val_loss: 0.7269 - val_accuracy: 0.5862\n",
      "Epoch 424/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5148 - accuracy: 0.7327 - val_loss: 0.7008 - val_accuracy: 0.6552\n",
      "Epoch 425/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5105 - accuracy: 0.7269 - val_loss: 0.7114 - val_accuracy: 0.5862\n",
      "Epoch 426/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5101 - accuracy: 0.7231 - val_loss: 0.7608 - val_accuracy: 0.5862\n",
      "Epoch 427/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5110 - accuracy: 0.7269 - val_loss: 0.7053 - val_accuracy: 0.6207\n",
      "Epoch 428/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5079 - accuracy: 0.7308 - val_loss: 0.7433 - val_accuracy: 0.6034\n",
      "Epoch 429/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5094 - accuracy: 0.7288 - val_loss: 0.7161 - val_accuracy: 0.6207\n",
      "Epoch 430/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5075 - accuracy: 0.7269 - val_loss: 0.7745 - val_accuracy: 0.5862\n",
      "Epoch 431/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5111 - accuracy: 0.7192 - val_loss: 0.8533 - val_accuracy: 0.5517\n",
      "Epoch 432/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5142 - accuracy: 0.7173 - val_loss: 0.7276 - val_accuracy: 0.5862\n",
      "Epoch 433/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5110 - accuracy: 0.7346 - val_loss: 0.7433 - val_accuracy: 0.6034\n",
      "Epoch 434/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5069 - accuracy: 0.7192 - val_loss: 0.7283 - val_accuracy: 0.6034\n",
      "Epoch 435/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5073 - accuracy: 0.7212 - val_loss: 0.7090 - val_accuracy: 0.6207\n",
      "Epoch 436/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5056 - accuracy: 0.7404 - val_loss: 0.7398 - val_accuracy: 0.5862\n",
      "Epoch 437/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5193 - accuracy: 0.7327 - val_loss: 0.6748 - val_accuracy: 0.6724\n",
      "Epoch 438/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5165 - accuracy: 0.7288 - val_loss: 0.7638 - val_accuracy: 0.6034\n",
      "Epoch 439/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5256 - accuracy: 0.7288 - val_loss: 0.7428 - val_accuracy: 0.6034\n",
      "Epoch 440/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5100 - accuracy: 0.7212 - val_loss: 0.7148 - val_accuracy: 0.6207\n",
      "Epoch 441/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5035 - accuracy: 0.7327 - val_loss: 0.7485 - val_accuracy: 0.6034\n",
      "Epoch 442/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5034 - accuracy: 0.7327 - val_loss: 0.7149 - val_accuracy: 0.6379\n",
      "Epoch 443/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5162 - accuracy: 0.7346 - val_loss: 0.7355 - val_accuracy: 0.6552\n",
      "Epoch 444/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5132 - accuracy: 0.7269 - val_loss: 0.7144 - val_accuracy: 0.6207\n",
      "Epoch 445/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5055 - accuracy: 0.7346 - val_loss: 0.7678 - val_accuracy: 0.5690\n",
      "Epoch 446/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5042 - accuracy: 0.7365 - val_loss: 0.7476 - val_accuracy: 0.5690\n",
      "Epoch 447/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5066 - accuracy: 0.7250 - val_loss: 0.7343 - val_accuracy: 0.6034\n",
      "Epoch 448/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5041 - accuracy: 0.7385 - val_loss: 0.7425 - val_accuracy: 0.5690\n",
      "Epoch 449/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5077 - accuracy: 0.7327 - val_loss: 0.7151 - val_accuracy: 0.6379\n",
      "Epoch 450/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5051 - accuracy: 0.7231 - val_loss: 0.7188 - val_accuracy: 0.6207\n",
      "Epoch 451/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5002 - accuracy: 0.7423 - val_loss: 0.7289 - val_accuracy: 0.6207\n",
      "Epoch 452/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5030 - accuracy: 0.7442 - val_loss: 0.7161 - val_accuracy: 0.6207\n",
      "Epoch 453/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5024 - accuracy: 0.7308 - val_loss: 0.7402 - val_accuracy: 0.5862\n",
      "Epoch 454/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5024 - accuracy: 0.7346 - val_loss: 0.6982 - val_accuracy: 0.6379\n",
      "Epoch 455/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5075 - accuracy: 0.7231 - val_loss: 0.7467 - val_accuracy: 0.6207\n",
      "Epoch 456/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5010 - accuracy: 0.7327 - val_loss: 0.7364 - val_accuracy: 0.5862\n",
      "Epoch 457/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5014 - accuracy: 0.7327 - val_loss: 0.7188 - val_accuracy: 0.6034\n",
      "Epoch 458/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5010 - accuracy: 0.7365 - val_loss: 0.7248 - val_accuracy: 0.6207\n",
      "Epoch 459/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5095 - accuracy: 0.7404 - val_loss: 0.8599 - val_accuracy: 0.5690\n",
      "Epoch 460/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5283 - accuracy: 0.7115 - val_loss: 0.7137 - val_accuracy: 0.6034\n",
      "Epoch 461/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5092 - accuracy: 0.7250 - val_loss: 0.7258 - val_accuracy: 0.6379\n",
      "Epoch 462/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5052 - accuracy: 0.7212 - val_loss: 0.7447 - val_accuracy: 0.6034\n",
      "Epoch 463/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4976 - accuracy: 0.7462 - val_loss: 0.7210 - val_accuracy: 0.6379\n",
      "Epoch 464/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4992 - accuracy: 0.7327 - val_loss: 0.7360 - val_accuracy: 0.6034\n",
      "Epoch 465/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5100 - accuracy: 0.7288 - val_loss: 0.7001 - val_accuracy: 0.6207\n",
      "Epoch 466/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4982 - accuracy: 0.7385 - val_loss: 0.7355 - val_accuracy: 0.6207\n",
      "Epoch 467/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5003 - accuracy: 0.7308 - val_loss: 0.6975 - val_accuracy: 0.6552\n",
      "Epoch 468/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4960 - accuracy: 0.7404 - val_loss: 0.7240 - val_accuracy: 0.6207\n",
      "Epoch 469/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5013 - accuracy: 0.7288 - val_loss: 0.7264 - val_accuracy: 0.6034\n",
      "Epoch 470/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5076 - accuracy: 0.7327 - val_loss: 0.7413 - val_accuracy: 0.5862\n",
      "Epoch 471/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5100 - accuracy: 0.7288 - val_loss: 0.7197 - val_accuracy: 0.6552\n",
      "Epoch 472/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4997 - accuracy: 0.7385 - val_loss: 0.7584 - val_accuracy: 0.6207\n",
      "Epoch 473/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4980 - accuracy: 0.7404 - val_loss: 0.7283 - val_accuracy: 0.6207\n",
      "Epoch 474/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4953 - accuracy: 0.7442 - val_loss: 0.7806 - val_accuracy: 0.6034\n",
      "Epoch 475/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4960 - accuracy: 0.7308 - val_loss: 0.7237 - val_accuracy: 0.6379\n",
      "Epoch 476/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4994 - accuracy: 0.7404 - val_loss: 0.7267 - val_accuracy: 0.6207\n",
      "Epoch 477/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4948 - accuracy: 0.7404 - val_loss: 0.7325 - val_accuracy: 0.6379\n",
      "Epoch 478/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5023 - accuracy: 0.7269 - val_loss: 0.7138 - val_accuracy: 0.6552\n",
      "Epoch 479/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5039 - accuracy: 0.7327 - val_loss: 0.8354 - val_accuracy: 0.5690\n",
      "Epoch 480/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4973 - accuracy: 0.7288 - val_loss: 0.7612 - val_accuracy: 0.5862\n",
      "Epoch 481/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4996 - accuracy: 0.7269 - val_loss: 0.7247 - val_accuracy: 0.6379\n",
      "Epoch 482/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4990 - accuracy: 0.7269 - val_loss: 0.7294 - val_accuracy: 0.6207\n",
      "Epoch 483/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5070 - accuracy: 0.7346 - val_loss: 0.7297 - val_accuracy: 0.6379\n",
      "Epoch 484/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4967 - accuracy: 0.7327 - val_loss: 0.7299 - val_accuracy: 0.6379\n",
      "Epoch 485/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4966 - accuracy: 0.7404 - val_loss: 0.7339 - val_accuracy: 0.6379\n",
      "Epoch 486/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5028 - accuracy: 0.7423 - val_loss: 0.7088 - val_accuracy: 0.6207\n",
      "Epoch 487/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4955 - accuracy: 0.7462 - val_loss: 0.7293 - val_accuracy: 0.6034\n",
      "Epoch 488/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5008 - accuracy: 0.7346 - val_loss: 0.7768 - val_accuracy: 0.5690\n",
      "Epoch 489/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4964 - accuracy: 0.7462 - val_loss: 0.7904 - val_accuracy: 0.5862\n",
      "Epoch 490/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4940 - accuracy: 0.7462 - val_loss: 0.7357 - val_accuracy: 0.6207\n",
      "Epoch 491/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4945 - accuracy: 0.7442 - val_loss: 0.7248 - val_accuracy: 0.6552\n",
      "Epoch 492/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5199 - accuracy: 0.7192 - val_loss: 0.7569 - val_accuracy: 0.6379\n",
      "Epoch 493/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.5117 - accuracy: 0.7327 - val_loss: 0.7612 - val_accuracy: 0.6207\n",
      "Epoch 494/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4960 - accuracy: 0.7308 - val_loss: 0.7777 - val_accuracy: 0.5862\n",
      "Epoch 495/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4896 - accuracy: 0.7365 - val_loss: 0.7461 - val_accuracy: 0.6207\n",
      "Epoch 496/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4921 - accuracy: 0.7385 - val_loss: 0.7221 - val_accuracy: 0.6207\n",
      "Epoch 497/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4915 - accuracy: 0.7481 - val_loss: 0.7650 - val_accuracy: 0.6034\n",
      "Epoch 498/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4935 - accuracy: 0.7327 - val_loss: 0.7653 - val_accuracy: 0.5862\n",
      "Epoch 499/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4878 - accuracy: 0.7346 - val_loss: 0.7522 - val_accuracy: 0.6379\n",
      "Epoch 500/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4928 - accuracy: 0.7385 - val_loss: 0.7187 - val_accuracy: 0.6379\n",
      "Epoch 501/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4970 - accuracy: 0.7288 - val_loss: 0.7711 - val_accuracy: 0.5862\n",
      "Epoch 502/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4907 - accuracy: 0.7308 - val_loss: 0.7904 - val_accuracy: 0.5862\n",
      "Epoch 503/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4954 - accuracy: 0.7442 - val_loss: 0.7536 - val_accuracy: 0.6034\n",
      "Epoch 504/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4909 - accuracy: 0.7346 - val_loss: 0.8337 - val_accuracy: 0.5690\n",
      "Epoch 505/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4965 - accuracy: 0.7462 - val_loss: 0.7983 - val_accuracy: 0.6034\n",
      "Epoch 506/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4924 - accuracy: 0.7231 - val_loss: 0.7815 - val_accuracy: 0.5000\n",
      "Epoch 507/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4915 - accuracy: 0.7365 - val_loss: 0.7584 - val_accuracy: 0.5172\n",
      "Epoch 508/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4942 - accuracy: 0.7327 - val_loss: 0.7626 - val_accuracy: 0.6034\n",
      "Epoch 509/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4891 - accuracy: 0.7135 - val_loss: 0.7192 - val_accuracy: 0.6552\n",
      "Epoch 510/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4915 - accuracy: 0.7154 - val_loss: 0.7367 - val_accuracy: 0.6207\n",
      "Epoch 511/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4917 - accuracy: 0.7288 - val_loss: 0.7465 - val_accuracy: 0.6379\n",
      "Epoch 512/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5048 - accuracy: 0.7173 - val_loss: 0.8225 - val_accuracy: 0.5517\n",
      "Epoch 513/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5151 - accuracy: 0.7365 - val_loss: 0.7257 - val_accuracy: 0.5345\n",
      "Epoch 514/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4976 - accuracy: 0.7308 - val_loss: 0.6927 - val_accuracy: 0.6552\n",
      "Epoch 515/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4977 - accuracy: 0.7327 - val_loss: 0.7025 - val_accuracy: 0.6552\n",
      "Epoch 516/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4963 - accuracy: 0.7404 - val_loss: 0.7670 - val_accuracy: 0.6207\n",
      "Epoch 517/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4973 - accuracy: 0.7154 - val_loss: 0.7182 - val_accuracy: 0.5517\n",
      "Epoch 518/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4972 - accuracy: 0.7173 - val_loss: 0.7729 - val_accuracy: 0.5172\n",
      "Epoch 519/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4984 - accuracy: 0.7423 - val_loss: 0.7329 - val_accuracy: 0.5172\n",
      "Epoch 520/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4941 - accuracy: 0.7538 - val_loss: 0.7217 - val_accuracy: 0.5517\n",
      "Epoch 521/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5065 - accuracy: 0.7327 - val_loss: 0.7765 - val_accuracy: 0.5345\n",
      "Epoch 522/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4939 - accuracy: 0.7462 - val_loss: 0.7775 - val_accuracy: 0.5172\n",
      "Epoch 523/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4930 - accuracy: 0.7423 - val_loss: 0.7491 - val_accuracy: 0.5345\n",
      "Epoch 524/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4986 - accuracy: 0.7442 - val_loss: 0.7560 - val_accuracy: 0.5345\n",
      "Epoch 525/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4953 - accuracy: 0.7404 - val_loss: 0.8017 - val_accuracy: 0.5345\n",
      "Epoch 526/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4972 - accuracy: 0.7365 - val_loss: 0.7759 - val_accuracy: 0.5345\n",
      "Epoch 527/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4913 - accuracy: 0.7385 - val_loss: 0.8426 - val_accuracy: 0.5000\n",
      "Epoch 528/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5019 - accuracy: 0.7346 - val_loss: 0.7203 - val_accuracy: 0.5862\n",
      "Epoch 529/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4950 - accuracy: 0.7423 - val_loss: 0.7402 - val_accuracy: 0.5517\n",
      "Epoch 530/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4927 - accuracy: 0.7385 - val_loss: 0.7472 - val_accuracy: 0.5345\n",
      "Epoch 531/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4915 - accuracy: 0.7500 - val_loss: 0.7170 - val_accuracy: 0.5690\n",
      "Epoch 532/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4994 - accuracy: 0.7327 - val_loss: 0.6904 - val_accuracy: 0.5862\n",
      "Epoch 533/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4940 - accuracy: 0.7365 - val_loss: 0.7462 - val_accuracy: 0.5172\n",
      "Epoch 534/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4935 - accuracy: 0.7385 - val_loss: 0.8058 - val_accuracy: 0.5172\n",
      "Epoch 535/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4879 - accuracy: 0.7442 - val_loss: 0.7360 - val_accuracy: 0.5517\n",
      "Epoch 536/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4894 - accuracy: 0.7308 - val_loss: 0.7798 - val_accuracy: 0.5345\n",
      "Epoch 537/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4868 - accuracy: 0.7423 - val_loss: 0.7526 - val_accuracy: 0.5345\n",
      "Epoch 538/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4895 - accuracy: 0.7462 - val_loss: 0.7107 - val_accuracy: 0.5862\n",
      "Epoch 539/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4897 - accuracy: 0.7500 - val_loss: 0.7474 - val_accuracy: 0.5345\n",
      "Epoch 540/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4906 - accuracy: 0.7385 - val_loss: 0.7745 - val_accuracy: 0.5345\n",
      "Epoch 541/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4881 - accuracy: 0.7288 - val_loss: 0.7453 - val_accuracy: 0.5345\n",
      "Epoch 542/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4900 - accuracy: 0.7481 - val_loss: 0.7206 - val_accuracy: 0.5690\n",
      "Epoch 543/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5011 - accuracy: 0.7269 - val_loss: 0.7361 - val_accuracy: 0.5517\n",
      "Epoch 544/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4963 - accuracy: 0.7327 - val_loss: 0.7631 - val_accuracy: 0.5517\n",
      "Epoch 545/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4864 - accuracy: 0.7385 - val_loss: 0.7447 - val_accuracy: 0.5345\n",
      "Epoch 546/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4854 - accuracy: 0.7500 - val_loss: 0.7323 - val_accuracy: 0.5345\n",
      "Epoch 547/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4809 - accuracy: 0.7558 - val_loss: 0.7599 - val_accuracy: 0.5345\n",
      "Epoch 548/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4816 - accuracy: 0.7423 - val_loss: 0.8434 - val_accuracy: 0.5000\n",
      "Epoch 549/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4820 - accuracy: 0.7442 - val_loss: 0.7787 - val_accuracy: 0.5345\n",
      "Epoch 550/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4897 - accuracy: 0.7365 - val_loss: 0.7349 - val_accuracy: 0.5345\n",
      "Epoch 551/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4852 - accuracy: 0.7423 - val_loss: 0.8407 - val_accuracy: 0.5172\n",
      "Epoch 552/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5045 - accuracy: 0.7404 - val_loss: 0.7594 - val_accuracy: 0.5345\n",
      "Epoch 553/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5161 - accuracy: 0.7308 - val_loss: 0.6987 - val_accuracy: 0.5517\n",
      "Epoch 554/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4955 - accuracy: 0.7538 - val_loss: 0.7602 - val_accuracy: 0.5345\n",
      "Epoch 555/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4905 - accuracy: 0.7404 - val_loss: 0.7480 - val_accuracy: 0.5345\n",
      "Epoch 556/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4827 - accuracy: 0.7404 - val_loss: 0.7371 - val_accuracy: 0.5345\n",
      "Epoch 557/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4791 - accuracy: 0.7519 - val_loss: 0.7741 - val_accuracy: 0.5172\n",
      "Epoch 558/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4794 - accuracy: 0.7577 - val_loss: 0.7589 - val_accuracy: 0.5172\n",
      "Epoch 559/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4794 - accuracy: 0.7519 - val_loss: 0.7657 - val_accuracy: 0.5172\n",
      "Epoch 560/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4806 - accuracy: 0.7558 - val_loss: 0.7913 - val_accuracy: 0.5345\n",
      "Epoch 561/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4755 - accuracy: 0.7442 - val_loss: 0.7213 - val_accuracy: 0.5345\n",
      "Epoch 562/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4872 - accuracy: 0.7500 - val_loss: 0.7247 - val_accuracy: 0.5345\n",
      "Epoch 563/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4795 - accuracy: 0.7654 - val_loss: 0.7587 - val_accuracy: 0.5345\n",
      "Epoch 564/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4807 - accuracy: 0.7538 - val_loss: 0.8547 - val_accuracy: 0.5172\n",
      "Epoch 565/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4817 - accuracy: 0.7462 - val_loss: 0.7323 - val_accuracy: 0.5345\n",
      "Epoch 566/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4777 - accuracy: 0.7538 - val_loss: 0.7669 - val_accuracy: 0.5172\n",
      "Epoch 567/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4780 - accuracy: 0.7596 - val_loss: 0.8013 - val_accuracy: 0.5172\n",
      "Epoch 568/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4818 - accuracy: 0.7423 - val_loss: 0.7624 - val_accuracy: 0.5345\n",
      "Epoch 569/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4776 - accuracy: 0.7519 - val_loss: 0.7334 - val_accuracy: 0.5172\n",
      "Epoch 570/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4828 - accuracy: 0.7423 - val_loss: 0.8201 - val_accuracy: 0.5172\n",
      "Epoch 571/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4825 - accuracy: 0.7519 - val_loss: 0.8777 - val_accuracy: 0.5000\n",
      "Epoch 572/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4798 - accuracy: 0.7635 - val_loss: 0.8253 - val_accuracy: 0.5345\n",
      "Epoch 573/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4816 - accuracy: 0.7519 - val_loss: 0.7436 - val_accuracy: 0.5345\n",
      "Epoch 574/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4816 - accuracy: 0.7481 - val_loss: 0.7572 - val_accuracy: 0.5345\n",
      "Epoch 575/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4874 - accuracy: 0.7519 - val_loss: 0.7613 - val_accuracy: 0.5172\n",
      "Epoch 576/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4802 - accuracy: 0.7442 - val_loss: 0.8383 - val_accuracy: 0.5172\n",
      "Epoch 577/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4774 - accuracy: 0.7462 - val_loss: 0.8569 - val_accuracy: 0.5172\n",
      "Epoch 578/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4871 - accuracy: 0.7423 - val_loss: 0.7645 - val_accuracy: 0.5172\n",
      "Epoch 579/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4745 - accuracy: 0.7654 - val_loss: 0.8380 - val_accuracy: 0.5172\n",
      "Epoch 580/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.7577 - val_loss: 0.8041 - val_accuracy: 0.5172\n",
      "Epoch 581/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4890 - accuracy: 0.7538 - val_loss: 0.7844 - val_accuracy: 0.5345\n",
      "Epoch 582/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4759 - accuracy: 0.7538 - val_loss: 0.8109 - val_accuracy: 0.5345\n",
      "Epoch 583/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4746 - accuracy: 0.7654 - val_loss: 0.7836 - val_accuracy: 0.5345\n",
      "Epoch 584/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4727 - accuracy: 0.7538 - val_loss: 0.7789 - val_accuracy: 0.5517\n",
      "Epoch 585/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4722 - accuracy: 0.7462 - val_loss: 0.7628 - val_accuracy: 0.5345\n",
      "Epoch 586/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4752 - accuracy: 0.7538 - val_loss: 0.8046 - val_accuracy: 0.5172\n",
      "Epoch 587/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4734 - accuracy: 0.7673 - val_loss: 0.8044 - val_accuracy: 0.5000\n",
      "Epoch 588/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4774 - accuracy: 0.7577 - val_loss: 0.8121 - val_accuracy: 0.5517\n",
      "Epoch 589/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4741 - accuracy: 0.7538 - val_loss: 0.7321 - val_accuracy: 0.5862\n",
      "Epoch 590/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4856 - accuracy: 0.7462 - val_loss: 0.7744 - val_accuracy: 0.5345\n",
      "Epoch 591/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4943 - accuracy: 0.7404 - val_loss: 0.7815 - val_accuracy: 0.5345\n",
      "Epoch 592/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4696 - accuracy: 0.7615 - val_loss: 0.8299 - val_accuracy: 0.5000\n",
      "Epoch 593/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4814 - accuracy: 0.7596 - val_loss: 0.8308 - val_accuracy: 0.5172\n",
      "Epoch 594/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4705 - accuracy: 0.7615 - val_loss: 0.8519 - val_accuracy: 0.5345\n",
      "Epoch 595/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4714 - accuracy: 0.7462 - val_loss: 1.0182 - val_accuracy: 0.4655\n",
      "Epoch 596/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5156 - accuracy: 0.7577 - val_loss: 0.8566 - val_accuracy: 0.4828\n",
      "Epoch 597/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4853 - accuracy: 0.7442 - val_loss: 0.8100 - val_accuracy: 0.5000\n",
      "Epoch 598/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4686 - accuracy: 0.7538 - val_loss: 0.8420 - val_accuracy: 0.5000\n",
      "Epoch 599/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4716 - accuracy: 0.7596 - val_loss: 0.8431 - val_accuracy: 0.5172\n",
      "Epoch 600/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4681 - accuracy: 0.7635 - val_loss: 0.8130 - val_accuracy: 0.4828\n",
      "Epoch 601/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4716 - accuracy: 0.7423 - val_loss: 0.8436 - val_accuracy: 0.5000\n",
      "Epoch 602/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4690 - accuracy: 0.7519 - val_loss: 0.8598 - val_accuracy: 0.5172\n",
      "Epoch 603/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4685 - accuracy: 0.7538 - val_loss: 0.7921 - val_accuracy: 0.4828\n",
      "Epoch 604/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4620 - accuracy: 0.7654 - val_loss: 0.7810 - val_accuracy: 0.4828\n",
      "Epoch 605/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4719 - accuracy: 0.7538 - val_loss: 0.8191 - val_accuracy: 0.5000\n",
      "Epoch 606/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4681 - accuracy: 0.7731 - val_loss: 0.7839 - val_accuracy: 0.5000\n",
      "Epoch 607/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4774 - accuracy: 0.7538 - val_loss: 0.8941 - val_accuracy: 0.5000\n",
      "Epoch 608/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5125 - accuracy: 0.7404 - val_loss: 0.7544 - val_accuracy: 0.5172\n",
      "Epoch 609/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4657 - accuracy: 0.7635 - val_loss: 0.8120 - val_accuracy: 0.5172\n",
      "Epoch 610/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4677 - accuracy: 0.7538 - val_loss: 0.8011 - val_accuracy: 0.5345\n",
      "Epoch 611/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4784 - accuracy: 0.7692 - val_loss: 0.8351 - val_accuracy: 0.5000\n",
      "Epoch 612/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4690 - accuracy: 0.7750 - val_loss: 0.8517 - val_accuracy: 0.5000\n",
      "Epoch 613/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4794 - accuracy: 0.7577 - val_loss: 0.9077 - val_accuracy: 0.5000\n",
      "Epoch 614/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4661 - accuracy: 0.7577 - val_loss: 0.8206 - val_accuracy: 0.5172\n",
      "Epoch 615/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4665 - accuracy: 0.7615 - val_loss: 0.7748 - val_accuracy: 0.4828\n",
      "Epoch 616/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4639 - accuracy: 0.7635 - val_loss: 0.8515 - val_accuracy: 0.5172\n",
      "Epoch 617/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4651 - accuracy: 0.7635 - val_loss: 0.9043 - val_accuracy: 0.5000\n",
      "Epoch 618/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4640 - accuracy: 0.7615 - val_loss: 0.8329 - val_accuracy: 0.5172\n",
      "Epoch 619/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4611 - accuracy: 0.7750 - val_loss: 0.8026 - val_accuracy: 0.5172\n",
      "Epoch 620/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4617 - accuracy: 0.7577 - val_loss: 0.8226 - val_accuracy: 0.5172\n",
      "Epoch 621/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4611 - accuracy: 0.7558 - val_loss: 0.7940 - val_accuracy: 0.5345\n",
      "Epoch 622/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4743 - accuracy: 0.7481 - val_loss: 0.7839 - val_accuracy: 0.5172\n",
      "Epoch 623/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4874 - accuracy: 0.7500 - val_loss: 0.8260 - val_accuracy: 0.5172\n",
      "Epoch 624/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4631 - accuracy: 0.7558 - val_loss: 0.7801 - val_accuracy: 0.5172\n",
      "Epoch 625/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4655 - accuracy: 0.7615 - val_loss: 0.7921 - val_accuracy: 0.5172\n",
      "Epoch 626/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4617 - accuracy: 0.7635 - val_loss: 0.8378 - val_accuracy: 0.5172\n",
      "Epoch 627/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4617 - accuracy: 0.7519 - val_loss: 0.8151 - val_accuracy: 0.5172\n",
      "Epoch 628/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4647 - accuracy: 0.7615 - val_loss: 0.8496 - val_accuracy: 0.5172\n",
      "Epoch 629/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4644 - accuracy: 0.7615 - val_loss: 0.7975 - val_accuracy: 0.5000\n",
      "Epoch 630/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4672 - accuracy: 0.7558 - val_loss: 0.8305 - val_accuracy: 0.5000\n",
      "Epoch 631/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4623 - accuracy: 0.7615 - val_loss: 0.8414 - val_accuracy: 0.5172\n",
      "Epoch 632/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4589 - accuracy: 0.7673 - val_loss: 0.9021 - val_accuracy: 0.5000\n",
      "Epoch 633/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4606 - accuracy: 0.7635 - val_loss: 0.8581 - val_accuracy: 0.5345\n",
      "Epoch 634/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4626 - accuracy: 0.7558 - val_loss: 0.9574 - val_accuracy: 0.5172\n",
      "Epoch 635/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4904 - accuracy: 0.7365 - val_loss: 0.8188 - val_accuracy: 0.5000\n",
      "Epoch 636/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4718 - accuracy: 0.7673 - val_loss: 0.8464 - val_accuracy: 0.5345\n",
      "Epoch 637/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4593 - accuracy: 0.7692 - val_loss: 0.8663 - val_accuracy: 0.5000\n",
      "Epoch 638/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4625 - accuracy: 0.7500 - val_loss: 0.8039 - val_accuracy: 0.4828\n",
      "Epoch 639/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4591 - accuracy: 0.7635 - val_loss: 0.9387 - val_accuracy: 0.5172\n",
      "Epoch 640/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4644 - accuracy: 0.7654 - val_loss: 0.8806 - val_accuracy: 0.5172\n",
      "Epoch 641/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4584 - accuracy: 0.7788 - val_loss: 0.8520 - val_accuracy: 0.5000\n",
      "Epoch 642/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4545 - accuracy: 0.7673 - val_loss: 0.9056 - val_accuracy: 0.5172\n",
      "Epoch 643/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4558 - accuracy: 0.7731 - val_loss: 0.9166 - val_accuracy: 0.5000\n",
      "Epoch 644/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4625 - accuracy: 0.7692 - val_loss: 0.8818 - val_accuracy: 0.5345\n",
      "Epoch 645/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4576 - accuracy: 0.7692 - val_loss: 0.8881 - val_accuracy: 0.5172\n",
      "Epoch 646/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4640 - accuracy: 0.7654 - val_loss: 0.8382 - val_accuracy: 0.5172\n",
      "Epoch 647/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4807 - accuracy: 0.7538 - val_loss: 0.7742 - val_accuracy: 0.5345\n",
      "Epoch 648/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4719 - accuracy: 0.7481 - val_loss: 0.8804 - val_accuracy: 0.5172\n",
      "Epoch 649/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4674 - accuracy: 0.7596 - val_loss: 0.8050 - val_accuracy: 0.4828\n",
      "Epoch 650/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4581 - accuracy: 0.7615 - val_loss: 0.9109 - val_accuracy: 0.5172\n",
      "Epoch 651/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4585 - accuracy: 0.7731 - val_loss: 0.8625 - val_accuracy: 0.5172\n",
      "Epoch 652/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4583 - accuracy: 0.7673 - val_loss: 0.8806 - val_accuracy: 0.5172\n",
      "Epoch 653/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4655 - accuracy: 0.7635 - val_loss: 0.8975 - val_accuracy: 0.5172\n",
      "Epoch 654/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4561 - accuracy: 0.7731 - val_loss: 0.9354 - val_accuracy: 0.5172\n",
      "Epoch 655/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4543 - accuracy: 0.7731 - val_loss: 0.8640 - val_accuracy: 0.4828\n",
      "Epoch 656/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4590 - accuracy: 0.7615 - val_loss: 0.9553 - val_accuracy: 0.5000\n",
      "Epoch 657/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4587 - accuracy: 0.7673 - val_loss: 0.8916 - val_accuracy: 0.5000\n",
      "Epoch 658/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4759 - accuracy: 0.7635 - val_loss: 0.8552 - val_accuracy: 0.5172\n",
      "Epoch 659/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4642 - accuracy: 0.7692 - val_loss: 0.8494 - val_accuracy: 0.5172\n",
      "Epoch 660/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4710 - accuracy: 0.7558 - val_loss: 0.8933 - val_accuracy: 0.5517\n",
      "Epoch 661/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4758 - accuracy: 0.7577 - val_loss: 0.8434 - val_accuracy: 0.5000\n",
      "Epoch 662/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4621 - accuracy: 0.7596 - val_loss: 0.7963 - val_accuracy: 0.5345\n",
      "Epoch 663/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4553 - accuracy: 0.7654 - val_loss: 0.8471 - val_accuracy: 0.5345\n",
      "Epoch 664/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4539 - accuracy: 0.7692 - val_loss: 0.8683 - val_accuracy: 0.5000\n",
      "Epoch 665/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4538 - accuracy: 0.7808 - val_loss: 0.8733 - val_accuracy: 0.4655\n",
      "Epoch 666/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4490 - accuracy: 0.7673 - val_loss: 0.9146 - val_accuracy: 0.5000\n",
      "Epoch 667/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4579 - accuracy: 0.7596 - val_loss: 0.9850 - val_accuracy: 0.5000\n",
      "Epoch 668/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4555 - accuracy: 0.7654 - val_loss: 0.9124 - val_accuracy: 0.5000\n",
      "Epoch 669/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4581 - accuracy: 0.7654 - val_loss: 0.8658 - val_accuracy: 0.5172\n",
      "Epoch 670/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4572 - accuracy: 0.7596 - val_loss: 0.8459 - val_accuracy: 0.5345\n",
      "Epoch 671/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4541 - accuracy: 0.7635 - val_loss: 0.8904 - val_accuracy: 0.4828\n",
      "Epoch 672/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4554 - accuracy: 0.7827 - val_loss: 0.8453 - val_accuracy: 0.5172\n",
      "Epoch 673/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4574 - accuracy: 0.7692 - val_loss: 0.8945 - val_accuracy: 0.5000\n",
      "Epoch 674/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4626 - accuracy: 0.7558 - val_loss: 0.9587 - val_accuracy: 0.5345\n",
      "Epoch 675/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4585 - accuracy: 0.7596 - val_loss: 0.8308 - val_accuracy: 0.5345\n",
      "Epoch 676/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4571 - accuracy: 0.7654 - val_loss: 0.9132 - val_accuracy: 0.5172\n",
      "Epoch 677/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4500 - accuracy: 0.7692 - val_loss: 0.8439 - val_accuracy: 0.5000\n",
      "Epoch 678/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4530 - accuracy: 0.7846 - val_loss: 0.9289 - val_accuracy: 0.5000\n",
      "Epoch 679/2000\n",
      "520/520 [==============================] - ETA: 0s - loss: 0.4432 - accuracy: 0.76 - 1s 1ms/step - loss: 0.4499 - accuracy: 0.7673 - val_loss: 0.9693 - val_accuracy: 0.5000\n",
      "Epoch 680/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4790 - accuracy: 0.7577 - val_loss: 0.8891 - val_accuracy: 0.5000\n",
      "Epoch 681/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4561 - accuracy: 0.7673 - val_loss: 0.8815 - val_accuracy: 0.5000\n",
      "Epoch 682/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4517 - accuracy: 0.7750 - val_loss: 1.0806 - val_accuracy: 0.5000\n",
      "Epoch 683/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4530 - accuracy: 0.7615 - val_loss: 0.8599 - val_accuracy: 0.5000\n",
      "Epoch 684/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4551 - accuracy: 0.7692 - val_loss: 0.9263 - val_accuracy: 0.5000\n",
      "Epoch 685/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4507 - accuracy: 0.7577 - val_loss: 0.8856 - val_accuracy: 0.5000\n",
      "Epoch 686/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4563 - accuracy: 0.7577 - val_loss: 0.9069 - val_accuracy: 0.5000\n",
      "Epoch 687/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4492 - accuracy: 0.7635 - val_loss: 0.9428 - val_accuracy: 0.5000\n",
      "Epoch 688/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4518 - accuracy: 0.7731 - val_loss: 0.8757 - val_accuracy: 0.4828\n",
      "Epoch 689/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4495 - accuracy: 0.7692 - val_loss: 0.8645 - val_accuracy: 0.5517\n",
      "Epoch 690/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4607 - accuracy: 0.7596 - val_loss: 0.8931 - val_accuracy: 0.4655\n",
      "Epoch 691/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4585 - accuracy: 0.7654 - val_loss: 0.8442 - val_accuracy: 0.4828\n",
      "Epoch 692/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4532 - accuracy: 0.7596 - val_loss: 0.9390 - val_accuracy: 0.5000\n",
      "Epoch 693/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4630 - accuracy: 0.7654 - val_loss: 0.9611 - val_accuracy: 0.5000\n",
      "Epoch 694/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4570 - accuracy: 0.7750 - val_loss: 0.9275 - val_accuracy: 0.5172\n",
      "Epoch 695/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4500 - accuracy: 0.7673 - val_loss: 0.9213 - val_accuracy: 0.5000\n",
      "Epoch 696/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4459 - accuracy: 0.7731 - val_loss: 0.8950 - val_accuracy: 0.5000\n",
      "Epoch 697/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4585 - accuracy: 0.7596 - val_loss: 0.9584 - val_accuracy: 0.5172\n",
      "Epoch 698/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4474 - accuracy: 0.7500 - val_loss: 0.8250 - val_accuracy: 0.5172\n",
      "Epoch 699/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4493 - accuracy: 0.7654 - val_loss: 0.8705 - val_accuracy: 0.5172\n",
      "Epoch 700/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4846 - accuracy: 0.7596 - val_loss: 0.9263 - val_accuracy: 0.5172\n",
      "Epoch 701/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.5162 - accuracy: 0.7481 - val_loss: 0.8172 - val_accuracy: 0.5345\n",
      "Epoch 702/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4540 - accuracy: 0.7692 - val_loss: 0.8196 - val_accuracy: 0.5345\n",
      "Epoch 703/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4484 - accuracy: 0.7788 - val_loss: 0.9518 - val_accuracy: 0.5172\n",
      "Epoch 704/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4429 - accuracy: 0.7808 - val_loss: 1.0387 - val_accuracy: 0.5000\n",
      "Epoch 705/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4535 - accuracy: 0.7673 - val_loss: 0.8978 - val_accuracy: 0.5000\n",
      "Epoch 706/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4470 - accuracy: 0.7846 - val_loss: 0.9019 - val_accuracy: 0.5345\n",
      "Epoch 707/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4532 - accuracy: 0.7712 - val_loss: 0.9161 - val_accuracy: 0.4828\n",
      "Epoch 708/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4483 - accuracy: 0.7712 - val_loss: 0.9246 - val_accuracy: 0.5172\n",
      "Epoch 709/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4490 - accuracy: 0.7635 - val_loss: 0.9687 - val_accuracy: 0.5172\n",
      "Epoch 710/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4484 - accuracy: 0.7692 - val_loss: 0.8413 - val_accuracy: 0.5000\n",
      "Epoch 711/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4549 - accuracy: 0.7442 - val_loss: 1.0108 - val_accuracy: 0.5000\n",
      "Epoch 712/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4504 - accuracy: 0.7712 - val_loss: 0.8656 - val_accuracy: 0.5000\n",
      "Epoch 713/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4561 - accuracy: 0.7577 - val_loss: 0.9394 - val_accuracy: 0.4828\n",
      "Epoch 714/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4460 - accuracy: 0.7692 - val_loss: 0.8690 - val_accuracy: 0.5172\n",
      "Epoch 715/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4505 - accuracy: 0.7654 - val_loss: 0.9219 - val_accuracy: 0.5172\n",
      "Epoch 716/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4478 - accuracy: 0.7750 - val_loss: 0.8396 - val_accuracy: 0.5345\n",
      "Epoch 717/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4511 - accuracy: 0.7654 - val_loss: 0.9692 - val_accuracy: 0.5345\n",
      "Epoch 718/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4484 - accuracy: 0.7577 - val_loss: 0.8749 - val_accuracy: 0.5000\n",
      "Epoch 719/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4505 - accuracy: 0.7846 - val_loss: 0.9338 - val_accuracy: 0.5172\n",
      "Epoch 720/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4470 - accuracy: 0.7846 - val_loss: 0.9474 - val_accuracy: 0.5000\n",
      "Epoch 721/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4466 - accuracy: 0.7673 - val_loss: 0.8721 - val_accuracy: 0.5000\n",
      "Epoch 722/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4497 - accuracy: 0.7692 - val_loss: 0.9447 - val_accuracy: 0.5172\n",
      "Epoch 723/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4508 - accuracy: 0.7654 - val_loss: 0.9412 - val_accuracy: 0.5172\n",
      "Epoch 724/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4448 - accuracy: 0.7769 - val_loss: 1.0317 - val_accuracy: 0.5172\n",
      "Epoch 725/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4505 - accuracy: 0.7673 - val_loss: 0.9676 - val_accuracy: 0.5000\n",
      "Epoch 726/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4437 - accuracy: 0.7827 - val_loss: 0.8425 - val_accuracy: 0.5000\n",
      "Epoch 727/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4482 - accuracy: 0.7750 - val_loss: 1.0252 - val_accuracy: 0.5000\n",
      "Epoch 728/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4501 - accuracy: 0.7615 - val_loss: 0.9544 - val_accuracy: 0.4483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 729/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4722 - accuracy: 0.7635 - val_loss: 1.0443 - val_accuracy: 0.5000\n",
      "Epoch 730/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4578 - accuracy: 0.7596 - val_loss: 0.9235 - val_accuracy: 0.5517\n",
      "Epoch 731/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4450 - accuracy: 0.7846 - val_loss: 0.9854 - val_accuracy: 0.5000\n",
      "Epoch 732/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4493 - accuracy: 0.7654 - val_loss: 0.9406 - val_accuracy: 0.5000\n",
      "Epoch 733/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4444 - accuracy: 0.7769 - val_loss: 0.9290 - val_accuracy: 0.5000\n",
      "Epoch 734/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4455 - accuracy: 0.7712 - val_loss: 0.9482 - val_accuracy: 0.5000\n",
      "Epoch 735/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4458 - accuracy: 0.7635 - val_loss: 0.9461 - val_accuracy: 0.5172\n",
      "Epoch 736/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4438 - accuracy: 0.7865 - val_loss: 0.9825 - val_accuracy: 0.5000\n",
      "Epoch 737/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4519 - accuracy: 0.7692 - val_loss: 0.9170 - val_accuracy: 0.5172\n",
      "Epoch 738/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4461 - accuracy: 0.7673 - val_loss: 0.9833 - val_accuracy: 0.5345\n",
      "Epoch 739/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4428 - accuracy: 0.7731 - val_loss: 0.9489 - val_accuracy: 0.5000\n",
      "Epoch 740/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4485 - accuracy: 0.7692 - val_loss: 0.8985 - val_accuracy: 0.5172\n",
      "Epoch 741/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4476 - accuracy: 0.7808 - val_loss: 0.9828 - val_accuracy: 0.5000\n",
      "Epoch 742/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4446 - accuracy: 0.7635 - val_loss: 1.0120 - val_accuracy: 0.5172\n",
      "Epoch 743/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4485 - accuracy: 0.7635 - val_loss: 0.8873 - val_accuracy: 0.4828\n",
      "Epoch 744/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4477 - accuracy: 0.7750 - val_loss: 0.8695 - val_accuracy: 0.5172\n",
      "Epoch 745/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4402 - accuracy: 0.7712 - val_loss: 0.9179 - val_accuracy: 0.5345\n",
      "Epoch 746/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4521 - accuracy: 0.7692 - val_loss: 0.8679 - val_accuracy: 0.5345\n",
      "Epoch 747/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4774 - accuracy: 0.7481 - val_loss: 0.9590 - val_accuracy: 0.5000\n",
      "Epoch 748/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4455 - accuracy: 0.7769 - val_loss: 0.8940 - val_accuracy: 0.5000\n",
      "Epoch 749/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4338 - accuracy: 0.7654 - val_loss: 1.0234 - val_accuracy: 0.5172\n",
      "Epoch 750/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4458 - accuracy: 0.7615 - val_loss: 0.9037 - val_accuracy: 0.4828\n",
      "Epoch 751/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4364 - accuracy: 0.7865 - val_loss: 0.8442 - val_accuracy: 0.5172\n",
      "Epoch 752/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4443 - accuracy: 0.7654 - val_loss: 0.9560 - val_accuracy: 0.5172\n",
      "Epoch 753/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4409 - accuracy: 0.7788 - val_loss: 0.8952 - val_accuracy: 0.5345\n",
      "Epoch 754/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4394 - accuracy: 0.7673 - val_loss: 0.9419 - val_accuracy: 0.5172\n",
      "Epoch 755/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4496 - accuracy: 0.7712 - val_loss: 0.9130 - val_accuracy: 0.5000\n",
      "Epoch 756/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4374 - accuracy: 0.7808 - val_loss: 0.9247 - val_accuracy: 0.5172\n",
      "Epoch 757/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4477 - accuracy: 0.7692 - val_loss: 0.9819 - val_accuracy: 0.5172\n",
      "Epoch 758/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4370 - accuracy: 0.7846 - val_loss: 0.9956 - val_accuracy: 0.4828\n",
      "Epoch 759/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4397 - accuracy: 0.7808 - val_loss: 1.0762 - val_accuracy: 0.5000\n",
      "Epoch 760/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4438 - accuracy: 0.7712 - val_loss: 0.9372 - val_accuracy: 0.5345\n",
      "Epoch 761/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4395 - accuracy: 0.7692 - val_loss: 0.9499 - val_accuracy: 0.5345\n",
      "Epoch 762/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4383 - accuracy: 0.7712 - val_loss: 0.8572 - val_accuracy: 0.5000\n",
      "Epoch 763/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4870 - accuracy: 0.7538 - val_loss: 0.9660 - val_accuracy: 0.5172\n",
      "Epoch 764/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4576 - accuracy: 0.7635 - val_loss: 1.0121 - val_accuracy: 0.5000\n",
      "Epoch 765/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4377 - accuracy: 0.7712 - val_loss: 1.0235 - val_accuracy: 0.5345\n",
      "Epoch 766/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4324 - accuracy: 0.7827 - val_loss: 1.0412 - val_accuracy: 0.5000\n",
      "Epoch 767/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4477 - accuracy: 0.7731 - val_loss: 0.9952 - val_accuracy: 0.5172\n",
      "Epoch 768/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4506 - accuracy: 0.7692 - val_loss: 0.8999 - val_accuracy: 0.5172\n",
      "Epoch 769/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4366 - accuracy: 0.7692 - val_loss: 1.0468 - val_accuracy: 0.5172\n",
      "Epoch 770/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4408 - accuracy: 0.7692 - val_loss: 0.9044 - val_accuracy: 0.5000\n",
      "Epoch 771/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4382 - accuracy: 0.7846 - val_loss: 0.9732 - val_accuracy: 0.5000\n",
      "Epoch 772/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4347 - accuracy: 0.7712 - val_loss: 1.0162 - val_accuracy: 0.5172\n",
      "Epoch 773/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4275 - accuracy: 0.7808 - val_loss: 1.0715 - val_accuracy: 0.5172\n",
      "Epoch 774/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4374 - accuracy: 0.7750 - val_loss: 0.9329 - val_accuracy: 0.5000\n",
      "Epoch 775/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4314 - accuracy: 0.7846 - val_loss: 1.0258 - val_accuracy: 0.5000\n",
      "Epoch 776/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4474 - accuracy: 0.7654 - val_loss: 0.8657 - val_accuracy: 0.5172\n",
      "Epoch 777/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4614 - accuracy: 0.7673 - val_loss: 0.8535 - val_accuracy: 0.5345\n",
      "Epoch 778/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4421 - accuracy: 0.7712 - val_loss: 0.9103 - val_accuracy: 0.5345\n",
      "Epoch 779/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4369 - accuracy: 0.7846 - val_loss: 0.9416 - val_accuracy: 0.4828\n",
      "Epoch 780/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4386 - accuracy: 0.7885 - val_loss: 1.0352 - val_accuracy: 0.5172\n",
      "Epoch 781/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4363 - accuracy: 0.7635 - val_loss: 0.9893 - val_accuracy: 0.5172\n",
      "Epoch 782/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4376 - accuracy: 0.7750 - val_loss: 0.8680 - val_accuracy: 0.5517\n",
      "Epoch 783/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4390 - accuracy: 0.7769 - val_loss: 0.9820 - val_accuracy: 0.5000\n",
      "Epoch 784/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4348 - accuracy: 0.7865 - val_loss: 0.9897 - val_accuracy: 0.5172\n",
      "Epoch 785/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4355 - accuracy: 0.7731 - val_loss: 0.8700 - val_accuracy: 0.5000\n",
      "Epoch 786/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4318 - accuracy: 0.7904 - val_loss: 0.9241 - val_accuracy: 0.5172\n",
      "Epoch 787/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4334 - accuracy: 0.7846 - val_loss: 0.8974 - val_accuracy: 0.5000\n",
      "Epoch 788/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4335 - accuracy: 0.7827 - val_loss: 0.8625 - val_accuracy: 0.5345\n",
      "Epoch 789/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4376 - accuracy: 0.7827 - val_loss: 0.9329 - val_accuracy: 0.5172\n",
      "Epoch 790/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4358 - accuracy: 0.7846 - val_loss: 1.0957 - val_accuracy: 0.5000\n",
      "Epoch 791/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4366 - accuracy: 0.7808 - val_loss: 0.9221 - val_accuracy: 0.5000\n",
      "Epoch 792/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4316 - accuracy: 0.7865 - val_loss: 0.9141 - val_accuracy: 0.4828\n",
      "Epoch 793/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4406 - accuracy: 0.7692 - val_loss: 0.9372 - val_accuracy: 0.5000\n",
      "Epoch 794/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4434 - accuracy: 0.7750 - val_loss: 1.0354 - val_accuracy: 0.5172\n",
      "Epoch 795/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4397 - accuracy: 0.7904 - val_loss: 0.9242 - val_accuracy: 0.5345\n",
      "Epoch 796/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4290 - accuracy: 0.8000 - val_loss: 0.9168 - val_accuracy: 0.5172\n",
      "Epoch 797/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4379 - accuracy: 0.7865 - val_loss: 0.9079 - val_accuracy: 0.5000\n",
      "Epoch 798/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4307 - accuracy: 0.7942 - val_loss: 0.9101 - val_accuracy: 0.5000\n",
      "Epoch 799/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4342 - accuracy: 0.7769 - val_loss: 0.9343 - val_accuracy: 0.5000\n",
      "Epoch 800/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4470 - accuracy: 0.7865 - val_loss: 0.9117 - val_accuracy: 0.5345\n",
      "Epoch 801/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4684 - accuracy: 0.7712 - val_loss: 0.9501 - val_accuracy: 0.5345\n",
      "Epoch 802/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4420 - accuracy: 0.7769 - val_loss: 0.8960 - val_accuracy: 0.5345\n",
      "Epoch 803/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4368 - accuracy: 0.7712 - val_loss: 0.9377 - val_accuracy: 0.5345\n",
      "Epoch 804/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4322 - accuracy: 0.7769 - val_loss: 1.0767 - val_accuracy: 0.4828\n",
      "Epoch 805/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4309 - accuracy: 0.7827 - val_loss: 0.9649 - val_accuracy: 0.5000\n",
      "Epoch 806/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4292 - accuracy: 0.7808 - val_loss: 1.0374 - val_accuracy: 0.5172\n",
      "Epoch 807/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4353 - accuracy: 0.7827 - val_loss: 0.8559 - val_accuracy: 0.5517\n",
      "Epoch 808/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4304 - accuracy: 0.7808 - val_loss: 0.8471 - val_accuracy: 0.5345\n",
      "Epoch 809/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4344 - accuracy: 0.7846 - val_loss: 0.9895 - val_accuracy: 0.4828\n",
      "Epoch 810/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4305 - accuracy: 0.7904 - val_loss: 0.9587 - val_accuracy: 0.5172\n",
      "Epoch 811/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4416 - accuracy: 0.7673 - val_loss: 0.9805 - val_accuracy: 0.4828\n",
      "Epoch 812/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4307 - accuracy: 0.7865 - val_loss: 1.0438 - val_accuracy: 0.5000\n",
      "Epoch 813/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4310 - accuracy: 0.7827 - val_loss: 1.0317 - val_accuracy: 0.4828\n",
      "Epoch 814/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4293 - accuracy: 0.7904 - val_loss: 1.0495 - val_accuracy: 0.5000\n",
      "Epoch 815/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4326 - accuracy: 0.7885 - val_loss: 0.8926 - val_accuracy: 0.5172\n",
      "Epoch 816/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4365 - accuracy: 0.7865 - val_loss: 0.9819 - val_accuracy: 0.5172\n",
      "Epoch 817/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4778 - accuracy: 0.7558 - val_loss: 0.9603 - val_accuracy: 0.5172\n",
      "Epoch 818/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4406 - accuracy: 0.7731 - val_loss: 1.0269 - val_accuracy: 0.5000\n",
      "Epoch 819/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4290 - accuracy: 0.7846 - val_loss: 1.0195 - val_accuracy: 0.5172\n",
      "Epoch 820/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4329 - accuracy: 0.7865 - val_loss: 1.0184 - val_accuracy: 0.5345\n",
      "Epoch 821/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4237 - accuracy: 0.7923 - val_loss: 0.9617 - val_accuracy: 0.4655\n",
      "Epoch 822/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4266 - accuracy: 0.7923 - val_loss: 0.9993 - val_accuracy: 0.4828\n",
      "Epoch 823/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4259 - accuracy: 0.7846 - val_loss: 1.1264 - val_accuracy: 0.4828\n",
      "Epoch 824/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4349 - accuracy: 0.7865 - val_loss: 0.9926 - val_accuracy: 0.4655\n",
      "Epoch 825/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4506 - accuracy: 0.7731 - val_loss: 1.0264 - val_accuracy: 0.5172\n",
      "Epoch 826/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4316 - accuracy: 0.7942 - val_loss: 1.2157 - val_accuracy: 0.4655\n",
      "Epoch 827/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4416 - accuracy: 0.7923 - val_loss: 1.0233 - val_accuracy: 0.5000\n",
      "Epoch 828/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4424 - accuracy: 0.7962 - val_loss: 0.9213 - val_accuracy: 0.5000\n",
      "Epoch 829/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4400 - accuracy: 0.7731 - val_loss: 1.0105 - val_accuracy: 0.5000\n",
      "Epoch 830/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4262 - accuracy: 0.7846 - val_loss: 0.9499 - val_accuracy: 0.5000\n",
      "Epoch 831/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4225 - accuracy: 0.7904 - val_loss: 1.1966 - val_accuracy: 0.5000\n",
      "Epoch 832/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4319 - accuracy: 0.7808 - val_loss: 1.0047 - val_accuracy: 0.5172\n",
      "Epoch 833/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4217 - accuracy: 0.7981 - val_loss: 0.9460 - val_accuracy: 0.5345\n",
      "Epoch 834/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4315 - accuracy: 0.7769 - val_loss: 1.1139 - val_accuracy: 0.5000\n",
      "Epoch 835/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4354 - accuracy: 0.7788 - val_loss: 0.9556 - val_accuracy: 0.5172\n",
      "Epoch 836/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4280 - accuracy: 0.7846 - val_loss: 0.9314 - val_accuracy: 0.5172\n",
      "Epoch 837/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4372 - accuracy: 0.7750 - val_loss: 1.0762 - val_accuracy: 0.5345\n",
      "Epoch 838/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4252 - accuracy: 0.7981 - val_loss: 1.1879 - val_accuracy: 0.5000\n",
      "Epoch 839/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4386 - accuracy: 0.7827 - val_loss: 1.0189 - val_accuracy: 0.5172\n",
      "Epoch 840/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4350 - accuracy: 0.7865 - val_loss: 0.9774 - val_accuracy: 0.5000\n",
      "Epoch 841/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4257 - accuracy: 0.7962 - val_loss: 0.9094 - val_accuracy: 0.5000\n",
      "Epoch 842/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4303 - accuracy: 0.7904 - val_loss: 0.8158 - val_accuracy: 0.5172\n",
      "Epoch 843/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4267 - accuracy: 0.7865 - val_loss: 1.0056 - val_accuracy: 0.5000\n",
      "Epoch 844/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4279 - accuracy: 0.7846 - val_loss: 1.2055 - val_accuracy: 0.4828\n",
      "Epoch 845/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4318 - accuracy: 0.7769 - val_loss: 1.0385 - val_accuracy: 0.5172\n",
      "Epoch 846/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4411 - accuracy: 0.7731 - val_loss: 0.8835 - val_accuracy: 0.5000\n",
      "Epoch 847/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4329 - accuracy: 0.7827 - val_loss: 0.8703 - val_accuracy: 0.5517\n",
      "Epoch 848/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4296 - accuracy: 0.7731 - val_loss: 0.8588 - val_accuracy: 0.5345\n",
      "Epoch 849/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4362 - accuracy: 0.7712 - val_loss: 0.9199 - val_accuracy: 0.5172\n",
      "Epoch 850/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4351 - accuracy: 0.7788 - val_loss: 0.9530 - val_accuracy: 0.5172\n",
      "Epoch 851/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4247 - accuracy: 0.7923 - val_loss: 1.0063 - val_accuracy: 0.5000\n",
      "Epoch 852/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4248 - accuracy: 0.7885 - val_loss: 0.8731 - val_accuracy: 0.5345\n",
      "Epoch 853/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4353 - accuracy: 0.7712 - val_loss: 0.9306 - val_accuracy: 0.5172\n",
      "Epoch 854/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4236 - accuracy: 0.8077 - val_loss: 1.0114 - val_accuracy: 0.5000\n",
      "Epoch 855/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4410 - accuracy: 0.7827 - val_loss: 0.8682 - val_accuracy: 0.5172\n",
      "Epoch 856/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4308 - accuracy: 0.7808 - val_loss: 0.9367 - val_accuracy: 0.5172\n",
      "Epoch 857/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4218 - accuracy: 0.7962 - val_loss: 1.0473 - val_accuracy: 0.5172\n",
      "Epoch 858/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4237 - accuracy: 0.7846 - val_loss: 1.0085 - val_accuracy: 0.5000\n",
      "Epoch 859/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4237 - accuracy: 0.7962 - val_loss: 0.9086 - val_accuracy: 0.5345\n",
      "Epoch 860/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4223 - accuracy: 0.7962 - val_loss: 0.9000 - val_accuracy: 0.5172\n",
      "Epoch 861/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.7904 - val_loss: 0.9603 - val_accuracy: 0.5000\n",
      "Epoch 862/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4264 - accuracy: 0.7904 - val_loss: 0.8915 - val_accuracy: 0.5517\n",
      "Epoch 863/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4349 - accuracy: 0.7808 - val_loss: 0.9356 - val_accuracy: 0.5172\n",
      "Epoch 864/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.7923 - val_loss: 0.8315 - val_accuracy: 0.5172\n",
      "Epoch 865/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4184 - accuracy: 0.8000 - val_loss: 0.9517 - val_accuracy: 0.5345\n",
      "Epoch 866/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4209 - accuracy: 0.7827 - val_loss: 1.0400 - val_accuracy: 0.5000\n",
      "Epoch 867/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4255 - accuracy: 0.7827 - val_loss: 1.1132 - val_accuracy: 0.5172\n",
      "Epoch 868/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4289 - accuracy: 0.7788 - val_loss: 1.0230 - val_accuracy: 0.5172\n",
      "Epoch 869/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4235 - accuracy: 0.8038 - val_loss: 0.9878 - val_accuracy: 0.5345\n",
      "Epoch 870/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4367 - accuracy: 0.7673 - val_loss: 0.9299 - val_accuracy: 0.5345\n",
      "Epoch 871/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4237 - accuracy: 0.7981 - val_loss: 0.9610 - val_accuracy: 0.5172\n",
      "Epoch 872/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4145 - accuracy: 0.7962 - val_loss: 1.1267 - val_accuracy: 0.4828\n",
      "Epoch 873/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4260 - accuracy: 0.7885 - val_loss: 1.0067 - val_accuracy: 0.5172\n",
      "Epoch 874/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4208 - accuracy: 0.7827 - val_loss: 0.9525 - val_accuracy: 0.5517\n",
      "Epoch 875/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4375 - accuracy: 0.7769 - val_loss: 0.9457 - val_accuracy: 0.5345\n",
      "Epoch 876/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4432 - accuracy: 0.7769 - val_loss: 1.0693 - val_accuracy: 0.5172\n",
      "Epoch 877/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4313 - accuracy: 0.7808 - val_loss: 1.0565 - val_accuracy: 0.5172\n",
      "Epoch 878/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4301 - accuracy: 0.8000 - val_loss: 1.0568 - val_accuracy: 0.5172\n",
      "Epoch 879/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4243 - accuracy: 0.8000 - val_loss: 1.0704 - val_accuracy: 0.4828\n",
      "Epoch 880/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4268 - accuracy: 0.7942 - val_loss: 1.1113 - val_accuracy: 0.5000\n",
      "Epoch 881/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4129 - accuracy: 0.7962 - val_loss: 0.8468 - val_accuracy: 0.5000\n",
      "Epoch 882/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4220 - accuracy: 0.7731 - val_loss: 0.9112 - val_accuracy: 0.5517\n",
      "Epoch 883/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4231 - accuracy: 0.7885 - val_loss: 0.9158 - val_accuracy: 0.5345\n",
      "Epoch 884/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4620 - accuracy: 0.7865 - val_loss: 0.8408 - val_accuracy: 0.5517\n",
      "Epoch 885/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4328 - accuracy: 0.7885 - val_loss: 1.0599 - val_accuracy: 0.4655\n",
      "Epoch 886/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4252 - accuracy: 0.7923 - val_loss: 0.8728 - val_accuracy: 0.5172\n",
      "Epoch 887/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4328 - accuracy: 0.7827 - val_loss: 0.9717 - val_accuracy: 0.5517\n",
      "Epoch 888/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4251 - accuracy: 0.7962 - val_loss: 0.9347 - val_accuracy: 0.5345\n",
      "Epoch 889/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4233 - accuracy: 0.7846 - val_loss: 1.0838 - val_accuracy: 0.4828\n",
      "Epoch 890/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4233 - accuracy: 0.8000 - val_loss: 0.9190 - val_accuracy: 0.5172\n",
      "Epoch 891/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4265 - accuracy: 0.7962 - val_loss: 1.2010 - val_accuracy: 0.5000\n",
      "Epoch 892/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4217 - accuracy: 0.8019 - val_loss: 1.0445 - val_accuracy: 0.5172\n",
      "Epoch 893/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4309 - accuracy: 0.7865 - val_loss: 1.0602 - val_accuracy: 0.4828\n",
      "Epoch 894/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4213 - accuracy: 0.8000 - val_loss: 0.8986 - val_accuracy: 0.5172\n",
      "Epoch 895/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4146 - accuracy: 0.8019 - val_loss: 1.0520 - val_accuracy: 0.5172\n",
      "Epoch 896/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4313 - accuracy: 0.7788 - val_loss: 0.9917 - val_accuracy: 0.5345\n",
      "Epoch 897/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4169 - accuracy: 0.8019 - val_loss: 1.0794 - val_accuracy: 0.4828\n",
      "Epoch 898/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4271 - accuracy: 0.7769 - val_loss: 1.0072 - val_accuracy: 0.5172\n",
      "Epoch 899/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4261 - accuracy: 0.7885 - val_loss: 0.9978 - val_accuracy: 0.5345\n",
      "Epoch 900/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4229 - accuracy: 0.7808 - val_loss: 1.0472 - val_accuracy: 0.5172\n",
      "Epoch 901/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4292 - accuracy: 0.7885 - val_loss: 0.8882 - val_accuracy: 0.5345\n",
      "Epoch 902/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4185 - accuracy: 0.7923 - val_loss: 0.9504 - val_accuracy: 0.5517\n",
      "Epoch 903/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4111 - accuracy: 0.8000 - val_loss: 1.0488 - val_accuracy: 0.5345\n",
      "Epoch 904/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4457 - accuracy: 0.7615 - val_loss: 0.9375 - val_accuracy: 0.5345\n",
      "Epoch 905/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4332 - accuracy: 0.7731 - val_loss: 1.0709 - val_accuracy: 0.5172\n",
      "Epoch 906/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4285 - accuracy: 0.7923 - val_loss: 0.9698 - val_accuracy: 0.5172\n",
      "Epoch 907/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4260 - accuracy: 0.7981 - val_loss: 0.9146 - val_accuracy: 0.5345\n",
      "Epoch 908/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4361 - accuracy: 0.7923 - val_loss: 0.9254 - val_accuracy: 0.5690\n",
      "Epoch 909/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4192 - accuracy: 0.7923 - val_loss: 1.0991 - val_accuracy: 0.5000\n",
      "Epoch 910/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4210 - accuracy: 0.7923 - val_loss: 1.0506 - val_accuracy: 0.5000\n",
      "Epoch 911/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4147 - accuracy: 0.8019 - val_loss: 0.9265 - val_accuracy: 0.5690\n",
      "Epoch 912/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4251 - accuracy: 0.7962 - val_loss: 0.8873 - val_accuracy: 0.5690\n",
      "Epoch 913/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4185 - accuracy: 0.7981 - val_loss: 0.9212 - val_accuracy: 0.5345\n",
      "Epoch 914/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4185 - accuracy: 0.7885 - val_loss: 0.8874 - val_accuracy: 0.5517\n",
      "Epoch 915/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.7942 - val_loss: 1.0120 - val_accuracy: 0.5000\n",
      "Epoch 916/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4159 - accuracy: 0.7904 - val_loss: 0.9374 - val_accuracy: 0.5517\n",
      "Epoch 917/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4097 - accuracy: 0.8096 - val_loss: 0.8849 - val_accuracy: 0.5690\n",
      "Epoch 918/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4226 - accuracy: 0.7981 - val_loss: 1.0057 - val_accuracy: 0.5172\n",
      "Epoch 919/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4687 - accuracy: 0.7692 - val_loss: 0.9489 - val_accuracy: 0.5345\n",
      "Epoch 920/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4237 - accuracy: 0.7904 - val_loss: 0.9391 - val_accuracy: 0.5345\n",
      "Epoch 921/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4212 - accuracy: 0.8000 - val_loss: 0.8963 - val_accuracy: 0.5862\n",
      "Epoch 922/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4133 - accuracy: 0.7962 - val_loss: 1.0533 - val_accuracy: 0.4828\n",
      "Epoch 923/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4127 - accuracy: 0.8058 - val_loss: 1.1239 - val_accuracy: 0.4828\n",
      "Epoch 924/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4214 - accuracy: 0.7942 - val_loss: 0.9895 - val_accuracy: 0.5345\n",
      "Epoch 925/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4164 - accuracy: 0.7981 - val_loss: 0.9688 - val_accuracy: 0.5172\n",
      "Epoch 926/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4229 - accuracy: 0.7923 - val_loss: 0.9993 - val_accuracy: 0.5000\n",
      "Epoch 927/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4185 - accuracy: 0.7846 - val_loss: 1.0603 - val_accuracy: 0.4828\n",
      "Epoch 928/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4269 - accuracy: 0.7942 - val_loss: 1.0813 - val_accuracy: 0.5000\n",
      "Epoch 929/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4220 - accuracy: 0.8000 - val_loss: 0.9995 - val_accuracy: 0.5517\n",
      "Epoch 930/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4143 - accuracy: 0.7923 - val_loss: 0.9962 - val_accuracy: 0.4828\n",
      "Epoch 931/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4151 - accuracy: 0.7923 - val_loss: 1.0617 - val_accuracy: 0.5172\n",
      "Epoch 932/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.7923 - val_loss: 1.1862 - val_accuracy: 0.5000\n",
      "Epoch 933/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4218 - accuracy: 0.7904 - val_loss: 1.1753 - val_accuracy: 0.4828\n",
      "Epoch 934/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4192 - accuracy: 0.8019 - val_loss: 0.9412 - val_accuracy: 0.5345\n",
      "Epoch 935/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4261 - accuracy: 0.7885 - val_loss: 0.9814 - val_accuracy: 0.5517\n",
      "Epoch 936/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4485 - accuracy: 0.7769 - val_loss: 1.0102 - val_accuracy: 0.5172\n",
      "Epoch 937/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.7942 - val_loss: 0.9647 - val_accuracy: 0.5000\n",
      "Epoch 938/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4152 - accuracy: 0.7923 - val_loss: 0.9681 - val_accuracy: 0.5172\n",
      "Epoch 939/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4097 - accuracy: 0.8096 - val_loss: 1.0514 - val_accuracy: 0.4828\n",
      "Epoch 940/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4343 - accuracy: 0.7865 - val_loss: 0.9339 - val_accuracy: 0.5690\n",
      "Epoch 941/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4163 - accuracy: 0.7846 - val_loss: 1.0512 - val_accuracy: 0.5000\n",
      "Epoch 942/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4119 - accuracy: 0.7981 - val_loss: 1.0444 - val_accuracy: 0.5000\n",
      "Epoch 943/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4311 - accuracy: 0.7981 - val_loss: 0.9412 - val_accuracy: 0.5690\n",
      "Epoch 944/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4398 - accuracy: 0.7865 - val_loss: 0.9990 - val_accuracy: 0.5000\n",
      "Epoch 945/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4167 - accuracy: 0.7942 - val_loss: 0.9435 - val_accuracy: 0.5345\n",
      "Epoch 946/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4072 - accuracy: 0.8096 - val_loss: 1.0090 - val_accuracy: 0.5000\n",
      "Epoch 947/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4124 - accuracy: 0.7962 - val_loss: 1.0810 - val_accuracy: 0.4828\n",
      "Epoch 948/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4096 - accuracy: 0.8038 - val_loss: 0.8709 - val_accuracy: 0.5517\n",
      "Epoch 949/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4172 - accuracy: 0.7942 - val_loss: 0.9891 - val_accuracy: 0.5172\n",
      "Epoch 950/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4102 - accuracy: 0.8058 - val_loss: 1.0243 - val_accuracy: 0.5517\n",
      "Epoch 951/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4056 - accuracy: 0.7923 - val_loss: 1.0441 - val_accuracy: 0.5000\n",
      "Epoch 952/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4134 - accuracy: 0.7981 - val_loss: 0.9788 - val_accuracy: 0.5172\n",
      "Epoch 953/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4093 - accuracy: 0.8000 - val_loss: 0.9149 - val_accuracy: 0.5345\n",
      "Epoch 954/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4102 - accuracy: 0.7962 - val_loss: 0.8149 - val_accuracy: 0.5517\n",
      "Epoch 955/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4154 - accuracy: 0.8019 - val_loss: 0.8623 - val_accuracy: 0.5862\n",
      "Epoch 956/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4173 - accuracy: 0.7962 - val_loss: 1.0194 - val_accuracy: 0.5345\n",
      "Epoch 957/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4606 - accuracy: 0.7827 - val_loss: 0.9987 - val_accuracy: 0.5345\n",
      "Epoch 958/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4685 - accuracy: 0.7788 - val_loss: 0.9178 - val_accuracy: 0.5345\n",
      "Epoch 959/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4232 - accuracy: 0.7923 - val_loss: 0.9641 - val_accuracy: 0.5345\n",
      "Epoch 960/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4210 - accuracy: 0.7885 - val_loss: 1.0915 - val_accuracy: 0.5000\n",
      "Epoch 961/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4059 - accuracy: 0.8058 - val_loss: 0.9946 - val_accuracy: 0.5172\n",
      "Epoch 962/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4058 - accuracy: 0.8019 - val_loss: 0.8504 - val_accuracy: 0.5862\n",
      "Epoch 963/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4092 - accuracy: 0.8019 - val_loss: 1.1040 - val_accuracy: 0.5172\n",
      "Epoch 964/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4180 - accuracy: 0.7865 - val_loss: 1.1365 - val_accuracy: 0.5000\n",
      "Epoch 965/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4140 - accuracy: 0.7981 - val_loss: 1.0271 - val_accuracy: 0.5172\n",
      "Epoch 966/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4132 - accuracy: 0.8038 - val_loss: 1.1505 - val_accuracy: 0.4828\n",
      "Epoch 967/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4169 - accuracy: 0.8077 - val_loss: 0.9124 - val_accuracy: 0.5345\n",
      "Epoch 968/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4192 - accuracy: 0.7942 - val_loss: 0.9627 - val_accuracy: 0.5000\n",
      "Epoch 969/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4104 - accuracy: 0.8115 - val_loss: 1.0598 - val_accuracy: 0.5000\n",
      "Epoch 970/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4107 - accuracy: 0.8135 - val_loss: 0.9511 - val_accuracy: 0.5345\n",
      "Epoch 971/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4105 - accuracy: 0.8019 - val_loss: 0.9417 - val_accuracy: 0.5517\n",
      "Epoch 972/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4509 - accuracy: 0.7692 - val_loss: 0.9320 - val_accuracy: 0.5517\n",
      "Epoch 973/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4322 - accuracy: 0.7885 - val_loss: 0.9121 - val_accuracy: 0.5345\n",
      "Epoch 974/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4093 - accuracy: 0.8019 - val_loss: 0.9681 - val_accuracy: 0.5690\n",
      "Epoch 975/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4190 - accuracy: 0.7885 - val_loss: 1.1317 - val_accuracy: 0.4828\n",
      "Epoch 976/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4080 - accuracy: 0.8115 - val_loss: 0.9888 - val_accuracy: 0.5517\n",
      "Epoch 977/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4120 - accuracy: 0.8019 - val_loss: 0.9807 - val_accuracy: 0.5690\n",
      "Epoch 978/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4110 - accuracy: 0.7942 - val_loss: 1.0611 - val_accuracy: 0.5172\n",
      "Epoch 979/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4061 - accuracy: 0.8096 - val_loss: 0.9696 - val_accuracy: 0.5172\n",
      "Epoch 980/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4082 - accuracy: 0.7942 - val_loss: 1.0316 - val_accuracy: 0.5000\n",
      "Epoch 981/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4176 - accuracy: 0.7923 - val_loss: 0.8576 - val_accuracy: 0.5690\n",
      "Epoch 982/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4386 - accuracy: 0.7769 - val_loss: 0.8726 - val_accuracy: 0.5862\n",
      "Epoch 983/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4097 - accuracy: 0.8154 - val_loss: 1.0123 - val_accuracy: 0.5517\n",
      "Epoch 984/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4122 - accuracy: 0.8096 - val_loss: 0.9634 - val_accuracy: 0.5690\n",
      "Epoch 985/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4120 - accuracy: 0.7865 - val_loss: 0.8779 - val_accuracy: 0.5517\n",
      "Epoch 986/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4093 - accuracy: 0.7962 - val_loss: 1.0225 - val_accuracy: 0.4828\n",
      "Epoch 987/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4150 - accuracy: 0.7942 - val_loss: 1.0599 - val_accuracy: 0.5000\n",
      "Epoch 988/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4130 - accuracy: 0.8019 - val_loss: 1.1025 - val_accuracy: 0.4828\n",
      "Epoch 989/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4151 - accuracy: 0.7923 - val_loss: 0.8586 - val_accuracy: 0.5690\n",
      "Epoch 990/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4099 - accuracy: 0.8000 - val_loss: 0.9136 - val_accuracy: 0.5345\n",
      "Epoch 991/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4217 - accuracy: 0.7885 - val_loss: 0.9764 - val_accuracy: 0.5172\n",
      "Epoch 992/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4127 - accuracy: 0.7846 - val_loss: 0.9630 - val_accuracy: 0.5690\n",
      "Epoch 993/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4150 - accuracy: 0.7865 - val_loss: 1.0091 - val_accuracy: 0.5172\n",
      "Epoch 994/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4154 - accuracy: 0.8019 - val_loss: 1.0363 - val_accuracy: 0.5517\n",
      "Epoch 995/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4155 - accuracy: 0.7981 - val_loss: 0.9451 - val_accuracy: 0.5517\n",
      "Epoch 996/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4116 - accuracy: 0.8038 - val_loss: 1.0062 - val_accuracy: 0.5345\n",
      "Epoch 997/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3996 - accuracy: 0.8135 - val_loss: 0.8979 - val_accuracy: 0.5517\n",
      "Epoch 998/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4099 - accuracy: 0.8000 - val_loss: 1.0108 - val_accuracy: 0.5690\n",
      "Epoch 999/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4143 - accuracy: 0.7885 - val_loss: 0.9527 - val_accuracy: 0.5172\n",
      "Epoch 1000/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4076 - accuracy: 0.8038 - val_loss: 1.0355 - val_accuracy: 0.5345\n",
      "Epoch 1001/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4234 - accuracy: 0.7808 - val_loss: 1.1563 - val_accuracy: 0.4828\n",
      "Epoch 1002/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4476 - accuracy: 0.7654 - val_loss: 0.9780 - val_accuracy: 0.5345\n",
      "Epoch 1003/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4277 - accuracy: 0.7885 - val_loss: 0.9621 - val_accuracy: 0.5517\n",
      "Epoch 1004/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4029 - accuracy: 0.7981 - val_loss: 1.2056 - val_accuracy: 0.5000\n",
      "Epoch 1005/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4108 - accuracy: 0.7846 - val_loss: 0.8686 - val_accuracy: 0.5517\n",
      "Epoch 1006/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4019 - accuracy: 0.8115 - val_loss: 1.2885 - val_accuracy: 0.5172\n",
      "Epoch 1007/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4144 - accuracy: 0.7981 - val_loss: 1.0040 - val_accuracy: 0.5517\n",
      "Epoch 1008/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4106 - accuracy: 0.7904 - val_loss: 1.1386 - val_accuracy: 0.5000\n",
      "Epoch 1009/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3994 - accuracy: 0.8058 - val_loss: 1.0919 - val_accuracy: 0.5000\n",
      "Epoch 1010/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4104 - accuracy: 0.7904 - val_loss: 0.9454 - val_accuracy: 0.5517\n",
      "Epoch 1011/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4107 - accuracy: 0.7904 - val_loss: 0.9812 - val_accuracy: 0.5345\n",
      "Epoch 1012/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4129 - accuracy: 0.8019 - val_loss: 1.1717 - val_accuracy: 0.5000\n",
      "Epoch 1013/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4104 - accuracy: 0.7981 - val_loss: 0.9437 - val_accuracy: 0.5517\n",
      "Epoch 1014/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4451 - accuracy: 0.7962 - val_loss: 0.9644 - val_accuracy: 0.5517\n",
      "Epoch 1015/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4218 - accuracy: 0.8000 - val_loss: 1.0590 - val_accuracy: 0.5172\n",
      "Epoch 1016/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4286 - accuracy: 0.7923 - val_loss: 0.9726 - val_accuracy: 0.5517\n",
      "Epoch 1017/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4081 - accuracy: 0.8019 - val_loss: 0.9633 - val_accuracy: 0.5517\n",
      "Epoch 1018/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4045 - accuracy: 0.7865 - val_loss: 0.9828 - val_accuracy: 0.5517\n",
      "Epoch 1019/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3999 - accuracy: 0.8096 - val_loss: 0.8879 - val_accuracy: 0.5345\n",
      "Epoch 1020/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4120 - accuracy: 0.7962 - val_loss: 1.0764 - val_accuracy: 0.5172\n",
      "Epoch 1021/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.7769 - val_loss: 1.1840 - val_accuracy: 0.5345\n",
      "Epoch 1022/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4091 - accuracy: 0.7865 - val_loss: 1.0230 - val_accuracy: 0.5690\n",
      "Epoch 1023/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4148 - accuracy: 0.7923 - val_loss: 0.9441 - val_accuracy: 0.5517\n",
      "Epoch 1024/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4022 - accuracy: 0.8096 - val_loss: 1.0306 - val_accuracy: 0.5517\n",
      "Epoch 1025/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4145 - accuracy: 0.7865 - val_loss: 1.0436 - val_accuracy: 0.5000\n",
      "Epoch 1026/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4058 - accuracy: 0.7885 - val_loss: 1.0809 - val_accuracy: 0.5690\n",
      "Epoch 1027/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4056 - accuracy: 0.7942 - val_loss: 1.1651 - val_accuracy: 0.4828\n",
      "Epoch 1028/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4140 - accuracy: 0.7942 - val_loss: 1.0712 - val_accuracy: 0.5517\n",
      "Epoch 1029/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4180 - accuracy: 0.7923 - val_loss: 0.9479 - val_accuracy: 0.5690\n",
      "Epoch 1030/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4064 - accuracy: 0.8000 - val_loss: 0.9607 - val_accuracy: 0.5345\n",
      "Epoch 1031/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4125 - accuracy: 0.7846 - val_loss: 1.0863 - val_accuracy: 0.5345\n",
      "Epoch 1032/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.7923 - val_loss: 0.8925 - val_accuracy: 0.6034\n",
      "Epoch 1033/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4041 - accuracy: 0.7981 - val_loss: 1.1253 - val_accuracy: 0.5172\n",
      "Epoch 1034/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4030 - accuracy: 0.7942 - val_loss: 1.0367 - val_accuracy: 0.5862\n",
      "Epoch 1035/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4077 - accuracy: 0.8000 - val_loss: 1.0571 - val_accuracy: 0.5517\n",
      "Epoch 1036/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.7885 - val_loss: 0.9334 - val_accuracy: 0.5862\n",
      "Epoch 1037/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4021 - accuracy: 0.7981 - val_loss: 1.1180 - val_accuracy: 0.5172\n",
      "Epoch 1038/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4110 - accuracy: 0.7827 - val_loss: 0.9924 - val_accuracy: 0.5517\n",
      "Epoch 1039/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4081 - accuracy: 0.7962 - val_loss: 1.0680 - val_accuracy: 0.5517\n",
      "Epoch 1040/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4098 - accuracy: 0.8038 - val_loss: 1.0310 - val_accuracy: 0.5517\n",
      "Epoch 1041/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4024 - accuracy: 0.8000 - val_loss: 1.0647 - val_accuracy: 0.5345\n",
      "Epoch 1042/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8096 - val_loss: 1.3522 - val_accuracy: 0.5000\n",
      "Epoch 1043/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.7923 - val_loss: 1.1576 - val_accuracy: 0.5517\n",
      "Epoch 1044/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4084 - accuracy: 0.7846 - val_loss: 1.0647 - val_accuracy: 0.5172\n",
      "Epoch 1045/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4103 - accuracy: 0.7846 - val_loss: 1.1127 - val_accuracy: 0.5000\n",
      "Epoch 1046/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4277 - accuracy: 0.7769 - val_loss: 1.0457 - val_accuracy: 0.5517\n",
      "Epoch 1047/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4065 - accuracy: 0.7962 - val_loss: 0.9954 - val_accuracy: 0.5517\n",
      "Epoch 1048/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4016 - accuracy: 0.8019 - val_loss: 0.9672 - val_accuracy: 0.5690\n",
      "Epoch 1049/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3970 - accuracy: 0.8058 - val_loss: 1.0581 - val_accuracy: 0.5517\n",
      "Epoch 1050/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3990 - accuracy: 0.8038 - val_loss: 1.1237 - val_accuracy: 0.5345\n",
      "Epoch 1051/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4051 - accuracy: 0.7885 - val_loss: 1.1359 - val_accuracy: 0.5000\n",
      "Epoch 1052/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3985 - accuracy: 0.8096 - val_loss: 1.1336 - val_accuracy: 0.5172\n",
      "Epoch 1053/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3929 - accuracy: 0.8096 - val_loss: 0.9668 - val_accuracy: 0.5345\n",
      "Epoch 1054/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4132 - accuracy: 0.8019 - val_loss: 1.0613 - val_accuracy: 0.5517\n",
      "Epoch 1055/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4004 - accuracy: 0.8115 - val_loss: 1.0110 - val_accuracy: 0.5517\n",
      "Epoch 1056/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3932 - accuracy: 0.8250 - val_loss: 0.9649 - val_accuracy: 0.5690\n",
      "Epoch 1057/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3922 - accuracy: 0.8115 - val_loss: 1.1975 - val_accuracy: 0.5345\n",
      "Epoch 1058/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4001 - accuracy: 0.8000 - val_loss: 1.3187 - val_accuracy: 0.4655\n",
      "Epoch 1059/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4272 - accuracy: 0.7846 - val_loss: 1.2393 - val_accuracy: 0.5000\n",
      "Epoch 1060/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.7904 - val_loss: 0.9732 - val_accuracy: 0.5517\n",
      "Epoch 1061/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4315 - accuracy: 0.7865 - val_loss: 1.1176 - val_accuracy: 0.5345\n",
      "Epoch 1062/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3996 - accuracy: 0.8000 - val_loss: 1.0258 - val_accuracy: 0.5517\n",
      "Epoch 1063/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3984 - accuracy: 0.8058 - val_loss: 1.0885 - val_accuracy: 0.5345\n",
      "Epoch 1064/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3956 - accuracy: 0.8058 - val_loss: 1.0633 - val_accuracy: 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1065/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3913 - accuracy: 0.7962 - val_loss: 1.0647 - val_accuracy: 0.5517\n",
      "Epoch 1066/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3980 - accuracy: 0.8058 - val_loss: 1.0580 - val_accuracy: 0.5000\n",
      "Epoch 1067/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3927 - accuracy: 0.8019 - val_loss: 1.1430 - val_accuracy: 0.5345\n",
      "Epoch 1068/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4060 - accuracy: 0.7827 - val_loss: 1.0572 - val_accuracy: 0.5517\n",
      "Epoch 1069/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4280 - accuracy: 0.7808 - val_loss: 1.0512 - val_accuracy: 0.5690\n",
      "Epoch 1070/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3961 - accuracy: 0.8096 - val_loss: 1.0560 - val_accuracy: 0.5690\n",
      "Epoch 1071/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4040 - accuracy: 0.7962 - val_loss: 1.0181 - val_accuracy: 0.5517\n",
      "Epoch 1072/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3966 - accuracy: 0.8038 - val_loss: 1.0725 - val_accuracy: 0.5517\n",
      "Epoch 1073/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3955 - accuracy: 0.8038 - val_loss: 1.0479 - val_accuracy: 0.5690\n",
      "Epoch 1074/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3909 - accuracy: 0.8000 - val_loss: 0.9822 - val_accuracy: 0.5862\n",
      "Epoch 1075/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3922 - accuracy: 0.8096 - val_loss: 1.1082 - val_accuracy: 0.5345\n",
      "Epoch 1076/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3936 - accuracy: 0.8115 - val_loss: 1.0197 - val_accuracy: 0.5690\n",
      "Epoch 1077/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3806 - accuracy: 0.8038 - val_loss: 1.2460 - val_accuracy: 0.5172\n",
      "Epoch 1078/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4012 - accuracy: 0.8058 - val_loss: 1.2051 - val_accuracy: 0.5172\n",
      "Epoch 1079/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4341 - accuracy: 0.7904 - val_loss: 1.2125 - val_accuracy: 0.5172\n",
      "Epoch 1080/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4051 - accuracy: 0.7923 - val_loss: 1.0927 - val_accuracy: 0.5517\n",
      "Epoch 1081/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4044 - accuracy: 0.7981 - val_loss: 1.1207 - val_accuracy: 0.5517\n",
      "Epoch 1082/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3996 - accuracy: 0.8096 - val_loss: 1.0404 - val_accuracy: 0.5345\n",
      "Epoch 1083/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3901 - accuracy: 0.8192 - val_loss: 1.0688 - val_accuracy: 0.5172\n",
      "Epoch 1084/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4089 - accuracy: 0.7981 - val_loss: 1.0700 - val_accuracy: 0.5517\n",
      "Epoch 1085/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3964 - accuracy: 0.7942 - val_loss: 1.2573 - val_accuracy: 0.5172\n",
      "Epoch 1086/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3949 - accuracy: 0.8077 - val_loss: 0.9219 - val_accuracy: 0.6034\n",
      "Epoch 1087/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3943 - accuracy: 0.7981 - val_loss: 0.9980 - val_accuracy: 0.5690\n",
      "Epoch 1088/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3866 - accuracy: 0.8019 - val_loss: 1.0485 - val_accuracy: 0.5172\n",
      "Epoch 1089/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3918 - accuracy: 0.7981 - val_loss: 1.2151 - val_accuracy: 0.5345\n",
      "Epoch 1090/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3872 - accuracy: 0.8000 - val_loss: 1.1222 - val_accuracy: 0.5172\n",
      "Epoch 1091/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3948 - accuracy: 0.8096 - val_loss: 1.1423 - val_accuracy: 0.4828\n",
      "Epoch 1092/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4312 - accuracy: 0.7981 - val_loss: 1.1262 - val_accuracy: 0.5000\n",
      "Epoch 1093/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4268 - accuracy: 0.7981 - val_loss: 1.2877 - val_accuracy: 0.5172\n",
      "Epoch 1094/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4073 - accuracy: 0.7923 - val_loss: 1.0337 - val_accuracy: 0.5517\n",
      "Epoch 1095/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3941 - accuracy: 0.8058 - val_loss: 1.0134 - val_accuracy: 0.5862\n",
      "Epoch 1096/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3935 - accuracy: 0.8000 - val_loss: 0.9914 - val_accuracy: 0.5517\n",
      "Epoch 1097/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3900 - accuracy: 0.8096 - val_loss: 1.1405 - val_accuracy: 0.5172\n",
      "Epoch 1098/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3966 - accuracy: 0.8077 - val_loss: 1.0750 - val_accuracy: 0.5862\n",
      "Epoch 1099/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3897 - accuracy: 0.8000 - val_loss: 1.1131 - val_accuracy: 0.5172\n",
      "Epoch 1100/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3883 - accuracy: 0.8019 - val_loss: 1.0453 - val_accuracy: 0.5690\n",
      "Epoch 1101/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3938 - accuracy: 0.8019 - val_loss: 1.1171 - val_accuracy: 0.4828\n",
      "Epoch 1102/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3904 - accuracy: 0.7962 - val_loss: 1.0361 - val_accuracy: 0.5517\n",
      "Epoch 1103/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3852 - accuracy: 0.8154 - val_loss: 0.9939 - val_accuracy: 0.5690\n",
      "Epoch 1104/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3965 - accuracy: 0.7981 - val_loss: 1.0978 - val_accuracy: 0.5862\n",
      "Epoch 1105/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4091 - accuracy: 0.7981 - val_loss: 1.0845 - val_accuracy: 0.5345\n",
      "Epoch 1106/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3829 - accuracy: 0.8115 - val_loss: 1.3162 - val_accuracy: 0.5000\n",
      "Epoch 1107/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3856 - accuracy: 0.8058 - val_loss: 1.2751 - val_accuracy: 0.5000\n",
      "Epoch 1108/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3861 - accuracy: 0.8077 - val_loss: 1.1366 - val_accuracy: 0.4828\n",
      "Epoch 1109/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3976 - accuracy: 0.8000 - val_loss: 1.1242 - val_accuracy: 0.5517\n",
      "Epoch 1110/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4025 - accuracy: 0.7846 - val_loss: 1.1143 - val_accuracy: 0.5345\n",
      "Epoch 1111/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3865 - accuracy: 0.8019 - val_loss: 1.0441 - val_accuracy: 0.5862\n",
      "Epoch 1112/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3866 - accuracy: 0.8096 - val_loss: 1.0515 - val_accuracy: 0.6034\n",
      "Epoch 1113/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3851 - accuracy: 0.8096 - val_loss: 1.0428 - val_accuracy: 0.5690\n",
      "Epoch 1114/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3800 - accuracy: 0.8077 - val_loss: 1.1576 - val_accuracy: 0.5517\n",
      "Epoch 1115/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3948 - accuracy: 0.7962 - val_loss: 1.1127 - val_accuracy: 0.5517\n",
      "Epoch 1116/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8096 - val_loss: 1.1140 - val_accuracy: 0.5517\n",
      "Epoch 1117/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4132 - accuracy: 0.7962 - val_loss: 1.0767 - val_accuracy: 0.5345\n",
      "Epoch 1118/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3981 - accuracy: 0.8154 - val_loss: 1.1152 - val_accuracy: 0.5690\n",
      "Epoch 1119/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3833 - accuracy: 0.8096 - val_loss: 1.0201 - val_accuracy: 0.5517\n",
      "Epoch 1120/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3878 - accuracy: 0.8058 - val_loss: 1.2192 - val_accuracy: 0.5172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1121/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3854 - accuracy: 0.8077 - val_loss: 1.1612 - val_accuracy: 0.5517\n",
      "Epoch 1122/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3868 - accuracy: 0.8115 - val_loss: 1.2090 - val_accuracy: 0.5862\n",
      "Epoch 1123/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3837 - accuracy: 0.8038 - val_loss: 1.1376 - val_accuracy: 0.5517\n",
      "Epoch 1124/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3857 - accuracy: 0.8038 - val_loss: 1.1458 - val_accuracy: 0.5172\n",
      "Epoch 1125/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3909 - accuracy: 0.8000 - val_loss: 1.0910 - val_accuracy: 0.5517\n",
      "Epoch 1126/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.7981 - val_loss: 1.0986 - val_accuracy: 0.5172\n",
      "Epoch 1127/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3983 - accuracy: 0.8038 - val_loss: 1.1011 - val_accuracy: 0.5862\n",
      "Epoch 1128/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4002 - accuracy: 0.8058 - val_loss: 1.1375 - val_accuracy: 0.5690\n",
      "Epoch 1129/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3859 - accuracy: 0.8173 - val_loss: 1.1420 - val_accuracy: 0.5690\n",
      "Epoch 1130/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3760 - accuracy: 0.8308 - val_loss: 1.0335 - val_accuracy: 0.5517\n",
      "Epoch 1131/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3811 - accuracy: 0.8077 - val_loss: 1.3390 - val_accuracy: 0.5345\n",
      "Epoch 1132/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3924 - accuracy: 0.8000 - val_loss: 1.0317 - val_accuracy: 0.5690\n",
      "Epoch 1133/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3838 - accuracy: 0.8154 - val_loss: 1.0939 - val_accuracy: 0.6034\n",
      "Epoch 1134/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3942 - accuracy: 0.7962 - val_loss: 1.0723 - val_accuracy: 0.5862\n",
      "Epoch 1135/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3921 - accuracy: 0.7904 - val_loss: 1.1893 - val_accuracy: 0.5172\n",
      "Epoch 1136/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3786 - accuracy: 0.8077 - val_loss: 1.2927 - val_accuracy: 0.5172\n",
      "Epoch 1137/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3895 - accuracy: 0.8077 - val_loss: 1.0464 - val_accuracy: 0.6034\n",
      "Epoch 1138/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3829 - accuracy: 0.8058 - val_loss: 1.1322 - val_accuracy: 0.5690\n",
      "Epoch 1139/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3910 - accuracy: 0.8038 - val_loss: 1.2504 - val_accuracy: 0.5690\n",
      "Epoch 1140/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4106 - accuracy: 0.7923 - val_loss: 1.0645 - val_accuracy: 0.5690\n",
      "Epoch 1141/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3935 - accuracy: 0.7962 - val_loss: 1.0840 - val_accuracy: 0.5862\n",
      "Epoch 1142/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3851 - accuracy: 0.8115 - val_loss: 1.3479 - val_accuracy: 0.5172\n",
      "Epoch 1143/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4040 - accuracy: 0.8000 - val_loss: 1.0115 - val_accuracy: 0.5345\n",
      "Epoch 1144/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3891 - accuracy: 0.8077 - val_loss: 1.2295 - val_accuracy: 0.5000\n",
      "Epoch 1145/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3765 - accuracy: 0.8154 - val_loss: 1.0471 - val_accuracy: 0.5690\n",
      "Epoch 1146/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8038 - val_loss: 1.1503 - val_accuracy: 0.5172\n",
      "Epoch 1147/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3767 - accuracy: 0.8115 - val_loss: 1.2167 - val_accuracy: 0.5172\n",
      "Epoch 1148/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3992 - accuracy: 0.8212 - val_loss: 1.3193 - val_accuracy: 0.5172\n",
      "Epoch 1149/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3817 - accuracy: 0.8115 - val_loss: 1.0360 - val_accuracy: 0.5862\n",
      "Epoch 1150/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3962 - accuracy: 0.8096 - val_loss: 1.1629 - val_accuracy: 0.5345\n",
      "Epoch 1151/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3693 - accuracy: 0.8135 - val_loss: 1.0464 - val_accuracy: 0.5690\n",
      "Epoch 1152/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3827 - accuracy: 0.8154 - val_loss: 1.2302 - val_accuracy: 0.5690\n",
      "Epoch 1153/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4050 - accuracy: 0.7981 - val_loss: 1.2012 - val_accuracy: 0.5345\n",
      "Epoch 1154/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3932 - accuracy: 0.8058 - val_loss: 1.1316 - val_accuracy: 0.5345\n",
      "Epoch 1155/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3975 - accuracy: 0.8038 - val_loss: 1.1339 - val_accuracy: 0.5690\n",
      "Epoch 1156/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3840 - accuracy: 0.8058 - val_loss: 1.1533 - val_accuracy: 0.5172\n",
      "Epoch 1157/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3847 - accuracy: 0.8096 - val_loss: 1.1905 - val_accuracy: 0.5345\n",
      "Epoch 1158/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3797 - accuracy: 0.8000 - val_loss: 1.0183 - val_accuracy: 0.5862\n",
      "Epoch 1159/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3774 - accuracy: 0.8231 - val_loss: 1.3494 - val_accuracy: 0.5172\n",
      "Epoch 1160/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3781 - accuracy: 0.8077 - val_loss: 1.2286 - val_accuracy: 0.5345\n",
      "Epoch 1161/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3935 - accuracy: 0.8115 - val_loss: 1.1132 - val_accuracy: 0.5172\n",
      "Epoch 1162/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3814 - accuracy: 0.8154 - val_loss: 1.2339 - val_accuracy: 0.5345\n",
      "Epoch 1163/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3796 - accuracy: 0.8115 - val_loss: 1.1565 - val_accuracy: 0.5690\n",
      "Epoch 1164/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3931 - accuracy: 0.7923 - val_loss: 1.1430 - val_accuracy: 0.5345\n",
      "Epoch 1165/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3918 - accuracy: 0.8192 - val_loss: 1.3180 - val_accuracy: 0.5000\n",
      "Epoch 1166/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3967 - accuracy: 0.7962 - val_loss: 1.0014 - val_accuracy: 0.6034\n",
      "Epoch 1167/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3800 - accuracy: 0.8115 - val_loss: 1.2669 - val_accuracy: 0.5172\n",
      "Epoch 1168/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3814 - accuracy: 0.8096 - val_loss: 1.1257 - val_accuracy: 0.5862\n",
      "Epoch 1169/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3950 - accuracy: 0.8077 - val_loss: 1.3198 - val_accuracy: 0.5000\n",
      "Epoch 1170/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3869 - accuracy: 0.8096 - val_loss: 1.1463 - val_accuracy: 0.4828\n",
      "Epoch 1171/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3828 - accuracy: 0.8096 - val_loss: 1.2944 - val_accuracy: 0.4828\n",
      "Epoch 1172/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3784 - accuracy: 0.8154 - val_loss: 1.1714 - val_accuracy: 0.5690\n",
      "Epoch 1173/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3796 - accuracy: 0.8212 - val_loss: 1.2360 - val_accuracy: 0.5345\n",
      "Epoch 1174/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3828 - accuracy: 0.8096 - val_loss: 1.1157 - val_accuracy: 0.5690\n",
      "Epoch 1175/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3697 - accuracy: 0.8173 - val_loss: 1.1676 - val_accuracy: 0.5517\n",
      "Epoch 1176/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3739 - accuracy: 0.8115 - val_loss: 1.2322 - val_accuracy: 0.5172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1177/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3739 - accuracy: 0.8000 - val_loss: 1.0803 - val_accuracy: 0.5862\n",
      "Epoch 1178/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3970 - accuracy: 0.7962 - val_loss: 1.0275 - val_accuracy: 0.5690\n",
      "Epoch 1179/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4032 - accuracy: 0.8058 - val_loss: 1.1906 - val_accuracy: 0.5345\n",
      "Epoch 1180/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3886 - accuracy: 0.8038 - val_loss: 1.1303 - val_accuracy: 0.5517\n",
      "Epoch 1181/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3868 - accuracy: 0.8115 - val_loss: 1.2427 - val_accuracy: 0.5345\n",
      "Epoch 1182/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3932 - accuracy: 0.8038 - val_loss: 1.1841 - val_accuracy: 0.5517\n",
      "Epoch 1183/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4089 - accuracy: 0.8115 - val_loss: 1.3830 - val_accuracy: 0.5000\n",
      "Epoch 1184/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4117 - accuracy: 0.7788 - val_loss: 1.3147 - val_accuracy: 0.5345\n",
      "Epoch 1185/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3822 - accuracy: 0.8231 - val_loss: 1.1226 - val_accuracy: 0.5690\n",
      "Epoch 1186/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3787 - accuracy: 0.8154 - val_loss: 1.1913 - val_accuracy: 0.5690\n",
      "Epoch 1187/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3742 - accuracy: 0.8038 - val_loss: 1.1944 - val_accuracy: 0.5690\n",
      "Epoch 1188/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3795 - accuracy: 0.8173 - val_loss: 1.2827 - val_accuracy: 0.5690\n",
      "Epoch 1189/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3783 - accuracy: 0.8135 - val_loss: 1.4645 - val_accuracy: 0.4828\n",
      "Epoch 1190/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3845 - accuracy: 0.8000 - val_loss: 1.2062 - val_accuracy: 0.5517\n",
      "Epoch 1191/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3745 - accuracy: 0.8115 - val_loss: 1.3395 - val_accuracy: 0.5000\n",
      "Epoch 1192/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3684 - accuracy: 0.8135 - val_loss: 1.1279 - val_accuracy: 0.5690\n",
      "Epoch 1193/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3823 - accuracy: 0.8096 - val_loss: 1.1653 - val_accuracy: 0.5517\n",
      "Epoch 1194/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3832 - accuracy: 0.7981 - val_loss: 1.1559 - val_accuracy: 0.5345\n",
      "Epoch 1195/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3794 - accuracy: 0.8096 - val_loss: 1.3334 - val_accuracy: 0.5345\n",
      "Epoch 1196/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3884 - accuracy: 0.8096 - val_loss: 1.2643 - val_accuracy: 0.5345\n",
      "Epoch 1197/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3901 - accuracy: 0.8058 - val_loss: 1.2017 - val_accuracy: 0.5517\n",
      "Epoch 1198/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3823 - accuracy: 0.8115 - val_loss: 1.1576 - val_accuracy: 0.5517\n",
      "Epoch 1199/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3788 - accuracy: 0.8192 - val_loss: 1.0119 - val_accuracy: 0.6207\n",
      "Epoch 1200/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3821 - accuracy: 0.8058 - val_loss: 1.2166 - val_accuracy: 0.5345\n",
      "Epoch 1201/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3774 - accuracy: 0.8135 - val_loss: 1.0935 - val_accuracy: 0.5862\n",
      "Epoch 1202/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3839 - accuracy: 0.8212 - val_loss: 1.3798 - val_accuracy: 0.4828\n",
      "Epoch 1203/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3908 - accuracy: 0.8135 - val_loss: 1.1004 - val_accuracy: 0.6034\n",
      "Epoch 1204/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3752 - accuracy: 0.8173 - val_loss: 1.2734 - val_accuracy: 0.5345\n",
      "Epoch 1205/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3726 - accuracy: 0.8231 - val_loss: 1.3119 - val_accuracy: 0.5690\n",
      "Epoch 1206/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3844 - accuracy: 0.8058 - val_loss: 1.1703 - val_accuracy: 0.5517\n",
      "Epoch 1207/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3815 - accuracy: 0.8115 - val_loss: 1.1324 - val_accuracy: 0.5862\n",
      "Epoch 1208/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3719 - accuracy: 0.8173 - val_loss: 1.2078 - val_accuracy: 0.5517\n",
      "Epoch 1209/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3703 - accuracy: 0.8192 - val_loss: 1.2838 - val_accuracy: 0.5172\n",
      "Epoch 1210/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3766 - accuracy: 0.8135 - val_loss: 1.1097 - val_accuracy: 0.5862\n",
      "Epoch 1211/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4092 - accuracy: 0.7923 - val_loss: 1.2376 - val_accuracy: 0.5000\n",
      "Epoch 1212/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4036 - accuracy: 0.7904 - val_loss: 1.2423 - val_accuracy: 0.5172\n",
      "Epoch 1213/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4014 - accuracy: 0.8115 - val_loss: 1.1726 - val_accuracy: 0.5862\n",
      "Epoch 1214/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3906 - accuracy: 0.8038 - val_loss: 1.1226 - val_accuracy: 0.5862\n",
      "Epoch 1215/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4244 - accuracy: 0.7962 - val_loss: 1.4318 - val_accuracy: 0.4828\n",
      "Epoch 1216/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4717 - accuracy: 0.7865 - val_loss: 1.0706 - val_accuracy: 0.5517\n",
      "Epoch 1217/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4347 - accuracy: 0.8019 - val_loss: 1.1951 - val_accuracy: 0.5690\n",
      "Epoch 1218/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4165 - accuracy: 0.8154 - val_loss: 1.2988 - val_accuracy: 0.5345\n",
      "Epoch 1219/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3868 - accuracy: 0.8212 - val_loss: 0.9968 - val_accuracy: 0.6207\n",
      "Epoch 1220/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3937 - accuracy: 0.8019 - val_loss: 1.1244 - val_accuracy: 0.5690\n",
      "Epoch 1221/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3826 - accuracy: 0.8154 - val_loss: 1.3415 - val_accuracy: 0.4828\n",
      "Epoch 1222/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3848 - accuracy: 0.8231 - val_loss: 1.1597 - val_accuracy: 0.5517\n",
      "Epoch 1223/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3839 - accuracy: 0.8173 - val_loss: 1.0619 - val_accuracy: 0.5862\n",
      "Epoch 1224/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3922 - accuracy: 0.8173 - val_loss: 1.1707 - val_accuracy: 0.5690\n",
      "Epoch 1225/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3833 - accuracy: 0.8096 - val_loss: 1.1949 - val_accuracy: 0.5345\n",
      "Epoch 1226/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3913 - accuracy: 0.8077 - val_loss: 1.2074 - val_accuracy: 0.5345\n",
      "Epoch 1227/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4065 - accuracy: 0.8096 - val_loss: 1.2673 - val_accuracy: 0.5517\n",
      "Epoch 1228/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4648 - accuracy: 0.7827 - val_loss: 0.9626 - val_accuracy: 0.5862\n",
      "Epoch 1229/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4478 - accuracy: 0.8038 - val_loss: 1.1619 - val_accuracy: 0.5517\n",
      "Epoch 1230/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4360 - accuracy: 0.8173 - val_loss: 1.2863 - val_accuracy: 0.5345\n",
      "Epoch 1231/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4269 - accuracy: 0.8077 - val_loss: 1.1279 - val_accuracy: 0.5862\n",
      "Epoch 1232/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4343 - accuracy: 0.8019 - val_loss: 1.1926 - val_accuracy: 0.5862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1233/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4322 - accuracy: 0.8077 - val_loss: 1.2292 - val_accuracy: 0.5345\n",
      "Epoch 1234/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4114 - accuracy: 0.8192 - val_loss: 1.4242 - val_accuracy: 0.4828\n",
      "Epoch 1235/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4148 - accuracy: 0.8000 - val_loss: 1.2931 - val_accuracy: 0.5690\n",
      "Epoch 1236/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4132 - accuracy: 0.8115 - val_loss: 1.4228 - val_accuracy: 0.4828\n",
      "Epoch 1237/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4096 - accuracy: 0.8038 - val_loss: 1.2884 - val_accuracy: 0.5000\n",
      "Epoch 1238/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4113 - accuracy: 0.8038 - val_loss: 1.2517 - val_accuracy: 0.5345\n",
      "Epoch 1239/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4145 - accuracy: 0.8077 - val_loss: 1.2757 - val_accuracy: 0.5172\n",
      "Epoch 1240/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4504 - accuracy: 0.7923 - val_loss: 1.4298 - val_accuracy: 0.5690\n",
      "Epoch 1241/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4209 - accuracy: 0.8058 - val_loss: 1.4189 - val_accuracy: 0.4828\n",
      "Epoch 1242/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4335 - accuracy: 0.8038 - val_loss: 1.4327 - val_accuracy: 0.5000\n",
      "Epoch 1243/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4192 - accuracy: 0.8096 - val_loss: 1.3389 - val_accuracy: 0.5690\n",
      "Epoch 1244/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4185 - accuracy: 0.8058 - val_loss: 1.3504 - val_accuracy: 0.5000\n",
      "Epoch 1245/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4153 - accuracy: 0.8058 - val_loss: 1.4162 - val_accuracy: 0.5517\n",
      "Epoch 1246/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4108 - accuracy: 0.8038 - val_loss: 1.4353 - val_accuracy: 0.5172\n",
      "Epoch 1247/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4064 - accuracy: 0.8154 - val_loss: 1.3817 - val_accuracy: 0.5172\n",
      "Epoch 1248/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4087 - accuracy: 0.8115 - val_loss: 1.4056 - val_accuracy: 0.5690\n",
      "Epoch 1249/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4032 - accuracy: 0.8192 - val_loss: 1.2201 - val_accuracy: 0.5517\n",
      "Epoch 1250/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4158 - accuracy: 0.8038 - val_loss: 1.5283 - val_accuracy: 0.5690\n",
      "Epoch 1251/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4155 - accuracy: 0.7962 - val_loss: 1.3260 - val_accuracy: 0.5690\n",
      "Epoch 1252/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4193 - accuracy: 0.7981 - val_loss: 1.4916 - val_accuracy: 0.5345\n",
      "Epoch 1253/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4133 - accuracy: 0.8038 - val_loss: 1.3786 - val_accuracy: 0.5345\n",
      "Epoch 1254/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4029 - accuracy: 0.8173 - val_loss: 1.3107 - val_accuracy: 0.5690\n",
      "Epoch 1255/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4243 - accuracy: 0.8038 - val_loss: 1.4980 - val_accuracy: 0.5517\n",
      "Epoch 1256/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4224 - accuracy: 0.8077 - val_loss: 1.2490 - val_accuracy: 0.5172\n",
      "Epoch 1257/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4079 - accuracy: 0.8212 - val_loss: 1.6862 - val_accuracy: 0.4828\n",
      "Epoch 1258/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4138 - accuracy: 0.8058 - val_loss: 1.2765 - val_accuracy: 0.5690\n",
      "Epoch 1259/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4032 - accuracy: 0.8058 - val_loss: 1.2807 - val_accuracy: 0.5690\n",
      "Epoch 1260/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4068 - accuracy: 0.8135 - val_loss: 1.5030 - val_accuracy: 0.5000\n",
      "Epoch 1261/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4218 - accuracy: 0.8038 - val_loss: 1.2905 - val_accuracy: 0.5690\n",
      "Epoch 1262/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8192 - val_loss: 1.3207 - val_accuracy: 0.5690\n",
      "Epoch 1263/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3998 - accuracy: 0.8154 - val_loss: 1.5335 - val_accuracy: 0.4828\n",
      "Epoch 1264/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4062 - accuracy: 0.8038 - val_loss: 1.3739 - val_accuracy: 0.5690\n",
      "Epoch 1265/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4049 - accuracy: 0.8077 - val_loss: 1.5008 - val_accuracy: 0.5517\n",
      "Epoch 1266/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4033 - accuracy: 0.8019 - val_loss: 1.5623 - val_accuracy: 0.5517\n",
      "Epoch 1267/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4100 - accuracy: 0.8096 - val_loss: 1.3625 - val_accuracy: 0.5517\n",
      "Epoch 1268/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4077 - accuracy: 0.8096 - val_loss: 1.3852 - val_accuracy: 0.5690\n",
      "Epoch 1269/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4105 - accuracy: 0.8096 - val_loss: 1.2819 - val_accuracy: 0.5345\n",
      "Epoch 1270/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4054 - accuracy: 0.8115 - val_loss: 1.3601 - val_accuracy: 0.5690\n",
      "Epoch 1271/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4020 - accuracy: 0.8135 - val_loss: 1.4191 - val_accuracy: 0.5690\n",
      "Epoch 1272/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4085 - accuracy: 0.8115 - val_loss: 1.3711 - val_accuracy: 0.5517\n",
      "Epoch 1273/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4454 - accuracy: 0.7962 - val_loss: 1.2785 - val_accuracy: 0.5517\n",
      "Epoch 1274/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4084 - accuracy: 0.8058 - val_loss: 1.4638 - val_accuracy: 0.5517\n",
      "Epoch 1275/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4211 - accuracy: 0.8019 - val_loss: 1.4407 - val_accuracy: 0.4828\n",
      "Epoch 1276/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3864 - accuracy: 0.8212 - val_loss: 1.7021 - val_accuracy: 0.4655\n",
      "Epoch 1277/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3949 - accuracy: 0.8154 - val_loss: 1.4869 - val_accuracy: 0.4828\n",
      "Epoch 1278/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3949 - accuracy: 0.8154 - val_loss: 1.3992 - val_accuracy: 0.5345\n",
      "Epoch 1279/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3970 - accuracy: 0.8135 - val_loss: 1.4162 - val_accuracy: 0.5690\n",
      "Epoch 1280/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4034 - accuracy: 0.8058 - val_loss: 1.5196 - val_accuracy: 0.5690\n",
      "Epoch 1281/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4124 - accuracy: 0.8115 - val_loss: 1.4449 - val_accuracy: 0.5000\n",
      "Epoch 1282/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4210 - accuracy: 0.7846 - val_loss: 1.3792 - val_accuracy: 0.5690\n",
      "Epoch 1283/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4016 - accuracy: 0.8115 - val_loss: 1.3116 - val_accuracy: 0.5862\n",
      "Epoch 1284/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3922 - accuracy: 0.8212 - val_loss: 1.4313 - val_accuracy: 0.5517\n",
      "Epoch 1285/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4137 - accuracy: 0.8173 - val_loss: 1.4026 - val_accuracy: 0.5690\n",
      "Epoch 1286/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4027 - accuracy: 0.8096 - val_loss: 1.3802 - val_accuracy: 0.5345\n",
      "Epoch 1287/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3952 - accuracy: 0.8077 - val_loss: 1.4492 - val_accuracy: 0.5517\n",
      "Epoch 1288/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3973 - accuracy: 0.8135 - val_loss: 1.2971 - val_accuracy: 0.5862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1289/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4035 - accuracy: 0.8019 - val_loss: 1.6151 - val_accuracy: 0.4828\n",
      "Epoch 1290/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3918 - accuracy: 0.8019 - val_loss: 1.3734 - val_accuracy: 0.5690\n",
      "Epoch 1291/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3937 - accuracy: 0.8038 - val_loss: 1.4491 - val_accuracy: 0.5345\n",
      "Epoch 1292/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4086 - accuracy: 0.7904 - val_loss: 1.5988 - val_accuracy: 0.5690\n",
      "Epoch 1293/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4350 - accuracy: 0.8077 - val_loss: 1.4839 - val_accuracy: 0.5172\n",
      "Epoch 1294/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4258 - accuracy: 0.7885 - val_loss: 1.4253 - val_accuracy: 0.5690\n",
      "Epoch 1295/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3965 - accuracy: 0.8192 - val_loss: 1.6232 - val_accuracy: 0.5000\n",
      "Epoch 1296/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3931 - accuracy: 0.8154 - val_loss: 1.5532 - val_accuracy: 0.5000\n",
      "Epoch 1297/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3977 - accuracy: 0.7942 - val_loss: 1.5400 - val_accuracy: 0.4828\n",
      "Epoch 1298/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3907 - accuracy: 0.8154 - val_loss: 1.5032 - val_accuracy: 0.5345\n",
      "Epoch 1299/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4147 - accuracy: 0.8096 - val_loss: 1.3801 - val_accuracy: 0.5690\n",
      "Epoch 1300/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.8096 - val_loss: 1.4728 - val_accuracy: 0.5517\n",
      "Epoch 1301/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3942 - accuracy: 0.8077 - val_loss: 1.4927 - val_accuracy: 0.5345\n",
      "Epoch 1302/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3939 - accuracy: 0.8192 - val_loss: 1.4402 - val_accuracy: 0.5345\n",
      "Epoch 1303/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3905 - accuracy: 0.8250 - val_loss: 1.4096 - val_accuracy: 0.5690\n",
      "Epoch 1304/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4060 - accuracy: 0.7942 - val_loss: 1.3497 - val_accuracy: 0.5345\n",
      "Epoch 1305/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4181 - accuracy: 0.7923 - val_loss: 1.4293 - val_accuracy: 0.5172\n",
      "Epoch 1306/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4113 - accuracy: 0.8135 - val_loss: 1.4022 - val_accuracy: 0.5690\n",
      "Epoch 1307/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3872 - accuracy: 0.8250 - val_loss: 1.4833 - val_accuracy: 0.5172\n",
      "Epoch 1308/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3932 - accuracy: 0.8173 - val_loss: 1.3212 - val_accuracy: 0.5690\n",
      "Epoch 1309/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3862 - accuracy: 0.8269 - val_loss: 1.3859 - val_accuracy: 0.5172\n",
      "Epoch 1310/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4031 - accuracy: 0.8135 - val_loss: 1.2405 - val_accuracy: 0.5690\n",
      "Epoch 1311/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4015 - accuracy: 0.8096 - val_loss: 1.5983 - val_accuracy: 0.5345\n",
      "Epoch 1312/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3955 - accuracy: 0.8135 - val_loss: 1.3360 - val_accuracy: 0.5690\n",
      "Epoch 1313/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3929 - accuracy: 0.8096 - val_loss: 1.7061 - val_accuracy: 0.4828\n",
      "Epoch 1314/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3859 - accuracy: 0.8250 - val_loss: 1.5489 - val_accuracy: 0.5172\n",
      "Epoch 1315/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3884 - accuracy: 0.8173 - val_loss: 1.3851 - val_accuracy: 0.5690\n",
      "Epoch 1316/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3888 - accuracy: 0.8077 - val_loss: 1.6223 - val_accuracy: 0.4828\n",
      "Epoch 1317/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4494 - accuracy: 0.7731 - val_loss: 1.5240 - val_accuracy: 0.5172\n",
      "Epoch 1318/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4136 - accuracy: 0.7981 - val_loss: 1.4821 - val_accuracy: 0.5345\n",
      "Epoch 1319/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3948 - accuracy: 0.8058 - val_loss: 1.5414 - val_accuracy: 0.5172\n",
      "Epoch 1320/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3974 - accuracy: 0.8115 - val_loss: 1.5265 - val_accuracy: 0.5690\n",
      "Epoch 1321/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3926 - accuracy: 0.8231 - val_loss: 1.3014 - val_accuracy: 0.5690\n",
      "Epoch 1322/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4044 - accuracy: 0.8038 - val_loss: 1.3394 - val_accuracy: 0.5690\n",
      "Epoch 1323/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3922 - accuracy: 0.8077 - val_loss: 1.4322 - val_accuracy: 0.4828\n",
      "Epoch 1324/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3881 - accuracy: 0.8212 - val_loss: 1.4450 - val_accuracy: 0.5172\n",
      "Epoch 1325/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3945 - accuracy: 0.8096 - val_loss: 1.4071 - val_accuracy: 0.5690\n",
      "Epoch 1326/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3906 - accuracy: 0.8192 - val_loss: 1.5273 - val_accuracy: 0.5345\n",
      "Epoch 1327/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.8154 - val_loss: 1.3860 - val_accuracy: 0.5690\n",
      "Epoch 1328/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3953 - accuracy: 0.8192 - val_loss: 1.5567 - val_accuracy: 0.4828\n",
      "Epoch 1329/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3957 - accuracy: 0.8135 - val_loss: 1.2984 - val_accuracy: 0.5690\n",
      "Epoch 1330/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3985 - accuracy: 0.8173 - val_loss: 1.4804 - val_accuracy: 0.5000\n",
      "Epoch 1331/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3954 - accuracy: 0.8115 - val_loss: 1.4517 - val_accuracy: 0.5000\n",
      "Epoch 1332/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3912 - accuracy: 0.8077 - val_loss: 1.3334 - val_accuracy: 0.5690\n",
      "Epoch 1333/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3943 - accuracy: 0.8077 - val_loss: 1.6080 - val_accuracy: 0.4828\n",
      "Epoch 1334/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4317 - accuracy: 0.8077 - val_loss: 1.3423 - val_accuracy: 0.5690\n",
      "Epoch 1335/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4009 - accuracy: 0.8154 - val_loss: 1.4202 - val_accuracy: 0.5172\n",
      "Epoch 1336/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3884 - accuracy: 0.8250 - val_loss: 1.4357 - val_accuracy: 0.5172\n",
      "Epoch 1337/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3921 - accuracy: 0.8231 - val_loss: 1.6061 - val_accuracy: 0.5000\n",
      "Epoch 1338/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3968 - accuracy: 0.8058 - val_loss: 1.3378 - val_accuracy: 0.5690\n",
      "Epoch 1339/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3965 - accuracy: 0.8135 - val_loss: 1.7417 - val_accuracy: 0.4828\n",
      "Epoch 1340/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3953 - accuracy: 0.8038 - val_loss: 1.4713 - val_accuracy: 0.5345\n",
      "Epoch 1341/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3837 - accuracy: 0.8115 - val_loss: 1.5571 - val_accuracy: 0.5345\n",
      "Epoch 1342/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4299 - accuracy: 0.7865 - val_loss: 1.3496 - val_accuracy: 0.5862\n",
      "Epoch 1343/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4271 - accuracy: 0.7962 - val_loss: 1.6763 - val_accuracy: 0.5345\n",
      "Epoch 1344/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3943 - accuracy: 0.8135 - val_loss: 1.4101 - val_accuracy: 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1345/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3845 - accuracy: 0.8231 - val_loss: 1.4276 - val_accuracy: 0.5690\n",
      "Epoch 1346/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3875 - accuracy: 0.8077 - val_loss: 1.3437 - val_accuracy: 0.5690\n",
      "Epoch 1347/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3877 - accuracy: 0.8154 - val_loss: 1.4395 - val_accuracy: 0.5690\n",
      "Epoch 1348/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3921 - accuracy: 0.8096 - val_loss: 1.4795 - val_accuracy: 0.5000\n",
      "Epoch 1349/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3880 - accuracy: 0.8154 - val_loss: 1.3890 - val_accuracy: 0.5345\n",
      "Epoch 1350/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3964 - accuracy: 0.8115 - val_loss: 1.4521 - val_accuracy: 0.5000\n",
      "Epoch 1351/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3963 - accuracy: 0.8077 - val_loss: 1.6466 - val_accuracy: 0.4828\n",
      "Epoch 1352/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3915 - accuracy: 0.8096 - val_loss: 1.4765 - val_accuracy: 0.5000\n",
      "Epoch 1353/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3911 - accuracy: 0.8154 - val_loss: 1.4853 - val_accuracy: 0.5000\n",
      "Epoch 1354/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4164 - accuracy: 0.8000 - val_loss: 1.4981 - val_accuracy: 0.5690\n",
      "Epoch 1355/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3869 - accuracy: 0.8096 - val_loss: 1.5269 - val_accuracy: 0.5172\n",
      "Epoch 1356/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4016 - accuracy: 0.8115 - val_loss: 1.3636 - val_accuracy: 0.5517\n",
      "Epoch 1357/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3966 - accuracy: 0.8019 - val_loss: 1.3735 - val_accuracy: 0.5517\n",
      "Epoch 1358/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3837 - accuracy: 0.8308 - val_loss: 1.6095 - val_accuracy: 0.4828\n",
      "Epoch 1359/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3844 - accuracy: 0.8096 - val_loss: 1.7228 - val_accuracy: 0.4828\n",
      "Epoch 1360/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3951 - accuracy: 0.8000 - val_loss: 1.6473 - val_accuracy: 0.5172\n",
      "Epoch 1361/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3921 - accuracy: 0.8231 - val_loss: 1.3330 - val_accuracy: 0.5690\n",
      "Epoch 1362/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4608 - accuracy: 0.7885 - val_loss: 1.3595 - val_accuracy: 0.5690\n",
      "Epoch 1363/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4121 - accuracy: 0.8058 - val_loss: 1.5476 - val_accuracy: 0.4828\n",
      "Epoch 1364/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3964 - accuracy: 0.8154 - val_loss: 1.5386 - val_accuracy: 0.5000\n",
      "Epoch 1365/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4026 - accuracy: 0.8231 - val_loss: 1.4754 - val_accuracy: 0.5517\n",
      "Epoch 1366/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3853 - accuracy: 0.8288 - val_loss: 1.6582 - val_accuracy: 0.4828\n",
      "Epoch 1367/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3898 - accuracy: 0.8192 - val_loss: 1.3413 - val_accuracy: 0.5690\n",
      "Epoch 1368/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3834 - accuracy: 0.8212 - val_loss: 1.4819 - val_accuracy: 0.5172\n",
      "Epoch 1369/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3864 - accuracy: 0.8115 - val_loss: 1.3680 - val_accuracy: 0.5172\n",
      "Epoch 1370/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3960 - accuracy: 0.8058 - val_loss: 1.5743 - val_accuracy: 0.4828\n",
      "Epoch 1371/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3928 - accuracy: 0.8135 - val_loss: 1.2952 - val_accuracy: 0.5862\n",
      "Epoch 1372/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3906 - accuracy: 0.8115 - val_loss: 1.5576 - val_accuracy: 0.5690\n",
      "Epoch 1373/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3948 - accuracy: 0.8115 - val_loss: 1.4571 - val_accuracy: 0.5345\n",
      "Epoch 1374/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3870 - accuracy: 0.8135 - val_loss: 1.5553 - val_accuracy: 0.5345\n",
      "Epoch 1375/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3916 - accuracy: 0.8058 - val_loss: 1.5338 - val_accuracy: 0.5172\n",
      "Epoch 1376/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3870 - accuracy: 0.8192 - val_loss: 1.4777 - val_accuracy: 0.5690\n",
      "Epoch 1377/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4447 - accuracy: 0.7981 - val_loss: 1.4207 - val_accuracy: 0.5517\n",
      "Epoch 1378/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3978 - accuracy: 0.8154 - val_loss: 1.6220 - val_accuracy: 0.4828\n",
      "Epoch 1379/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3798 - accuracy: 0.8173 - val_loss: 1.6344 - val_accuracy: 0.5000\n",
      "Epoch 1380/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3811 - accuracy: 0.8212 - val_loss: 1.4970 - val_accuracy: 0.5690\n",
      "Epoch 1381/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3792 - accuracy: 0.8173 - val_loss: 1.5776 - val_accuracy: 0.5345\n",
      "Epoch 1382/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3868 - accuracy: 0.8231 - val_loss: 1.5688 - val_accuracy: 0.5000\n",
      "Epoch 1383/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3849 - accuracy: 0.8154 - val_loss: 1.4183 - val_accuracy: 0.5517\n",
      "Epoch 1384/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3916 - accuracy: 0.8173 - val_loss: 1.6222 - val_accuracy: 0.5690\n",
      "Epoch 1385/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4253 - accuracy: 0.7981 - val_loss: 1.7498 - val_accuracy: 0.5172\n",
      "Epoch 1386/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4335 - accuracy: 0.7865 - val_loss: 1.6600 - val_accuracy: 0.4828\n",
      "Epoch 1387/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3907 - accuracy: 0.8154 - val_loss: 1.3771 - val_accuracy: 0.5517\n",
      "Epoch 1388/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3777 - accuracy: 0.8308 - val_loss: 1.5161 - val_accuracy: 0.5690\n",
      "Epoch 1389/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3861 - accuracy: 0.8173 - val_loss: 1.4316 - val_accuracy: 0.5862\n",
      "Epoch 1390/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3929 - accuracy: 0.8058 - val_loss: 1.4300 - val_accuracy: 0.5690\n",
      "Epoch 1391/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3818 - accuracy: 0.8135 - val_loss: 1.4774 - val_accuracy: 0.5517\n",
      "Epoch 1392/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3986 - accuracy: 0.8231 - val_loss: 1.4846 - val_accuracy: 0.5000\n",
      "Epoch 1393/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4134 - accuracy: 0.8038 - val_loss: 1.4768 - val_accuracy: 0.5690\n",
      "Epoch 1394/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4047 - accuracy: 0.8019 - val_loss: 1.5084 - val_accuracy: 0.5517\n",
      "Epoch 1395/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3987 - accuracy: 0.8019 - val_loss: 1.5388 - val_accuracy: 0.5000\n",
      "Epoch 1396/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4591 - accuracy: 0.7942 - val_loss: 1.5961 - val_accuracy: 0.5172\n",
      "Epoch 1397/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3910 - accuracy: 0.8212 - val_loss: 1.4913 - val_accuracy: 0.5690\n",
      "Epoch 1398/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3922 - accuracy: 0.8288 - val_loss: 1.5315 - val_accuracy: 0.5172\n",
      "Epoch 1399/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4258 - accuracy: 0.7981 - val_loss: 1.5855 - val_accuracy: 0.5345\n",
      "Epoch 1400/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3704 - accuracy: 0.8231 - val_loss: 1.6265 - val_accuracy: 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1401/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3652 - accuracy: 0.8269 - val_loss: 1.5356 - val_accuracy: 0.5172\n",
      "Epoch 1402/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3688 - accuracy: 0.8346 - val_loss: 1.4846 - val_accuracy: 0.5345\n",
      "Epoch 1403/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3754 - accuracy: 0.8212 - val_loss: 1.3846 - val_accuracy: 0.5862\n",
      "Epoch 1404/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3717 - accuracy: 0.8192 - val_loss: 1.5153 - val_accuracy: 0.5345\n",
      "Epoch 1405/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3726 - accuracy: 0.8269 - val_loss: 1.3955 - val_accuracy: 0.5345\n",
      "Epoch 1406/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3811 - accuracy: 0.8135 - val_loss: 1.5997 - val_accuracy: 0.4828\n",
      "Epoch 1407/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3770 - accuracy: 0.8154 - val_loss: 1.4161 - val_accuracy: 0.5862\n",
      "Epoch 1408/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3801 - accuracy: 0.8096 - val_loss: 1.4167 - val_accuracy: 0.5517\n",
      "Epoch 1409/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3793 - accuracy: 0.8212 - val_loss: 1.3911 - val_accuracy: 0.5690\n",
      "Epoch 1410/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3677 - accuracy: 0.8327 - val_loss: 1.4951 - val_accuracy: 0.5172\n",
      "Epoch 1411/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3743 - accuracy: 0.8212 - val_loss: 1.6454 - val_accuracy: 0.5345\n",
      "Epoch 1412/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3837 - accuracy: 0.8308 - val_loss: 1.5960 - val_accuracy: 0.5517\n",
      "Epoch 1413/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4109 - accuracy: 0.8000 - val_loss: 1.5310 - val_accuracy: 0.5690\n",
      "Epoch 1414/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3678 - accuracy: 0.8327 - val_loss: 1.6110 - val_accuracy: 0.5172\n",
      "Epoch 1415/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3703 - accuracy: 0.8231 - val_loss: 1.5046 - val_accuracy: 0.5517\n",
      "Epoch 1416/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3765 - accuracy: 0.8308 - val_loss: 1.6508 - val_accuracy: 0.5517\n",
      "Epoch 1417/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3841 - accuracy: 0.8192 - val_loss: 1.3945 - val_accuracy: 0.5517\n",
      "Epoch 1418/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3656 - accuracy: 0.8288 - val_loss: 1.7413 - val_accuracy: 0.5172\n",
      "Epoch 1419/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3701 - accuracy: 0.8192 - val_loss: 1.5397 - val_accuracy: 0.5690\n",
      "Epoch 1420/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3978 - accuracy: 0.8212 - val_loss: 1.5657 - val_accuracy: 0.5517\n",
      "Epoch 1421/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4083 - accuracy: 0.8154 - val_loss: 1.5416 - val_accuracy: 0.5690\n",
      "Epoch 1422/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3987 - accuracy: 0.8250 - val_loss: 1.4981 - val_accuracy: 0.5690\n",
      "Epoch 1423/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3945 - accuracy: 0.8115 - val_loss: 1.8800 - val_accuracy: 0.5000\n",
      "Epoch 1424/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3912 - accuracy: 0.8173 - val_loss: 1.5777 - val_accuracy: 0.5000\n",
      "Epoch 1425/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3683 - accuracy: 0.8288 - val_loss: 1.4550 - val_accuracy: 0.5690\n",
      "Epoch 1426/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3646 - accuracy: 0.8346 - val_loss: 1.8033 - val_accuracy: 0.5172\n",
      "Epoch 1427/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3861 - accuracy: 0.8135 - val_loss: 1.7274 - val_accuracy: 0.5517\n",
      "Epoch 1428/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3656 - accuracy: 0.8231 - val_loss: 1.3772 - val_accuracy: 0.5690\n",
      "Epoch 1429/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3773 - accuracy: 0.8250 - val_loss: 1.4966 - val_accuracy: 0.5862\n",
      "Epoch 1430/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3702 - accuracy: 0.8269 - val_loss: 1.6395 - val_accuracy: 0.5517\n",
      "Epoch 1431/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3705 - accuracy: 0.8250 - val_loss: 1.6980 - val_accuracy: 0.5345\n",
      "Epoch 1432/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3721 - accuracy: 0.8250 - val_loss: 1.6473 - val_accuracy: 0.5517\n",
      "Epoch 1433/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3956 - accuracy: 0.8077 - val_loss: 1.5095 - val_accuracy: 0.5690\n",
      "Epoch 1434/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3828 - accuracy: 0.8038 - val_loss: 1.4829 - val_accuracy: 0.5862\n",
      "Epoch 1435/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3738 - accuracy: 0.8173 - val_loss: 1.6778 - val_accuracy: 0.4828\n",
      "Epoch 1436/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3622 - accuracy: 0.8192 - val_loss: 1.5663 - val_accuracy: 0.5690\n",
      "Epoch 1437/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3890 - accuracy: 0.8096 - val_loss: 1.4720 - val_accuracy: 0.5345\n",
      "Epoch 1438/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4078 - accuracy: 0.7981 - val_loss: 1.4016 - val_accuracy: 0.5690\n",
      "Epoch 1439/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3802 - accuracy: 0.8212 - val_loss: 1.5915 - val_accuracy: 0.5000\n",
      "Epoch 1440/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4238 - accuracy: 0.7942 - val_loss: 1.5055 - val_accuracy: 0.5517\n",
      "Epoch 1441/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3692 - accuracy: 0.8212 - val_loss: 1.4508 - val_accuracy: 0.5690\n",
      "Epoch 1442/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3627 - accuracy: 0.8346 - val_loss: 1.8837 - val_accuracy: 0.5000\n",
      "Epoch 1443/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3684 - accuracy: 0.8212 - val_loss: 1.5358 - val_accuracy: 0.5517\n",
      "Epoch 1444/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3651 - accuracy: 0.8365 - val_loss: 1.5197 - val_accuracy: 0.5517\n",
      "Epoch 1445/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3639 - accuracy: 0.8192 - val_loss: 1.4750 - val_accuracy: 0.5690\n",
      "Epoch 1446/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3684 - accuracy: 0.8154 - val_loss: 1.3388 - val_accuracy: 0.5862\n",
      "Epoch 1447/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3665 - accuracy: 0.8288 - val_loss: 1.5933 - val_accuracy: 0.5690\n",
      "Epoch 1448/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3730 - accuracy: 0.8212 - val_loss: 1.5155 - val_accuracy: 0.5517\n",
      "Epoch 1449/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3996 - accuracy: 0.8077 - val_loss: 1.6075 - val_accuracy: 0.5345\n",
      "Epoch 1450/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3609 - accuracy: 0.8231 - val_loss: 1.4581 - val_accuracy: 0.5862\n",
      "Epoch 1451/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3747 - accuracy: 0.8212 - val_loss: 1.6497 - val_accuracy: 0.5517\n",
      "Epoch 1452/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3671 - accuracy: 0.8308 - val_loss: 1.4965 - val_accuracy: 0.5690\n",
      "Epoch 1453/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3679 - accuracy: 0.8135 - val_loss: 1.6024 - val_accuracy: 0.5690\n",
      "Epoch 1454/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3748 - accuracy: 0.8173 - val_loss: 1.5387 - val_accuracy: 0.5690\n",
      "Epoch 1455/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4161 - accuracy: 0.7923 - val_loss: 1.4806 - val_accuracy: 0.5690\n",
      "Epoch 1456/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3703 - accuracy: 0.8212 - val_loss: 1.6090 - val_accuracy: 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1457/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3715 - accuracy: 0.8231 - val_loss: 1.4683 - val_accuracy: 0.5862\n",
      "Epoch 1458/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3763 - accuracy: 0.8269 - val_loss: 1.5775 - val_accuracy: 0.5172\n",
      "Epoch 1459/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3853 - accuracy: 0.8173 - val_loss: 1.5629 - val_accuracy: 0.5517\n",
      "Epoch 1460/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3610 - accuracy: 0.8308 - val_loss: 1.7457 - val_accuracy: 0.5517\n",
      "Epoch 1461/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3582 - accuracy: 0.8288 - val_loss: 1.5542 - val_accuracy: 0.5690\n",
      "Epoch 1462/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3610 - accuracy: 0.8327 - val_loss: 1.6305 - val_accuracy: 0.5000\n",
      "Epoch 1463/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3604 - accuracy: 0.8288 - val_loss: 1.5511 - val_accuracy: 0.5862\n",
      "Epoch 1464/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3591 - accuracy: 0.8327 - val_loss: 1.6530 - val_accuracy: 0.5172\n",
      "Epoch 1465/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3615 - accuracy: 0.8212 - val_loss: 1.5003 - val_accuracy: 0.5690\n",
      "Epoch 1466/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3751 - accuracy: 0.8135 - val_loss: 1.8035 - val_accuracy: 0.5172\n",
      "Epoch 1467/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3681 - accuracy: 0.8231 - val_loss: 1.7347 - val_accuracy: 0.5172\n",
      "Epoch 1468/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3645 - accuracy: 0.8250 - val_loss: 1.4551 - val_accuracy: 0.5862\n",
      "Epoch 1469/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3902 - accuracy: 0.8038 - val_loss: 1.7805 - val_accuracy: 0.5172\n",
      "Epoch 1470/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3611 - accuracy: 0.8212 - val_loss: 1.7746 - val_accuracy: 0.5172\n",
      "Epoch 1471/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3947 - accuracy: 0.8115 - val_loss: 1.6177 - val_accuracy: 0.5517\n",
      "Epoch 1472/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4078 - accuracy: 0.8154 - val_loss: 1.6752 - val_accuracy: 0.5517\n",
      "Epoch 1473/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3599 - accuracy: 0.8250 - val_loss: 1.6700 - val_accuracy: 0.5172\n",
      "Epoch 1474/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3524 - accuracy: 0.8327 - val_loss: 1.8623 - val_accuracy: 0.4828\n",
      "Epoch 1475/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3575 - accuracy: 0.8212 - val_loss: 1.7246 - val_accuracy: 0.5690\n",
      "Epoch 1476/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3587 - accuracy: 0.8288 - val_loss: 1.6915 - val_accuracy: 0.5517\n",
      "Epoch 1477/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3641 - accuracy: 0.8269 - val_loss: 1.7311 - val_accuracy: 0.5517\n",
      "Epoch 1478/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3622 - accuracy: 0.8173 - val_loss: 1.8153 - val_accuracy: 0.5517\n",
      "Epoch 1479/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3606 - accuracy: 0.8231 - val_loss: 1.7493 - val_accuracy: 0.5517\n",
      "Epoch 1480/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3610 - accuracy: 0.8269 - val_loss: 1.8221 - val_accuracy: 0.4828\n",
      "Epoch 1481/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3728 - accuracy: 0.8250 - val_loss: 1.6254 - val_accuracy: 0.5517\n",
      "Epoch 1482/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3731 - accuracy: 0.8154 - val_loss: 1.6315 - val_accuracy: 0.5517\n",
      "Epoch 1483/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.4276 - accuracy: 0.7962 - val_loss: 1.6392 - val_accuracy: 0.5345\n",
      "Epoch 1484/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3922 - accuracy: 0.8115 - val_loss: 1.7180 - val_accuracy: 0.5000\n",
      "Epoch 1485/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3577 - accuracy: 0.8346 - val_loss: 1.3799 - val_accuracy: 0.5862\n",
      "Epoch 1486/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3807 - accuracy: 0.8231 - val_loss: 1.8572 - val_accuracy: 0.5172\n",
      "Epoch 1487/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3987 - accuracy: 0.8096 - val_loss: 1.5402 - val_accuracy: 0.5517\n",
      "Epoch 1488/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3591 - accuracy: 0.8346 - val_loss: 1.7652 - val_accuracy: 0.5172\n",
      "Epoch 1489/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3518 - accuracy: 0.8385 - val_loss: 1.7941 - val_accuracy: 0.5172\n",
      "Epoch 1490/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3572 - accuracy: 0.8327 - val_loss: 1.7479 - val_accuracy: 0.5517\n",
      "Epoch 1491/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3499 - accuracy: 0.8423 - val_loss: 1.6998 - val_accuracy: 0.5517\n",
      "Epoch 1492/2000\n",
      "520/520 [==============================] - 1s 2ms/step - loss: 0.3499 - accuracy: 0.8288 - val_loss: 1.6375 - val_accuracy: 0.5690\n",
      "Epoch 1493/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3560 - accuracy: 0.8173 - val_loss: 1.7000 - val_accuracy: 0.5345\n",
      "Epoch 1494/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3559 - accuracy: 0.8250 - val_loss: 1.5740 - val_accuracy: 0.5517\n",
      "Epoch 1495/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3616 - accuracy: 0.8269 - val_loss: 2.1252 - val_accuracy: 0.4828\n",
      "Epoch 1496/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3576 - accuracy: 0.8346 - val_loss: 1.7281 - val_accuracy: 0.5690\n",
      "Epoch 1497/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3732 - accuracy: 0.8192 - val_loss: 1.6312 - val_accuracy: 0.5862\n",
      "Epoch 1498/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3733 - accuracy: 0.8173 - val_loss: 1.8236 - val_accuracy: 0.5172\n",
      "Epoch 1499/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3932 - accuracy: 0.8135 - val_loss: 1.7095 - val_accuracy: 0.5690\n",
      "Epoch 1500/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3697 - accuracy: 0.8212 - val_loss: 1.7542 - val_accuracy: 0.5345\n",
      "Epoch 1501/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3559 - accuracy: 0.8308 - val_loss: 1.6349 - val_accuracy: 0.5172\n",
      "Epoch 1502/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3474 - accuracy: 0.8423 - val_loss: 1.8759 - val_accuracy: 0.5172\n",
      "Epoch 1503/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3561 - accuracy: 0.8308 - val_loss: 1.8008 - val_accuracy: 0.5345\n",
      "Epoch 1504/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3497 - accuracy: 0.8327 - val_loss: 2.0542 - val_accuracy: 0.5000\n",
      "Epoch 1505/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3677 - accuracy: 0.8269 - val_loss: 1.7039 - val_accuracy: 0.5690\n",
      "Epoch 1506/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3962 - accuracy: 0.8000 - val_loss: 1.8048 - val_accuracy: 0.5517\n",
      "Epoch 1507/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4315 - accuracy: 0.7981 - val_loss: 1.6293 - val_accuracy: 0.5517\n",
      "Epoch 1508/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3961 - accuracy: 0.8038 - val_loss: 1.7386 - val_accuracy: 0.5172\n",
      "Epoch 1509/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3562 - accuracy: 0.8346 - val_loss: 1.7739 - val_accuracy: 0.5345\n",
      "Epoch 1510/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3514 - accuracy: 0.8308 - val_loss: 1.7737 - val_accuracy: 0.5517\n",
      "Epoch 1511/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3480 - accuracy: 0.8346 - val_loss: 1.7517 - val_accuracy: 0.5345\n",
      "Epoch 1512/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3710 - accuracy: 0.8192 - val_loss: 1.7032 - val_accuracy: 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1513/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3569 - accuracy: 0.8308 - val_loss: 2.0903 - val_accuracy: 0.5000\n",
      "Epoch 1514/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3518 - accuracy: 0.8288 - val_loss: 1.7382 - val_accuracy: 0.5690\n",
      "Epoch 1515/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3506 - accuracy: 0.8346 - val_loss: 1.6627 - val_accuracy: 0.5517\n",
      "Epoch 1516/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3476 - accuracy: 0.8346 - val_loss: 1.8209 - val_accuracy: 0.5000\n",
      "Epoch 1517/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3938 - accuracy: 0.8269 - val_loss: 1.6844 - val_accuracy: 0.5690\n",
      "Epoch 1518/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4145 - accuracy: 0.8077 - val_loss: 1.5323 - val_accuracy: 0.5862\n",
      "Epoch 1519/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4101 - accuracy: 0.8058 - val_loss: 1.7863 - val_accuracy: 0.5517\n",
      "Epoch 1520/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3974 - accuracy: 0.8173 - val_loss: 1.8517 - val_accuracy: 0.5345\n",
      "Epoch 1521/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4008 - accuracy: 0.8192 - val_loss: 1.7424 - val_accuracy: 0.5690\n",
      "Epoch 1522/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4079 - accuracy: 0.8115 - val_loss: 1.8726 - val_accuracy: 0.5172\n",
      "Epoch 1523/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4036 - accuracy: 0.8135 - val_loss: 1.6796 - val_accuracy: 0.5690\n",
      "Epoch 1524/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4429 - accuracy: 0.8096 - val_loss: 1.7500 - val_accuracy: 0.5345\n",
      "Epoch 1525/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4548 - accuracy: 0.8077 - val_loss: 1.7342 - val_accuracy: 0.5517\n",
      "Epoch 1526/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4264 - accuracy: 0.8058 - val_loss: 1.6334 - val_accuracy: 0.5690\n",
      "Epoch 1527/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3950 - accuracy: 0.8135 - val_loss: 1.5417 - val_accuracy: 0.5690\n",
      "Epoch 1528/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3909 - accuracy: 0.8212 - val_loss: 1.7479 - val_accuracy: 0.5172\n",
      "Epoch 1529/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3946 - accuracy: 0.8058 - val_loss: 1.6952 - val_accuracy: 0.5862\n",
      "Epoch 1530/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3783 - accuracy: 0.8173 - val_loss: 1.6324 - val_accuracy: 0.5517\n",
      "Epoch 1531/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3766 - accuracy: 0.8250 - val_loss: 1.7815 - val_accuracy: 0.5172\n",
      "Epoch 1532/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3768 - accuracy: 0.8327 - val_loss: 1.7053 - val_accuracy: 0.5862\n",
      "Epoch 1533/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3915 - accuracy: 0.8231 - val_loss: 1.7258 - val_accuracy: 0.5517\n",
      "Epoch 1534/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3918 - accuracy: 0.8135 - val_loss: 1.6711 - val_accuracy: 0.5345\n",
      "Epoch 1535/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3997 - accuracy: 0.7962 - val_loss: 1.6557 - val_accuracy: 0.5517\n",
      "Epoch 1536/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8096 - val_loss: 1.7196 - val_accuracy: 0.5690\n",
      "Epoch 1537/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3939 - accuracy: 0.8000 - val_loss: 1.7869 - val_accuracy: 0.5690\n",
      "Epoch 1538/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3837 - accuracy: 0.8212 - val_loss: 1.9124 - val_accuracy: 0.5517\n",
      "Epoch 1539/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3901 - accuracy: 0.8058 - val_loss: 1.8747 - val_accuracy: 0.5172\n",
      "Epoch 1540/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4087 - accuracy: 0.8000 - val_loss: 1.6774 - val_accuracy: 0.5862\n",
      "Epoch 1541/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3711 - accuracy: 0.8231 - val_loss: 1.6793 - val_accuracy: 0.5172\n",
      "Epoch 1542/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3699 - accuracy: 0.8135 - val_loss: 1.6018 - val_accuracy: 0.5690\n",
      "Epoch 1543/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3620 - accuracy: 0.8288 - val_loss: 1.7100 - val_accuracy: 0.5690\n",
      "Epoch 1544/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3605 - accuracy: 0.8288 - val_loss: 1.6522 - val_accuracy: 0.5862\n",
      "Epoch 1545/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3691 - accuracy: 0.8173 - val_loss: 1.6165 - val_accuracy: 0.5690\n",
      "Epoch 1546/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3684 - accuracy: 0.8154 - val_loss: 1.8164 - val_accuracy: 0.5690\n",
      "Epoch 1547/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3609 - accuracy: 0.8365 - val_loss: 1.7134 - val_accuracy: 0.5517\n",
      "Epoch 1548/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3755 - accuracy: 0.8212 - val_loss: 1.7956 - val_accuracy: 0.5690\n",
      "Epoch 1549/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3746 - accuracy: 0.8308 - val_loss: 1.9059 - val_accuracy: 0.5690\n",
      "Epoch 1550/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3814 - accuracy: 0.8173 - val_loss: 1.8497 - val_accuracy: 0.5517\n",
      "Epoch 1551/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3715 - accuracy: 0.8212 - val_loss: 1.7357 - val_accuracy: 0.5517\n",
      "Epoch 1552/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3763 - accuracy: 0.8115 - val_loss: 1.6366 - val_accuracy: 0.5690\n",
      "Epoch 1553/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3878 - accuracy: 0.8173 - val_loss: 1.8460 - val_accuracy: 0.5517\n",
      "Epoch 1554/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3593 - accuracy: 0.8346 - val_loss: 1.7668 - val_accuracy: 0.5345\n",
      "Epoch 1555/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3806 - accuracy: 0.8154 - val_loss: 1.7613 - val_accuracy: 0.5172\n",
      "Epoch 1556/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3586 - accuracy: 0.8269 - val_loss: 1.7116 - val_accuracy: 0.5862\n",
      "Epoch 1557/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3714 - accuracy: 0.8115 - val_loss: 1.6609 - val_accuracy: 0.5690\n",
      "Epoch 1558/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3733 - accuracy: 0.8250 - val_loss: 1.7928 - val_accuracy: 0.5172\n",
      "Epoch 1559/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3595 - accuracy: 0.8327 - val_loss: 1.9428 - val_accuracy: 0.5000\n",
      "Epoch 1560/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3710 - accuracy: 0.8135 - val_loss: 1.8671 - val_accuracy: 0.5345\n",
      "Epoch 1561/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3699 - accuracy: 0.8154 - val_loss: 1.7234 - val_accuracy: 0.5517\n",
      "Epoch 1562/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3661 - accuracy: 0.8231 - val_loss: 1.7423 - val_accuracy: 0.5690\n",
      "Epoch 1563/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3601 - accuracy: 0.8212 - val_loss: 1.8160 - val_accuracy: 0.5517\n",
      "Epoch 1564/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3639 - accuracy: 0.8212 - val_loss: 1.9589 - val_accuracy: 0.5690\n",
      "Epoch 1565/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3609 - accuracy: 0.8135 - val_loss: 1.8155 - val_accuracy: 0.5172\n",
      "Epoch 1566/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3615 - accuracy: 0.8212 - val_loss: 1.7377 - val_accuracy: 0.5345\n",
      "Epoch 1567/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3758 - accuracy: 0.8192 - val_loss: 1.8773 - val_accuracy: 0.5690\n",
      "Epoch 1568/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3695 - accuracy: 0.8173 - val_loss: 1.6067 - val_accuracy: 0.5690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1569/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3553 - accuracy: 0.8365 - val_loss: 1.9017 - val_accuracy: 0.5517\n",
      "Epoch 1570/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3578 - accuracy: 0.8250 - val_loss: 1.7313 - val_accuracy: 0.5517\n",
      "Epoch 1571/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3610 - accuracy: 0.8212 - val_loss: 1.8178 - val_accuracy: 0.5345\n",
      "Epoch 1572/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3706 - accuracy: 0.8077 - val_loss: 1.7089 - val_accuracy: 0.5690\n",
      "Epoch 1573/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3924 - accuracy: 0.8115 - val_loss: 2.0661 - val_accuracy: 0.5517\n",
      "Epoch 1574/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4030 - accuracy: 0.8192 - val_loss: 1.7330 - val_accuracy: 0.5862\n",
      "Epoch 1575/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4366 - accuracy: 0.8000 - val_loss: 1.9397 - val_accuracy: 0.5690\n",
      "Epoch 1576/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4034 - accuracy: 0.8115 - val_loss: 1.9029 - val_accuracy: 0.5000\n",
      "Epoch 1577/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3679 - accuracy: 0.8154 - val_loss: 1.8989 - val_accuracy: 0.5517\n",
      "Epoch 1578/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3749 - accuracy: 0.8154 - val_loss: 1.7214 - val_accuracy: 0.5862\n",
      "Epoch 1579/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3643 - accuracy: 0.8327 - val_loss: 1.7227 - val_accuracy: 0.5517\n",
      "Epoch 1580/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3557 - accuracy: 0.8346 - val_loss: 1.9141 - val_accuracy: 0.5172\n",
      "Epoch 1581/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3563 - accuracy: 0.8327 - val_loss: 1.8630 - val_accuracy: 0.5862\n",
      "Epoch 1582/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3564 - accuracy: 0.8231 - val_loss: 1.6302 - val_accuracy: 0.5690\n",
      "Epoch 1583/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3515 - accuracy: 0.8269 - val_loss: 1.8328 - val_accuracy: 0.5517\n",
      "Epoch 1584/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3566 - accuracy: 0.8212 - val_loss: 2.0771 - val_accuracy: 0.5517\n",
      "Epoch 1585/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3644 - accuracy: 0.8192 - val_loss: 1.8670 - val_accuracy: 0.5862\n",
      "Epoch 1586/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3529 - accuracy: 0.8308 - val_loss: 1.7482 - val_accuracy: 0.5690\n",
      "Epoch 1587/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3567 - accuracy: 0.8269 - val_loss: 2.0334 - val_accuracy: 0.5000\n",
      "Epoch 1588/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3545 - accuracy: 0.8269 - val_loss: 2.0493 - val_accuracy: 0.5000\n",
      "Epoch 1589/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3644 - accuracy: 0.8231 - val_loss: 2.0097 - val_accuracy: 0.5345\n",
      "Epoch 1590/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3668 - accuracy: 0.8231 - val_loss: 1.9171 - val_accuracy: 0.5690\n",
      "Epoch 1591/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3962 - accuracy: 0.8019 - val_loss: 1.9717 - val_accuracy: 0.5517\n",
      "Epoch 1592/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3936 - accuracy: 0.8115 - val_loss: 1.8632 - val_accuracy: 0.5690\n",
      "Epoch 1593/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3985 - accuracy: 0.8115 - val_loss: 1.8745 - val_accuracy: 0.5517\n",
      "Epoch 1594/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4019 - accuracy: 0.8077 - val_loss: 2.0937 - val_accuracy: 0.5517\n",
      "Epoch 1595/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3903 - accuracy: 0.8058 - val_loss: 2.0829 - val_accuracy: 0.5172\n",
      "Epoch 1596/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3749 - accuracy: 0.8231 - val_loss: 2.0123 - val_accuracy: 0.5690\n",
      "Epoch 1597/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3742 - accuracy: 0.8192 - val_loss: 2.2179 - val_accuracy: 0.5000\n",
      "Epoch 1598/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3769 - accuracy: 0.8154 - val_loss: 1.8179 - val_accuracy: 0.5690\n",
      "Epoch 1599/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3738 - accuracy: 0.8231 - val_loss: 1.8206 - val_accuracy: 0.5172\n",
      "Epoch 1600/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.8038 - val_loss: 1.6143 - val_accuracy: 0.6034\n",
      "Epoch 1601/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3892 - accuracy: 0.8038 - val_loss: 2.0018 - val_accuracy: 0.5345\n",
      "Epoch 1602/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3722 - accuracy: 0.8173 - val_loss: 1.9410 - val_accuracy: 0.5690\n",
      "Epoch 1603/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3784 - accuracy: 0.8269 - val_loss: 1.9178 - val_accuracy: 0.5517\n",
      "Epoch 1604/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3722 - accuracy: 0.8135 - val_loss: 2.0232 - val_accuracy: 0.5172\n",
      "Epoch 1605/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3668 - accuracy: 0.8192 - val_loss: 1.9661 - val_accuracy: 0.5690\n",
      "Epoch 1606/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3661 - accuracy: 0.8192 - val_loss: 2.0426 - val_accuracy: 0.5690\n",
      "Epoch 1607/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3828 - accuracy: 0.8154 - val_loss: 1.8425 - val_accuracy: 0.5517\n",
      "Epoch 1608/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3926 - accuracy: 0.8173 - val_loss: 2.1073 - val_accuracy: 0.5690\n",
      "Epoch 1609/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3661 - accuracy: 0.8173 - val_loss: 1.8964 - val_accuracy: 0.5172\n",
      "Epoch 1610/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3625 - accuracy: 0.8269 - val_loss: 1.8732 - val_accuracy: 0.5690\n",
      "Epoch 1611/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3633 - accuracy: 0.8231 - val_loss: 1.9402 - val_accuracy: 0.5690\n",
      "Epoch 1612/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3696 - accuracy: 0.8269 - val_loss: 1.9154 - val_accuracy: 0.5517\n",
      "Epoch 1613/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3763 - accuracy: 0.8154 - val_loss: 1.7942 - val_accuracy: 0.5345\n",
      "Epoch 1614/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3807 - accuracy: 0.8096 - val_loss: 1.6015 - val_accuracy: 0.5862\n",
      "Epoch 1615/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3878 - accuracy: 0.8154 - val_loss: 1.8347 - val_accuracy: 0.6034\n",
      "Epoch 1616/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3683 - accuracy: 0.8250 - val_loss: 2.0858 - val_accuracy: 0.5517\n",
      "Epoch 1617/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3622 - accuracy: 0.8173 - val_loss: 1.8491 - val_accuracy: 0.5690\n",
      "Epoch 1618/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3602 - accuracy: 0.8288 - val_loss: 2.0874 - val_accuracy: 0.5690\n",
      "Epoch 1619/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3828 - accuracy: 0.8115 - val_loss: 1.8505 - val_accuracy: 0.5517\n",
      "Epoch 1620/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3755 - accuracy: 0.8269 - val_loss: 1.7391 - val_accuracy: 0.5345\n",
      "Epoch 1621/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3571 - accuracy: 0.8250 - val_loss: 1.9743 - val_accuracy: 0.5517\n",
      "Epoch 1622/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3603 - accuracy: 0.8308 - val_loss: 1.9792 - val_accuracy: 0.5517\n",
      "Epoch 1623/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3667 - accuracy: 0.8231 - val_loss: 2.2783 - val_accuracy: 0.4655\n",
      "Epoch 1624/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3583 - accuracy: 0.8250 - val_loss: 1.7958 - val_accuracy: 0.5862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1625/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3678 - accuracy: 0.8250 - val_loss: 1.9070 - val_accuracy: 0.5690\n",
      "Epoch 1626/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3626 - accuracy: 0.8250 - val_loss: 2.0009 - val_accuracy: 0.5690\n",
      "Epoch 1627/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3534 - accuracy: 0.8308 - val_loss: 2.1132 - val_accuracy: 0.5345\n",
      "Epoch 1628/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3797 - accuracy: 0.8019 - val_loss: 2.0457 - val_accuracy: 0.5345\n",
      "Epoch 1629/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3694 - accuracy: 0.8173 - val_loss: 2.0967 - val_accuracy: 0.5862\n",
      "Epoch 1630/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3637 - accuracy: 0.8231 - val_loss: 2.0358 - val_accuracy: 0.5517\n",
      "Epoch 1631/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3587 - accuracy: 0.8173 - val_loss: 1.7339 - val_accuracy: 0.6034\n",
      "Epoch 1632/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3525 - accuracy: 0.8327 - val_loss: 2.1034 - val_accuracy: 0.5517\n",
      "Epoch 1633/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3540 - accuracy: 0.8346 - val_loss: 2.3430 - val_accuracy: 0.5517\n",
      "Epoch 1634/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3737 - accuracy: 0.8154 - val_loss: 2.0030 - val_accuracy: 0.5690\n",
      "Epoch 1635/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3547 - accuracy: 0.8288 - val_loss: 1.9775 - val_accuracy: 0.5862\n",
      "Epoch 1636/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3613 - accuracy: 0.8250 - val_loss: 2.0695 - val_accuracy: 0.5517\n",
      "Epoch 1637/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3611 - accuracy: 0.8154 - val_loss: 1.9037 - val_accuracy: 0.5862\n",
      "Epoch 1638/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3694 - accuracy: 0.8173 - val_loss: 1.8963 - val_accuracy: 0.6034\n",
      "Epoch 1639/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3644 - accuracy: 0.8173 - val_loss: 1.8490 - val_accuracy: 0.5690\n",
      "Epoch 1640/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3517 - accuracy: 0.8269 - val_loss: 1.9657 - val_accuracy: 0.5690\n",
      "Epoch 1641/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3530 - accuracy: 0.8269 - val_loss: 1.8083 - val_accuracy: 0.5862\n",
      "Epoch 1642/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3693 - accuracy: 0.8135 - val_loss: 2.1079 - val_accuracy: 0.5690\n",
      "Epoch 1643/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3564 - accuracy: 0.8192 - val_loss: 1.9443 - val_accuracy: 0.5517\n",
      "Epoch 1644/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3481 - accuracy: 0.8346 - val_loss: 2.0953 - val_accuracy: 0.5690\n",
      "Epoch 1645/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3624 - accuracy: 0.8115 - val_loss: 1.8805 - val_accuracy: 0.5690\n",
      "Epoch 1646/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3603 - accuracy: 0.8192 - val_loss: 2.0129 - val_accuracy: 0.5690\n",
      "Epoch 1647/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3626 - accuracy: 0.8212 - val_loss: 1.7748 - val_accuracy: 0.5690\n",
      "Epoch 1648/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3773 - accuracy: 0.8135 - val_loss: 2.1535 - val_accuracy: 0.5517\n",
      "Epoch 1649/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3540 - accuracy: 0.8212 - val_loss: 2.1495 - val_accuracy: 0.5690\n",
      "Epoch 1650/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3573 - accuracy: 0.8135 - val_loss: 2.3152 - val_accuracy: 0.5345\n",
      "Epoch 1651/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3586 - accuracy: 0.8192 - val_loss: 2.1835 - val_accuracy: 0.5690\n",
      "Epoch 1652/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3458 - accuracy: 0.8250 - val_loss: 2.1613 - val_accuracy: 0.5517\n",
      "Epoch 1653/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3539 - accuracy: 0.8269 - val_loss: 1.9712 - val_accuracy: 0.6034\n",
      "Epoch 1654/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3558 - accuracy: 0.8288 - val_loss: 2.2156 - val_accuracy: 0.5690\n",
      "Epoch 1655/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3566 - accuracy: 0.8250 - val_loss: 2.0055 - val_accuracy: 0.5862\n",
      "Epoch 1656/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3473 - accuracy: 0.8250 - val_loss: 1.9431 - val_accuracy: 0.5690\n",
      "Epoch 1657/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3702 - accuracy: 0.8269 - val_loss: 2.2330 - val_accuracy: 0.5345\n",
      "Epoch 1658/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3495 - accuracy: 0.8308 - val_loss: 2.4917 - val_accuracy: 0.5345\n",
      "Epoch 1659/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3671 - accuracy: 0.8154 - val_loss: 2.0374 - val_accuracy: 0.5345\n",
      "Epoch 1660/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3601 - accuracy: 0.8269 - val_loss: 2.1286 - val_accuracy: 0.5517\n",
      "Epoch 1661/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3504 - accuracy: 0.8327 - val_loss: 1.9503 - val_accuracy: 0.5517\n",
      "Epoch 1662/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3624 - accuracy: 0.8308 - val_loss: 2.0203 - val_accuracy: 0.5345\n",
      "Epoch 1663/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3583 - accuracy: 0.8231 - val_loss: 1.8275 - val_accuracy: 0.5862\n",
      "Epoch 1664/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3507 - accuracy: 0.8269 - val_loss: 2.2995 - val_accuracy: 0.5172\n",
      "Epoch 1665/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3463 - accuracy: 0.8288 - val_loss: 2.1220 - val_accuracy: 0.5690\n",
      "Epoch 1666/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3428 - accuracy: 0.8385 - val_loss: 2.0395 - val_accuracy: 0.5862\n",
      "Epoch 1667/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3461 - accuracy: 0.8346 - val_loss: 2.0614 - val_accuracy: 0.5690\n",
      "Epoch 1668/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3475 - accuracy: 0.8346 - val_loss: 2.1449 - val_accuracy: 0.5345\n",
      "Epoch 1669/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3453 - accuracy: 0.8308 - val_loss: 2.4083 - val_accuracy: 0.5345\n",
      "Epoch 1670/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3547 - accuracy: 0.8308 - val_loss: 2.3229 - val_accuracy: 0.5345\n",
      "Epoch 1671/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3668 - accuracy: 0.8154 - val_loss: 2.0586 - val_accuracy: 0.5517\n",
      "Epoch 1672/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3759 - accuracy: 0.8038 - val_loss: 1.9632 - val_accuracy: 0.5517\n",
      "Epoch 1673/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3508 - accuracy: 0.8173 - val_loss: 2.0102 - val_accuracy: 0.5690\n",
      "Epoch 1674/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3522 - accuracy: 0.8250 - val_loss: 2.0916 - val_accuracy: 0.5517\n",
      "Epoch 1675/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3405 - accuracy: 0.8423 - val_loss: 2.1097 - val_accuracy: 0.5862\n",
      "Epoch 1676/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3464 - accuracy: 0.8269 - val_loss: 2.1696 - val_accuracy: 0.5690\n",
      "Epoch 1677/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3395 - accuracy: 0.8385 - val_loss: 2.0740 - val_accuracy: 0.5862\n",
      "Epoch 1678/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3386 - accuracy: 0.8500 - val_loss: 2.3704 - val_accuracy: 0.5517\n",
      "Epoch 1679/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3473 - accuracy: 0.8327 - val_loss: 2.0736 - val_accuracy: 0.5862\n",
      "Epoch 1680/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3573 - accuracy: 0.8192 - val_loss: 1.7914 - val_accuracy: 0.5862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1681/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3790 - accuracy: 0.8154 - val_loss: 2.2313 - val_accuracy: 0.5517\n",
      "Epoch 1682/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3538 - accuracy: 0.8269 - val_loss: 2.1104 - val_accuracy: 0.5690\n",
      "Epoch 1683/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3430 - accuracy: 0.8269 - val_loss: 2.1991 - val_accuracy: 0.5517\n",
      "Epoch 1684/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3534 - accuracy: 0.8346 - val_loss: 2.0209 - val_accuracy: 0.6034\n",
      "Epoch 1685/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3618 - accuracy: 0.8288 - val_loss: 1.9451 - val_accuracy: 0.5690\n",
      "Epoch 1686/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3587 - accuracy: 0.8231 - val_loss: 2.0609 - val_accuracy: 0.5690\n",
      "Epoch 1687/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3378 - accuracy: 0.8423 - val_loss: 2.1206 - val_accuracy: 0.5517\n",
      "Epoch 1688/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3395 - accuracy: 0.8346 - val_loss: 2.1545 - val_accuracy: 0.5517\n",
      "Epoch 1689/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3271 - accuracy: 0.8519 - val_loss: 2.1370 - val_accuracy: 0.5345\n",
      "Epoch 1690/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3486 - accuracy: 0.8346 - val_loss: 2.1949 - val_accuracy: 0.5517\n",
      "Epoch 1691/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3593 - accuracy: 0.8327 - val_loss: 2.2411 - val_accuracy: 0.5172\n",
      "Epoch 1692/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3866 - accuracy: 0.8154 - val_loss: 1.9722 - val_accuracy: 0.5690\n",
      "Epoch 1693/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3508 - accuracy: 0.8288 - val_loss: 2.1967 - val_accuracy: 0.5517\n",
      "Epoch 1694/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3431 - accuracy: 0.8327 - val_loss: 2.4978 - val_accuracy: 0.5345\n",
      "Epoch 1695/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3475 - accuracy: 0.8269 - val_loss: 2.0976 - val_accuracy: 0.5517\n",
      "Epoch 1696/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3804 - accuracy: 0.8154 - val_loss: 2.2790 - val_accuracy: 0.5345\n",
      "Epoch 1697/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3472 - accuracy: 0.8327 - val_loss: 2.3942 - val_accuracy: 0.5345\n",
      "Epoch 1698/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3374 - accuracy: 0.8423 - val_loss: 2.4496 - val_accuracy: 0.5345\n",
      "Epoch 1699/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3391 - accuracy: 0.8385 - val_loss: 2.4880 - val_accuracy: 0.5345\n",
      "Epoch 1700/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3479 - accuracy: 0.8269 - val_loss: 2.2395 - val_accuracy: 0.5517\n",
      "Epoch 1701/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3596 - accuracy: 0.8154 - val_loss: 1.6861 - val_accuracy: 0.5862\n",
      "Epoch 1702/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3739 - accuracy: 0.8115 - val_loss: 2.2092 - val_accuracy: 0.5345\n",
      "Epoch 1703/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3971 - accuracy: 0.8000 - val_loss: 2.3912 - val_accuracy: 0.5517\n",
      "Epoch 1704/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3382 - accuracy: 0.8481 - val_loss: 2.3410 - val_accuracy: 0.5172\n",
      "Epoch 1705/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3449 - accuracy: 0.8423 - val_loss: 2.3126 - val_accuracy: 0.5517\n",
      "Epoch 1706/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3349 - accuracy: 0.8308 - val_loss: 2.1433 - val_accuracy: 0.5690\n",
      "Epoch 1707/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3302 - accuracy: 0.8327 - val_loss: 2.1463 - val_accuracy: 0.5690\n",
      "Epoch 1708/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3408 - accuracy: 0.8327 - val_loss: 2.2238 - val_accuracy: 0.5517\n",
      "Epoch 1709/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3394 - accuracy: 0.8308 - val_loss: 2.1180 - val_accuracy: 0.5517\n",
      "Epoch 1710/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3377 - accuracy: 0.8365 - val_loss: 2.1038 - val_accuracy: 0.5345\n",
      "Epoch 1711/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3486 - accuracy: 0.8385 - val_loss: 2.3583 - val_accuracy: 0.5517\n",
      "Epoch 1712/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3448 - accuracy: 0.8308 - val_loss: 2.4801 - val_accuracy: 0.5517\n",
      "Epoch 1713/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3415 - accuracy: 0.8288 - val_loss: 2.1822 - val_accuracy: 0.5690\n",
      "Epoch 1714/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3401 - accuracy: 0.8404 - val_loss: 2.2617 - val_accuracy: 0.5517\n",
      "Epoch 1715/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3520 - accuracy: 0.8077 - val_loss: 2.4672 - val_accuracy: 0.5517\n",
      "Epoch 1716/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3450 - accuracy: 0.8365 - val_loss: 2.6590 - val_accuracy: 0.5345\n",
      "Epoch 1717/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3769 - accuracy: 0.8058 - val_loss: 2.2751 - val_accuracy: 0.5517\n",
      "Epoch 1718/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3365 - accuracy: 0.8365 - val_loss: 2.1460 - val_accuracy: 0.5690\n",
      "Epoch 1719/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3541 - accuracy: 0.8288 - val_loss: 2.2975 - val_accuracy: 0.5345\n",
      "Epoch 1720/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3359 - accuracy: 0.8385 - val_loss: 2.4119 - val_accuracy: 0.5345\n",
      "Epoch 1721/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3367 - accuracy: 0.8442 - val_loss: 2.4079 - val_accuracy: 0.5345\n",
      "Epoch 1722/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3338 - accuracy: 0.8404 - val_loss: 2.2993 - val_accuracy: 0.5690\n",
      "Epoch 1723/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3358 - accuracy: 0.8365 - val_loss: 2.4612 - val_accuracy: 0.5517\n",
      "Epoch 1724/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3283 - accuracy: 0.8442 - val_loss: 2.6413 - val_accuracy: 0.5345\n",
      "Epoch 1725/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3444 - accuracy: 0.8423 - val_loss: 2.2190 - val_accuracy: 0.5690\n",
      "Epoch 1726/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3498 - accuracy: 0.8250 - val_loss: 2.3357 - val_accuracy: 0.5517\n",
      "Epoch 1727/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3887 - accuracy: 0.8231 - val_loss: 2.5503 - val_accuracy: 0.5000\n",
      "Epoch 1728/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3564 - accuracy: 0.8288 - val_loss: 2.5631 - val_accuracy: 0.5345\n",
      "Epoch 1729/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3516 - accuracy: 0.8269 - val_loss: 2.3852 - val_accuracy: 0.5172\n",
      "Epoch 1730/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3419 - accuracy: 0.8365 - val_loss: 2.3658 - val_accuracy: 0.5172\n",
      "Epoch 1731/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3482 - accuracy: 0.8346 - val_loss: 2.1119 - val_accuracy: 0.5517\n",
      "Epoch 1732/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3338 - accuracy: 0.8385 - val_loss: 2.2902 - val_accuracy: 0.5345\n",
      "Epoch 1733/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3384 - accuracy: 0.8365 - val_loss: 2.5034 - val_accuracy: 0.5517\n",
      "Epoch 1734/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3293 - accuracy: 0.8442 - val_loss: 2.0571 - val_accuracy: 0.5517\n",
      "Epoch 1735/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3418 - accuracy: 0.8327 - val_loss: 2.1006 - val_accuracy: 0.5690\n",
      "Epoch 1736/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3378 - accuracy: 0.8308 - val_loss: 2.5115 - val_accuracy: 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1737/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3421 - accuracy: 0.8346 - val_loss: 2.3294 - val_accuracy: 0.5345\n",
      "Epoch 1738/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3364 - accuracy: 0.8385 - val_loss: 2.5756 - val_accuracy: 0.5345\n",
      "Epoch 1739/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3606 - accuracy: 0.8288 - val_loss: 2.9104 - val_accuracy: 0.4828\n",
      "Epoch 1740/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3637 - accuracy: 0.8154 - val_loss: 2.5040 - val_accuracy: 0.5690\n",
      "Epoch 1741/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3384 - accuracy: 0.8346 - val_loss: 2.0123 - val_accuracy: 0.5345\n",
      "Epoch 1742/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3411 - accuracy: 0.8327 - val_loss: 2.3091 - val_accuracy: 0.5345\n",
      "Epoch 1743/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3240 - accuracy: 0.8442 - val_loss: 2.5685 - val_accuracy: 0.5000\n",
      "Epoch 1744/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3403 - accuracy: 0.8269 - val_loss: 2.2879 - val_accuracy: 0.5517\n",
      "Epoch 1745/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3370 - accuracy: 0.8385 - val_loss: 2.4882 - val_accuracy: 0.5172\n",
      "Epoch 1746/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3314 - accuracy: 0.8365 - val_loss: 2.2430 - val_accuracy: 0.5000\n",
      "Epoch 1747/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3560 - accuracy: 0.8250 - val_loss: 2.0145 - val_accuracy: 0.5862\n",
      "Epoch 1748/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3323 - accuracy: 0.8423 - val_loss: 2.6331 - val_accuracy: 0.5000\n",
      "Epoch 1749/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3733 - accuracy: 0.8288 - val_loss: 2.3387 - val_accuracy: 0.5345\n",
      "Epoch 1750/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4105 - accuracy: 0.8058 - val_loss: 2.6270 - val_accuracy: 0.5345\n",
      "Epoch 1751/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3376 - accuracy: 0.8327 - val_loss: 2.3593 - val_accuracy: 0.5345\n",
      "Epoch 1752/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3325 - accuracy: 0.8346 - val_loss: 2.5044 - val_accuracy: 0.5000\n",
      "Epoch 1753/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3406 - accuracy: 0.8346 - val_loss: 2.4501 - val_accuracy: 0.5345\n",
      "Epoch 1754/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3350 - accuracy: 0.8365 - val_loss: 2.4811 - val_accuracy: 0.5172\n",
      "Epoch 1755/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3218 - accuracy: 0.8500 - val_loss: 2.6570 - val_accuracy: 0.5172\n",
      "Epoch 1756/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3496 - accuracy: 0.8327 - val_loss: 2.3919 - val_accuracy: 0.5345\n",
      "Epoch 1757/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3457 - accuracy: 0.8327 - val_loss: 2.3657 - val_accuracy: 0.5172\n",
      "Epoch 1758/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3771 - accuracy: 0.8077 - val_loss: 1.9711 - val_accuracy: 0.5517\n",
      "Epoch 1759/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3434 - accuracy: 0.8288 - val_loss: 2.4253 - val_accuracy: 0.5345\n",
      "Epoch 1760/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3285 - accuracy: 0.8462 - val_loss: 2.4719 - val_accuracy: 0.5517\n",
      "Epoch 1761/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3348 - accuracy: 0.8365 - val_loss: 2.3454 - val_accuracy: 0.5690\n",
      "Epoch 1762/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3662 - accuracy: 0.8173 - val_loss: 2.3087 - val_accuracy: 0.5345\n",
      "Epoch 1763/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3302 - accuracy: 0.8481 - val_loss: 2.4384 - val_accuracy: 0.5345\n",
      "Epoch 1764/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3327 - accuracy: 0.8481 - val_loss: 2.4180 - val_accuracy: 0.5517\n",
      "Epoch 1765/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3330 - accuracy: 0.8442 - val_loss: 2.1004 - val_accuracy: 0.5690\n",
      "Epoch 1766/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3400 - accuracy: 0.8327 - val_loss: 2.2301 - val_accuracy: 0.5517\n",
      "Epoch 1767/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3452 - accuracy: 0.8288 - val_loss: 2.4299 - val_accuracy: 0.5345\n",
      "Epoch 1768/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3555 - accuracy: 0.8154 - val_loss: 2.3162 - val_accuracy: 0.5172\n",
      "Epoch 1769/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3455 - accuracy: 0.8308 - val_loss: 2.3412 - val_accuracy: 0.5517\n",
      "Epoch 1770/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3645 - accuracy: 0.8192 - val_loss: 2.3098 - val_accuracy: 0.5690\n",
      "Epoch 1771/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3481 - accuracy: 0.8365 - val_loss: 2.4411 - val_accuracy: 0.5345\n",
      "Epoch 1772/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3437 - accuracy: 0.8250 - val_loss: 2.4168 - val_accuracy: 0.5345\n",
      "Epoch 1773/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3376 - accuracy: 0.8365 - val_loss: 2.2203 - val_accuracy: 0.5690\n",
      "Epoch 1774/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3232 - accuracy: 0.8481 - val_loss: 2.2253 - val_accuracy: 0.5517\n",
      "Epoch 1775/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3360 - accuracy: 0.8385 - val_loss: 2.8960 - val_accuracy: 0.4828\n",
      "Epoch 1776/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3546 - accuracy: 0.8250 - val_loss: 2.4379 - val_accuracy: 0.5517\n",
      "Epoch 1777/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3406 - accuracy: 0.8404 - val_loss: 2.4432 - val_accuracy: 0.5345\n",
      "Epoch 1778/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3501 - accuracy: 0.8288 - val_loss: 2.4031 - val_accuracy: 0.5345\n",
      "Epoch 1779/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3689 - accuracy: 0.8212 - val_loss: 2.1475 - val_accuracy: 0.5517\n",
      "Epoch 1780/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3378 - accuracy: 0.8462 - val_loss: 2.3964 - val_accuracy: 0.5517\n",
      "Epoch 1781/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3581 - accuracy: 0.8250 - val_loss: 2.4681 - val_accuracy: 0.5345\n",
      "Epoch 1782/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3382 - accuracy: 0.8288 - val_loss: 2.2932 - val_accuracy: 0.5517\n",
      "Epoch 1783/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3374 - accuracy: 0.8385 - val_loss: 2.2726 - val_accuracy: 0.5517\n",
      "Epoch 1784/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3414 - accuracy: 0.8346 - val_loss: 2.6675 - val_accuracy: 0.5345\n",
      "Epoch 1785/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3445 - accuracy: 0.8288 - val_loss: 2.6759 - val_accuracy: 0.4828\n",
      "Epoch 1786/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3288 - accuracy: 0.8365 - val_loss: 2.3815 - val_accuracy: 0.5517\n",
      "Epoch 1787/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3267 - accuracy: 0.8404 - val_loss: 2.3142 - val_accuracy: 0.5517\n",
      "Epoch 1788/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3266 - accuracy: 0.8423 - val_loss: 2.4134 - val_accuracy: 0.5345\n",
      "Epoch 1789/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3375 - accuracy: 0.8462 - val_loss: 2.2128 - val_accuracy: 0.5690\n",
      "Epoch 1790/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3798 - accuracy: 0.8212 - val_loss: 2.5105 - val_accuracy: 0.5172\n",
      "Epoch 1791/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3261 - accuracy: 0.8404 - val_loss: 2.4360 - val_accuracy: 0.5690\n",
      "Epoch 1792/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3343 - accuracy: 0.8442 - val_loss: 2.2968 - val_accuracy: 0.5172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1793/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3346 - accuracy: 0.8288 - val_loss: 2.4628 - val_accuracy: 0.5000\n",
      "Epoch 1794/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3662 - accuracy: 0.8327 - val_loss: 2.3439 - val_accuracy: 0.5517\n",
      "Epoch 1795/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3396 - accuracy: 0.8327 - val_loss: 2.5044 - val_accuracy: 0.5000\n",
      "Epoch 1796/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3247 - accuracy: 0.8519 - val_loss: 2.6932 - val_accuracy: 0.5345\n",
      "Epoch 1797/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3187 - accuracy: 0.8442 - val_loss: 2.4736 - val_accuracy: 0.5345\n",
      "Epoch 1798/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3413 - accuracy: 0.8346 - val_loss: 2.8504 - val_accuracy: 0.4828\n",
      "Epoch 1799/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3310 - accuracy: 0.8288 - val_loss: 2.0720 - val_accuracy: 0.5690\n",
      "Epoch 1800/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3282 - accuracy: 0.8423 - val_loss: 2.6290 - val_accuracy: 0.5517\n",
      "Epoch 1801/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3575 - accuracy: 0.8346 - val_loss: 2.2374 - val_accuracy: 0.5345\n",
      "Epoch 1802/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3938 - accuracy: 0.8096 - val_loss: 2.3459 - val_accuracy: 0.5517\n",
      "Epoch 1803/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3399 - accuracy: 0.8385 - val_loss: 2.4415 - val_accuracy: 0.5345\n",
      "Epoch 1804/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3338 - accuracy: 0.8423 - val_loss: 2.4174 - val_accuracy: 0.5345\n",
      "Epoch 1805/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3238 - accuracy: 0.8442 - val_loss: 2.5535 - val_accuracy: 0.5345\n",
      "Epoch 1806/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3225 - accuracy: 0.8423 - val_loss: 2.5586 - val_accuracy: 0.5000\n",
      "Epoch 1807/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3428 - accuracy: 0.8308 - val_loss: 2.3180 - val_accuracy: 0.5517\n",
      "Epoch 1808/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3665 - accuracy: 0.8154 - val_loss: 2.1812 - val_accuracy: 0.5345\n",
      "Epoch 1809/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3712 - accuracy: 0.8212 - val_loss: 2.6135 - val_accuracy: 0.5172\n",
      "Epoch 1810/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3401 - accuracy: 0.8404 - val_loss: 2.3759 - val_accuracy: 0.5517\n",
      "Epoch 1811/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3317 - accuracy: 0.8308 - val_loss: 2.4026 - val_accuracy: 0.5690\n",
      "Epoch 1812/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3334 - accuracy: 0.8481 - val_loss: 2.3153 - val_accuracy: 0.5517\n",
      "Epoch 1813/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3282 - accuracy: 0.8365 - val_loss: 2.5784 - val_accuracy: 0.5172\n",
      "Epoch 1814/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3192 - accuracy: 0.8481 - val_loss: 2.5630 - val_accuracy: 0.5517\n",
      "Epoch 1815/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3192 - accuracy: 0.8500 - val_loss: 2.6136 - val_accuracy: 0.4828\n",
      "Epoch 1816/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3331 - accuracy: 0.8327 - val_loss: 2.7744 - val_accuracy: 0.5172\n",
      "Epoch 1817/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3337 - accuracy: 0.8327 - val_loss: 2.3102 - val_accuracy: 0.5517\n",
      "Epoch 1818/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3505 - accuracy: 0.8231 - val_loss: 2.5008 - val_accuracy: 0.5172\n",
      "Epoch 1819/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3292 - accuracy: 0.8385 - val_loss: 2.4013 - val_accuracy: 0.5172\n",
      "Epoch 1820/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3347 - accuracy: 0.8346 - val_loss: 2.7302 - val_accuracy: 0.5000\n",
      "Epoch 1821/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3443 - accuracy: 0.8346 - val_loss: 3.0065 - val_accuracy: 0.5345\n",
      "Epoch 1822/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3568 - accuracy: 0.8288 - val_loss: 2.6276 - val_accuracy: 0.5000\n",
      "Epoch 1823/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3346 - accuracy: 0.8308 - val_loss: 2.2574 - val_accuracy: 0.5690\n",
      "Epoch 1824/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3660 - accuracy: 0.8135 - val_loss: 2.2697 - val_accuracy: 0.5517\n",
      "Epoch 1825/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3390 - accuracy: 0.8365 - val_loss: 2.1699 - val_accuracy: 0.5690\n",
      "Epoch 1826/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3578 - accuracy: 0.8173 - val_loss: 2.5581 - val_accuracy: 0.5172\n",
      "Epoch 1827/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3330 - accuracy: 0.8462 - val_loss: 2.3639 - val_accuracy: 0.5345\n",
      "Epoch 1828/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3470 - accuracy: 0.8212 - val_loss: 2.4696 - val_accuracy: 0.5690\n",
      "Epoch 1829/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3441 - accuracy: 0.8346 - val_loss: 2.4132 - val_accuracy: 0.5517\n",
      "Epoch 1830/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3436 - accuracy: 0.8346 - val_loss: 2.8577 - val_accuracy: 0.5345\n",
      "Epoch 1831/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3438 - accuracy: 0.8308 - val_loss: 2.7266 - val_accuracy: 0.5000\n",
      "Epoch 1832/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3352 - accuracy: 0.8308 - val_loss: 3.0836 - val_accuracy: 0.5345\n",
      "Epoch 1833/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3421 - accuracy: 0.8308 - val_loss: 2.4744 - val_accuracy: 0.5517\n",
      "Epoch 1834/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3206 - accuracy: 0.8442 - val_loss: 2.7271 - val_accuracy: 0.5172\n",
      "Epoch 1835/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3473 - accuracy: 0.8308 - val_loss: 2.5824 - val_accuracy: 0.5345\n",
      "Epoch 1836/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3307 - accuracy: 0.8423 - val_loss: 2.7700 - val_accuracy: 0.5345\n",
      "Epoch 1837/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3425 - accuracy: 0.8423 - val_loss: 2.9206 - val_accuracy: 0.5345\n",
      "Epoch 1838/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4164 - accuracy: 0.8154 - val_loss: 2.5577 - val_accuracy: 0.5000\n",
      "Epoch 1839/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3696 - accuracy: 0.8173 - val_loss: 2.6047 - val_accuracy: 0.5690\n",
      "Epoch 1840/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3216 - accuracy: 0.8404 - val_loss: 2.1518 - val_accuracy: 0.5690\n",
      "Epoch 1841/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3675 - accuracy: 0.8308 - val_loss: 3.0014 - val_accuracy: 0.5172\n",
      "Epoch 1842/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3532 - accuracy: 0.8308 - val_loss: 2.8514 - val_accuracy: 0.5172\n",
      "Epoch 1843/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3388 - accuracy: 0.8385 - val_loss: 2.6138 - val_accuracy: 0.5690\n",
      "Epoch 1844/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3293 - accuracy: 0.8481 - val_loss: 2.5737 - val_accuracy: 0.5517\n",
      "Epoch 1845/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3249 - accuracy: 0.8538 - val_loss: 2.2168 - val_accuracy: 0.5517\n",
      "Epoch 1846/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3221 - accuracy: 0.8481 - val_loss: 2.5793 - val_accuracy: 0.5517\n",
      "Epoch 1847/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3361 - accuracy: 0.8327 - val_loss: 2.5290 - val_accuracy: 0.5517\n",
      "Epoch 1848/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3311 - accuracy: 0.8365 - val_loss: 2.5849 - val_accuracy: 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1849/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3346 - accuracy: 0.8365 - val_loss: 2.8686 - val_accuracy: 0.5517\n",
      "Epoch 1850/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3402 - accuracy: 0.8308 - val_loss: 2.4530 - val_accuracy: 0.5517\n",
      "Epoch 1851/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3544 - accuracy: 0.8346 - val_loss: 2.4756 - val_accuracy: 0.5172\n",
      "Epoch 1852/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3512 - accuracy: 0.8250 - val_loss: 2.8594 - val_accuracy: 0.5345\n",
      "Epoch 1853/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3659 - accuracy: 0.8288 - val_loss: 2.6597 - val_accuracy: 0.5517\n",
      "Epoch 1854/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3441 - accuracy: 0.8288 - val_loss: 2.8408 - val_accuracy: 0.5345\n",
      "Epoch 1855/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3465 - accuracy: 0.8346 - val_loss: 2.7987 - val_accuracy: 0.5345\n",
      "Epoch 1856/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3375 - accuracy: 0.8327 - val_loss: 2.5686 - val_accuracy: 0.5517\n",
      "Epoch 1857/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3391 - accuracy: 0.8250 - val_loss: 2.7032 - val_accuracy: 0.5345\n",
      "Epoch 1858/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3925 - accuracy: 0.7981 - val_loss: 2.0834 - val_accuracy: 0.5517\n",
      "Epoch 1859/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3476 - accuracy: 0.8288 - val_loss: 2.5973 - val_accuracy: 0.5172\n",
      "Epoch 1860/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3473 - accuracy: 0.8250 - val_loss: 2.5293 - val_accuracy: 0.5517\n",
      "Epoch 1861/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3709 - accuracy: 0.8250 - val_loss: 2.1859 - val_accuracy: 0.5690\n",
      "Epoch 1862/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3818 - accuracy: 0.8212 - val_loss: 2.0219 - val_accuracy: 0.5862\n",
      "Epoch 1863/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3420 - accuracy: 0.8346 - val_loss: 2.6119 - val_accuracy: 0.5345\n",
      "Epoch 1864/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3292 - accuracy: 0.8404 - val_loss: 2.8167 - val_accuracy: 0.5345\n",
      "Epoch 1865/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3343 - accuracy: 0.8423 - val_loss: 2.4243 - val_accuracy: 0.5690\n",
      "Epoch 1866/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3296 - accuracy: 0.8404 - val_loss: 2.6425 - val_accuracy: 0.5172\n",
      "Epoch 1867/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3267 - accuracy: 0.8423 - val_loss: 2.5602 - val_accuracy: 0.5517\n",
      "Epoch 1868/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3307 - accuracy: 0.8365 - val_loss: 2.3786 - val_accuracy: 0.5690\n",
      "Epoch 1869/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3309 - accuracy: 0.8346 - val_loss: 2.6775 - val_accuracy: 0.5172\n",
      "Epoch 1870/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3373 - accuracy: 0.8346 - val_loss: 2.5026 - val_accuracy: 0.5345\n",
      "Epoch 1871/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3400 - accuracy: 0.8288 - val_loss: 2.4621 - val_accuracy: 0.5690\n",
      "Epoch 1872/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3544 - accuracy: 0.8250 - val_loss: 2.2859 - val_accuracy: 0.5517\n",
      "Epoch 1873/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3808 - accuracy: 0.7904 - val_loss: 2.3575 - val_accuracy: 0.5517\n",
      "Epoch 1874/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3378 - accuracy: 0.8346 - val_loss: 2.8012 - val_accuracy: 0.5172\n",
      "Epoch 1875/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3401 - accuracy: 0.8288 - val_loss: 2.4460 - val_accuracy: 0.5862\n",
      "Epoch 1876/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3350 - accuracy: 0.8346 - val_loss: 2.5683 - val_accuracy: 0.5517\n",
      "Epoch 1877/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3258 - accuracy: 0.8442 - val_loss: 2.4416 - val_accuracy: 0.5862\n",
      "Epoch 1878/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3294 - accuracy: 0.8308 - val_loss: 2.3514 - val_accuracy: 0.5690\n",
      "Epoch 1879/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3360 - accuracy: 0.8327 - val_loss: 2.5544 - val_accuracy: 0.5862\n",
      "Epoch 1880/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3658 - accuracy: 0.8212 - val_loss: 2.7859 - val_accuracy: 0.5517\n",
      "Epoch 1881/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3533 - accuracy: 0.8250 - val_loss: 2.3310 - val_accuracy: 0.5517\n",
      "Epoch 1882/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3330 - accuracy: 0.8404 - val_loss: 2.6085 - val_accuracy: 0.5690\n",
      "Epoch 1883/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3415 - accuracy: 0.8250 - val_loss: 2.1881 - val_accuracy: 0.5862\n",
      "Epoch 1884/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3755 - accuracy: 0.8115 - val_loss: 2.1602 - val_accuracy: 0.5690\n",
      "Epoch 1885/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3667 - accuracy: 0.8154 - val_loss: 2.7659 - val_accuracy: 0.5172\n",
      "Epoch 1886/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3873 - accuracy: 0.8212 - val_loss: 2.6044 - val_accuracy: 0.5345\n",
      "Epoch 1887/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3784 - accuracy: 0.8019 - val_loss: 2.4583 - val_accuracy: 0.5517\n",
      "Epoch 1888/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3568 - accuracy: 0.8231 - val_loss: 2.7590 - val_accuracy: 0.5517\n",
      "Epoch 1889/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3512 - accuracy: 0.8327 - val_loss: 2.8134 - val_accuracy: 0.5172\n",
      "Epoch 1890/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3590 - accuracy: 0.8250 - val_loss: 2.3592 - val_accuracy: 0.5690\n",
      "Epoch 1891/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3387 - accuracy: 0.8308 - val_loss: 2.6134 - val_accuracy: 0.5517\n",
      "Epoch 1892/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3340 - accuracy: 0.8385 - val_loss: 2.7749 - val_accuracy: 0.5345\n",
      "Epoch 1893/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3557 - accuracy: 0.8231 - val_loss: 2.6146 - val_accuracy: 0.5517\n",
      "Epoch 1894/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3465 - accuracy: 0.8288 - val_loss: 3.0451 - val_accuracy: 0.4655\n",
      "Epoch 1895/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3832 - accuracy: 0.8038 - val_loss: 2.4742 - val_accuracy: 0.5172\n",
      "Epoch 1896/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4963 - accuracy: 0.7712 - val_loss: 2.1365 - val_accuracy: 0.5345\n",
      "Epoch 1897/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4053 - accuracy: 0.8058 - val_loss: 2.7272 - val_accuracy: 0.5517\n",
      "Epoch 1898/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3562 - accuracy: 0.8250 - val_loss: 2.5755 - val_accuracy: 0.5345\n",
      "Epoch 1899/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3607 - accuracy: 0.8154 - val_loss: 2.8280 - val_accuracy: 0.5000\n",
      "Epoch 1900/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3520 - accuracy: 0.8250 - val_loss: 2.9807 - val_accuracy: 0.5172\n",
      "Epoch 1901/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3467 - accuracy: 0.8192 - val_loss: 2.8581 - val_accuracy: 0.5000\n",
      "Epoch 1902/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3379 - accuracy: 0.8231 - val_loss: 2.7912 - val_accuracy: 0.5000\n",
      "Epoch 1903/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3534 - accuracy: 0.8192 - val_loss: 2.7723 - val_accuracy: 0.5172\n",
      "Epoch 1904/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3428 - accuracy: 0.8250 - val_loss: 2.6416 - val_accuracy: 0.5517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1905/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3669 - accuracy: 0.8058 - val_loss: 2.5330 - val_accuracy: 0.5345\n",
      "Epoch 1906/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3514 - accuracy: 0.8288 - val_loss: 2.8144 - val_accuracy: 0.5172\n",
      "Epoch 1907/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3857 - accuracy: 0.7942 - val_loss: 2.7733 - val_accuracy: 0.5172\n",
      "Epoch 1908/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3506 - accuracy: 0.8250 - val_loss: 2.6048 - val_accuracy: 0.5172\n",
      "Epoch 1909/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3391 - accuracy: 0.8308 - val_loss: 2.8074 - val_accuracy: 0.5000\n",
      "Epoch 1910/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3358 - accuracy: 0.8288 - val_loss: 3.1344 - val_accuracy: 0.4655\n",
      "Epoch 1911/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3433 - accuracy: 0.8288 - val_loss: 2.7697 - val_accuracy: 0.5000\n",
      "Epoch 1912/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3439 - accuracy: 0.8327 - val_loss: 2.8886 - val_accuracy: 0.5172\n",
      "Epoch 1913/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3349 - accuracy: 0.8346 - val_loss: 2.7983 - val_accuracy: 0.5172\n",
      "Epoch 1914/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3994 - accuracy: 0.8077 - val_loss: 2.2709 - val_accuracy: 0.5517\n",
      "Epoch 1915/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3694 - accuracy: 0.8154 - val_loss: 3.1713 - val_accuracy: 0.4483\n",
      "Epoch 1916/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3985 - accuracy: 0.7904 - val_loss: 2.4034 - val_accuracy: 0.5345\n",
      "Epoch 1917/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3584 - accuracy: 0.8154 - val_loss: 3.0568 - val_accuracy: 0.4655\n",
      "Epoch 1918/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3730 - accuracy: 0.8212 - val_loss: 2.7297 - val_accuracy: 0.5172\n",
      "Epoch 1919/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3792 - accuracy: 0.8038 - val_loss: 2.4425 - val_accuracy: 0.5345\n",
      "Epoch 1920/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3523 - accuracy: 0.8212 - val_loss: 2.6286 - val_accuracy: 0.5345\n",
      "Epoch 1921/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3322 - accuracy: 0.8327 - val_loss: 2.6337 - val_accuracy: 0.5172\n",
      "Epoch 1922/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3381 - accuracy: 0.8231 - val_loss: 3.2277 - val_accuracy: 0.5172\n",
      "Epoch 1923/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3454 - accuracy: 0.8250 - val_loss: 2.5469 - val_accuracy: 0.5345\n",
      "Epoch 1924/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3264 - accuracy: 0.8288 - val_loss: 2.6954 - val_accuracy: 0.5172\n",
      "Epoch 1925/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3170 - accuracy: 0.8481 - val_loss: 2.2646 - val_accuracy: 0.5517\n",
      "Epoch 1926/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3438 - accuracy: 0.8231 - val_loss: 2.6681 - val_accuracy: 0.4828\n",
      "Epoch 1927/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3314 - accuracy: 0.8288 - val_loss: 2.9139 - val_accuracy: 0.5000\n",
      "Epoch 1928/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3625 - accuracy: 0.8135 - val_loss: 2.6671 - val_accuracy: 0.5345\n",
      "Epoch 1929/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3408 - accuracy: 0.8327 - val_loss: 2.5355 - val_accuracy: 0.5172\n",
      "Epoch 1930/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3471 - accuracy: 0.8192 - val_loss: 2.3507 - val_accuracy: 0.5517\n",
      "Epoch 1931/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3404 - accuracy: 0.8288 - val_loss: 2.8548 - val_accuracy: 0.5517\n",
      "Epoch 1932/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4135 - accuracy: 0.7962 - val_loss: 2.4333 - val_accuracy: 0.5345\n",
      "Epoch 1933/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3377 - accuracy: 0.8346 - val_loss: 2.4404 - val_accuracy: 0.5345\n",
      "Epoch 1934/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3413 - accuracy: 0.8404 - val_loss: 2.8285 - val_accuracy: 0.5172\n",
      "Epoch 1935/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3371 - accuracy: 0.8365 - val_loss: 2.5910 - val_accuracy: 0.5345\n",
      "Epoch 1936/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3270 - accuracy: 0.8346 - val_loss: 2.7723 - val_accuracy: 0.5517\n",
      "Epoch 1937/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3430 - accuracy: 0.8288 - val_loss: 2.5877 - val_accuracy: 0.5172\n",
      "Epoch 1938/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3449 - accuracy: 0.8231 - val_loss: 2.4842 - val_accuracy: 0.5172\n",
      "Epoch 1939/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3517 - accuracy: 0.8173 - val_loss: 2.4941 - val_accuracy: 0.5517\n",
      "Epoch 1940/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3693 - accuracy: 0.8250 - val_loss: 2.5823 - val_accuracy: 0.5517\n",
      "Epoch 1941/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3885 - accuracy: 0.8000 - val_loss: 2.1971 - val_accuracy: 0.5517\n",
      "Epoch 1942/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4113 - accuracy: 0.8038 - val_loss: 2.2681 - val_accuracy: 0.5690\n",
      "Epoch 1943/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.4174 - accuracy: 0.7981 - val_loss: 2.4186 - val_accuracy: 0.5690\n",
      "Epoch 1944/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3566 - accuracy: 0.8269 - val_loss: 2.3034 - val_accuracy: 0.5862\n",
      "Epoch 1945/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3376 - accuracy: 0.8288 - val_loss: 2.8832 - val_accuracy: 0.5000\n",
      "Epoch 1946/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3486 - accuracy: 0.8173 - val_loss: 2.5321 - val_accuracy: 0.5345\n",
      "Epoch 1947/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3159 - accuracy: 0.8500 - val_loss: 2.2803 - val_accuracy: 0.5345\n",
      "Epoch 1948/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3282 - accuracy: 0.8404 - val_loss: 2.5916 - val_accuracy: 0.5000\n",
      "Epoch 1949/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3657 - accuracy: 0.8269 - val_loss: 2.5841 - val_accuracy: 0.5517\n",
      "Epoch 1950/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3214 - accuracy: 0.8442 - val_loss: 2.6358 - val_accuracy: 0.5345\n",
      "Epoch 1951/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3317 - accuracy: 0.8365 - val_loss: 2.4328 - val_accuracy: 0.5345\n",
      "Epoch 1952/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3351 - accuracy: 0.8385 - val_loss: 2.3894 - val_accuracy: 0.5517\n",
      "Epoch 1953/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3196 - accuracy: 0.8442 - val_loss: 2.2893 - val_accuracy: 0.5690\n",
      "Epoch 1954/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3295 - accuracy: 0.8385 - val_loss: 2.5751 - val_accuracy: 0.5517\n",
      "Epoch 1955/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3525 - accuracy: 0.8288 - val_loss: 2.6249 - val_accuracy: 0.5345\n",
      "Epoch 1956/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3617 - accuracy: 0.8192 - val_loss: 2.8722 - val_accuracy: 0.5172\n",
      "Epoch 1957/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3312 - accuracy: 0.8365 - val_loss: 2.7314 - val_accuracy: 0.5172\n",
      "Epoch 1958/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3222 - accuracy: 0.8462 - val_loss: 2.6776 - val_accuracy: 0.5172\n",
      "Epoch 1959/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3393 - accuracy: 0.8231 - val_loss: 2.3390 - val_accuracy: 0.5517\n",
      "Epoch 1960/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3309 - accuracy: 0.8365 - val_loss: 3.0774 - val_accuracy: 0.5345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1961/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3511 - accuracy: 0.8269 - val_loss: 2.9622 - val_accuracy: 0.5172\n",
      "Epoch 1962/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3318 - accuracy: 0.8346 - val_loss: 2.4763 - val_accuracy: 0.5517\n",
      "Epoch 1963/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3241 - accuracy: 0.8423 - val_loss: 2.4427 - val_accuracy: 0.5862\n",
      "Epoch 1964/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3328 - accuracy: 0.8404 - val_loss: 2.7708 - val_accuracy: 0.5000\n",
      "Epoch 1965/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3191 - accuracy: 0.8519 - val_loss: 2.8588 - val_accuracy: 0.5000\n",
      "Epoch 1966/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3516 - accuracy: 0.8269 - val_loss: 2.6685 - val_accuracy: 0.5000\n",
      "Epoch 1967/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3456 - accuracy: 0.8269 - val_loss: 2.5489 - val_accuracy: 0.5345\n",
      "Epoch 1968/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3521 - accuracy: 0.8288 - val_loss: 2.8646 - val_accuracy: 0.5172\n",
      "Epoch 1969/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3286 - accuracy: 0.8365 - val_loss: 2.7102 - val_accuracy: 0.5172\n",
      "Epoch 1970/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3305 - accuracy: 0.8365 - val_loss: 2.8713 - val_accuracy: 0.5000\n",
      "Epoch 1971/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3199 - accuracy: 0.8538 - val_loss: 2.8014 - val_accuracy: 0.5345\n",
      "Epoch 1972/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3257 - accuracy: 0.8365 - val_loss: 2.7908 - val_accuracy: 0.5000\n",
      "Epoch 1973/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3272 - accuracy: 0.8442 - val_loss: 2.7394 - val_accuracy: 0.5172\n",
      "Epoch 1974/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3595 - accuracy: 0.8308 - val_loss: 2.2299 - val_accuracy: 0.5862\n",
      "Epoch 1975/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3258 - accuracy: 0.8462 - val_loss: 3.1309 - val_accuracy: 0.5517\n",
      "Epoch 1976/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3271 - accuracy: 0.8442 - val_loss: 2.6768 - val_accuracy: 0.5172\n",
      "Epoch 1977/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3279 - accuracy: 0.8404 - val_loss: 2.5117 - val_accuracy: 0.5690\n",
      "Epoch 1978/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3423 - accuracy: 0.8365 - val_loss: 2.6950 - val_accuracy: 0.5172\n",
      "Epoch 1979/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3386 - accuracy: 0.8327 - val_loss: 2.6217 - val_accuracy: 0.5690\n",
      "Epoch 1980/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3298 - accuracy: 0.8269 - val_loss: 3.0694 - val_accuracy: 0.5172\n",
      "Epoch 1981/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3213 - accuracy: 0.8481 - val_loss: 2.5399 - val_accuracy: 0.5172\n",
      "Epoch 1982/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3184 - accuracy: 0.8462 - val_loss: 2.1721 - val_accuracy: 0.5690\n",
      "Epoch 1983/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3408 - accuracy: 0.8269 - val_loss: 2.4635 - val_accuracy: 0.5345\n",
      "Epoch 1984/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3153 - accuracy: 0.8481 - val_loss: 2.5509 - val_accuracy: 0.5345\n",
      "Epoch 1985/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3207 - accuracy: 0.8500 - val_loss: 2.5814 - val_accuracy: 0.5517\n",
      "Epoch 1986/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3200 - accuracy: 0.8500 - val_loss: 2.8635 - val_accuracy: 0.5690\n",
      "Epoch 1987/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3262 - accuracy: 0.8385 - val_loss: 2.4052 - val_accuracy: 0.5862\n",
      "Epoch 1988/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3633 - accuracy: 0.8231 - val_loss: 2.6328 - val_accuracy: 0.5345\n",
      "Epoch 1989/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3332 - accuracy: 0.8327 - val_loss: 2.2630 - val_accuracy: 0.5517\n",
      "Epoch 1990/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3221 - accuracy: 0.8404 - val_loss: 2.7272 - val_accuracy: 0.5172\n",
      "Epoch 1991/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3163 - accuracy: 0.8462 - val_loss: 2.4883 - val_accuracy: 0.5517\n",
      "Epoch 1992/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3882 - accuracy: 0.8019 - val_loss: 2.2690 - val_accuracy: 0.5517\n",
      "Epoch 1993/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3483 - accuracy: 0.8212 - val_loss: 2.4130 - val_accuracy: 0.5517\n",
      "Epoch 1994/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3313 - accuracy: 0.8365 - val_loss: 2.4453 - val_accuracy: 0.5862\n",
      "Epoch 1995/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3386 - accuracy: 0.8346 - val_loss: 2.3506 - val_accuracy: 0.5862\n",
      "Epoch 1996/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3276 - accuracy: 0.8365 - val_loss: 2.7436 - val_accuracy: 0.5517\n",
      "Epoch 1997/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3318 - accuracy: 0.8404 - val_loss: 2.7435 - val_accuracy: 0.5517\n",
      "Epoch 1998/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3346 - accuracy: 0.8346 - val_loss: 2.6284 - val_accuracy: 0.5517\n",
      "Epoch 1999/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3278 - accuracy: 0.8442 - val_loss: 2.8405 - val_accuracy: 0.5345\n",
      "Epoch 2000/2000\n",
      "520/520 [==============================] - 1s 1ms/step - loss: 0.3232 - accuracy: 0.8385 - val_loss: 2.5084 - val_accuracy: 0.5345\n"
     ]
    }
   ],
   "source": [
    "model_simple = build_model_simple()\n",
    "\n",
    "history = model_simple.fit(X_train, y_train, validation_split=0.10, epochs=2000, batch_size=2,  verbose=1) # validation_split=0.05,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.723194694519043, 0.5799999833106995]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_simple.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_simple.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894],\n",
       "       [0.6102894]], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429/429 [==============================] - 0s 76us/step\n",
      "[0.6767883127108043, 0.5920745730400085]\n"
     ]
    }
   ],
   "source": [
    "eval_model=model.evaluate(X_train, y_train) # loss value & metrics\n",
    "print(eval_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=build_model, epochs=100, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "386/386 [==============================] - 0s 1ms/step - loss: 0.6886 - accuracy: 0.5907\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6922 - accuracy: 0.5907\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6814 - accuracy: 0.5907\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6860 - accuracy: 0.5907\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6832 - accuracy: 0.5907\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6820 - accuracy: 0.5907\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 643us/step - loss: 0.6829 - accuracy: 0.5907\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6868 - accuracy: 0.5907\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6834 - accuracy: 0.5907\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6819 - accuracy: 0.5907\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 648us/step - loss: 0.6803 - accuracy: 0.5907\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6808 - accuracy: 0.5907\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 648us/step - loss: 0.6808 - accuracy: 0.5907\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 614us/step - loss: 0.6789 - accuracy: 0.5907\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6803 - accuracy: 0.5907\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6780 - accuracy: 0.5907\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 718us/step - loss: 0.6835 - accuracy: 0.5907\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 636us/step - loss: 0.6785 - accuracy: 0.5907\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6827 - accuracy: 0.5907\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6795 - accuracy: 0.5907\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 618us/step - loss: 0.6787 - accuracy: 0.5907\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 653us/step - loss: 0.6791 - accuracy: 0.5907\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6808 - accuracy: 0.5907\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6796 - accuracy: 0.5907\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 749us/step - loss: 0.6788 - accuracy: 0.5907\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6786 - accuracy: 0.5907\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 650us/step - loss: 0.6809 - accuracy: 0.5907\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6790 - accuracy: 0.5907\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6803 - accuracy: 0.5907\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6777 - accuracy: 0.5907\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6795 - accuracy: 0.5907\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6779 - accuracy: 0.5907\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6778 - accuracy: 0.5907\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 661us/step - loss: 0.6787 - accuracy: 0.5907\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 629us/step - loss: 0.6786 - accuracy: 0.5907\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6782 - accuracy: 0.5907\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6779 - accuracy: 0.5907\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6780 - accuracy: 0.5907\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6781 - accuracy: 0.5907\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 692us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6760 - accuracy: 0.5907\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6781 - accuracy: 0.5907\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 749us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 731us/step - loss: 0.6777 - accuracy: 0.5907\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6777 - accuracy: 0.5907\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 688us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 757us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 656us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6762 - accuracy: 0.5907\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 718us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 705us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 795us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 710us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6785 - accuracy: 0.5907\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 653us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 671us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 617us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 643us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 643us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 614us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 661us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 617us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 636us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 713us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 731us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 648us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 700us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 759us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 649us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 0s 1ms/step - loss: 0.6918 - accuracy: 0.5855\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6806 - accuracy: 0.5907\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 615us/step - loss: 0.6839 - accuracy: 0.5907\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6885 - accuracy: 0.5907\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6836 - accuracy: 0.5907\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 610us/step - loss: 0.6818 - accuracy: 0.5907\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6860 - accuracy: 0.5907\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6828 - accuracy: 0.5907\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 614us/step - loss: 0.6854 - accuracy: 0.5907\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6762 - accuracy: 0.5881\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 775us/step - loss: 0.6844 - accuracy: 0.5855\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 650us/step - loss: 0.6836 - accuracy: 0.5907\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 692us/step - loss: 0.6821 - accuracy: 0.5907\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 702us/step - loss: 0.6806 - accuracy: 0.5907\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6792 - accuracy: 0.5907\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 626us/step - loss: 0.6796 - accuracy: 0.5907\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6804 - accuracy: 0.5907\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6810 - accuracy: 0.5907\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6781 - accuracy: 0.5907\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 693us/step - loss: 0.6797 - accuracy: 0.5907\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 666us/step - loss: 0.6798 - accuracy: 0.5907\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 767us/step - loss: 0.6795 - accuracy: 0.5907\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 795us/step - loss: 0.6796 - accuracy: 0.5907\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 830us/step - loss: 0.6790 - accuracy: 0.5907\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 715us/step - loss: 0.6786 - accuracy: 0.5907\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 668us/step - loss: 0.6799 - accuracy: 0.5907\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6788 - accuracy: 0.5907\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6783 - accuracy: 0.5907\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 587us/step - loss: 0.6802 - accuracy: 0.5907\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6782 - accuracy: 0.5907\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6782 - accuracy: 0.5907\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6787 - accuracy: 0.5907\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6779 - accuracy: 0.5907\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6780 - accuracy: 0.5907\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6780 - accuracy: 0.5907\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 579us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6758 - accuracy: 0.5907\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6805 - accuracy: 0.5907\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6779 - accuracy: 0.5907\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 724us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 700us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 796us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 718us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 653us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 579us/step - loss: 0.6763 - accuracy: 0.5907\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6780 - accuracy: 0.5907\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 550us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 750us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 668us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 610us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 731us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 697us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 762us/step - loss: 0.6778 - accuracy: 0.5907\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 775us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 676us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 731us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 826us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 813us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 762us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 681us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 705us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 720us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 643us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 772us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 705us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 732us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 731us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 623us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 629us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 649us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6781 - accuracy: 0.5907\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 694us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 813us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 712us/step - loss: 0.6793 - accuracy: 0.5907\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 671us/step - loss: 0.6782 - accuracy: 0.5907\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 653us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 692us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 699us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 839us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 681us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 785us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 847us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 813us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 1s 1ms/step - loss: 0.6847 - accuracy: 0.5907\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 749us/step - loss: 0.6842 - accuracy: 0.5907\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 845us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 723us/step - loss: 0.6883 - accuracy: 0.5907\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 734us/step - loss: 0.6851 - accuracy: 0.5907\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6855 - accuracy: 0.5907\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 679us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 751us/step - loss: 0.6847 - accuracy: 0.5907\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 653us/step - loss: 0.6884 - accuracy: 0.5907\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 734us/step - loss: 0.6864 - accuracy: 0.5907\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6795 - accuracy: 0.5907\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6849 - accuracy: 0.5907\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6839 - accuracy: 0.5907\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6825 - accuracy: 0.5907\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6797 - accuracy: 0.5907\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6805 - accuracy: 0.5907\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6811 - accuracy: 0.5907\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 780us/step - loss: 0.6815 - accuracy: 0.5907\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6796 - accuracy: 0.5907\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6796 - accuracy: 0.5907\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.59 - 0s 591us/step - loss: 0.6799 - accuracy: 0.5907\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6775 - accuracy: 0.5907\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6894 - accuracy: 0.5907\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6785 - accuracy: 0.5907\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6779 - accuracy: 0.5907\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6788 - accuracy: 0.5907\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 648us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 709us/step - loss: 0.6778 - accuracy: 0.5907\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 715us/step - loss: 0.6775 - accuracy: 0.5907\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 730us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 720us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 806us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 697us/step - loss: 0.6785 - accuracy: 0.5907\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 648us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 772us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 699us/step - loss: 0.6764 - accuracy: 0.5907\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 617us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 597us/step - loss: 0.6757 - accuracy: 0.5907\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6800 - accuracy: 0.5907\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 561us/step - loss: 0.6765 - accuracy: 0.5907\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 607us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6767 - accuracy: 0.5881\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 600us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6765 - accuracy: 0.5907\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 672us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 782us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 801us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 687us/step - loss: 0.6765 - accuracy: 0.5907\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 785us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 788us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 664us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 576us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 694us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 710us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6765 - accuracy: 0.5907\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 620us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 692us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 650us/step - loss: 0.6765 - accuracy: 0.5907\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 725us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 668us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 669us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 854us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 715us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 743us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 1s 1ms/step - loss: 0.6857 - accuracy: 0.5881\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 842us/step - loss: 0.6881 - accuracy: 0.5907\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 852us/step - loss: 0.6892 - accuracy: 0.5907\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 764us/step - loss: 0.6852 - accuracy: 0.5907\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 689us/step - loss: 0.6775 - accuracy: 0.5907\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 679us/step - loss: 0.6832 - accuracy: 0.5907\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 777us/step - loss: 0.6827 - accuracy: 0.5907\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 648us/step - loss: 0.6840 - accuracy: 0.5907\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 679us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 655us/step - loss: 0.6836 - accuracy: 0.5907\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 650us/step - loss: 0.6841 - accuracy: 0.5907\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6828 - accuracy: 0.5907\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6818 - accuracy: 0.5907\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6837 - accuracy: 0.5907\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6805 - accuracy: 0.5907\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6809 - accuracy: 0.5907\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 599us/step - loss: 0.6807 - accuracy: 0.5907\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6808 - accuracy: 0.5907\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6804 - accuracy: 0.5907\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6791 - accuracy: 0.5907\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6779 - accuracy: 0.5907\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 601us/step - loss: 0.6788 - accuracy: 0.5907\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6781 - accuracy: 0.5907\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6821 - accuracy: 0.5907\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6797 - accuracy: 0.5907\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6786 - accuracy: 0.5907\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6789 - accuracy: 0.5907\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 614us/step - loss: 0.6783 - accuracy: 0.5907\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6788 - accuracy: 0.5907\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6784 - accuracy: 0.5907\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 644us/step - loss: 0.6784 - accuracy: 0.5907\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 730us/step - loss: 0.6775 - accuracy: 0.5907\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 653us/step - loss: 0.6779 - accuracy: 0.5907\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 722us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 728us/step - loss: 0.6781 - accuracy: 0.5907\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 600us/step - loss: 0.6776 - accuracy: 0.5907\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 601us/step - loss: 0.6773 - accuracy: 0.5907\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 579us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 832us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 786us/step - loss: 0.6760 - accuracy: 0.5907\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 668us/step - loss: 0.6777 - accuracy: 0.5907\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 710us/step - loss: 0.6774 - accuracy: 0.5907\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 756us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 785us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 799us/step - loss: 0.6777 - accuracy: 0.5907\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 759us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 956us/step - loss: 0.6777 - accuracy: 0.5907\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 824us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 917us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 816us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 883us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 788us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 762us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 661us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 780us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 780us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 798us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 728us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 702us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 749us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 684us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 769us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 702us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 661us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 597us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6770 - accuracy: 0.5907\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6772 - accuracy: 0.5907\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 602us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 598us/step - loss: 0.6771 - accuracy: 0.5907\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 572us/step - loss: 0.6765 - accuracy: 0.5907\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6766 - accuracy: 0.5907\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 564us/step - loss: 0.6769 - accuracy: 0.5907\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6767 - accuracy: 0.5907\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 574us/step - loss: 0.6768 - accuracy: 0.5907\n",
      "43/43 [==============================] - 0s 977us/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 0s 1ms/step - loss: 0.6813 - accuracy: 0.5881\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6873 - accuracy: 0.5907\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6835 - accuracy: 0.5933\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6837 - accuracy: 0.5933\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6755 - accuracy: 0.5933\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6812 - accuracy: 0.5933\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6852 - accuracy: 0.5933\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6858 - accuracy: 0.5933\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6816 - accuracy: 0.5933\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6754 - accuracy: 0.5933\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6840 - accuracy: 0.5933\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 697us/step - loss: 0.6824 - accuracy: 0.5933\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6803 - accuracy: 0.5933\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6806 - accuracy: 0.5933\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 705us/step - loss: 0.6775 - accuracy: 0.5933\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6804 - accuracy: 0.5933\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 544us/step - loss: 0.6798 - accuracy: 0.5933\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6775 - accuracy: 0.5933\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6790 - accuracy: 0.5933\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6785 - accuracy: 0.5933\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 574us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6787 - accuracy: 0.5933\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6786 - accuracy: 0.5933\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6784 - accuracy: 0.5933\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 738us/step - loss: 0.6776 - accuracy: 0.5933\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6785 - accuracy: 0.5933\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 571us/step - loss: 0.6774 - accuracy: 0.5933\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6775 - accuracy: 0.5933\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 648us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 666us/step - loss: 0.6772 - accuracy: 0.5933\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 667us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 666us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6780 - accuracy: 0.5933\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 702us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6770 - accuracy: 0.5933\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6765 - accuracy: 0.5933\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6755 - accuracy: 0.5933\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6776 - accuracy: 0.5933\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6765 - accuracy: 0.5933\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 569us/step - loss: 0.6756 - accuracy: 0.5933\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 675us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 650us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 572us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 549us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 556us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 561us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 556us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 599us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6756 - accuracy: 0.5933\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 547us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 576us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 544us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 544us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 0s 1000us/step - loss: 0.6882 - accuracy: 0.5725\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6816 - accuracy: 0.5881\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 539us/step - loss: 0.6815 - accuracy: 0.5933\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6894 - accuracy: 0.5933\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6807 - accuracy: 0.5933\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6817 - accuracy: 0.5933\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 601us/step - loss: 0.6785 - accuracy: 0.5933\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6833 - accuracy: 0.5933\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6825 - accuracy: 0.5933\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6801 - accuracy: 0.5933\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6799 - accuracy: 0.5933\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6797 - accuracy: 0.5933\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6802 - accuracy: 0.5933\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6818 - accuracy: 0.5933\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6805 - accuracy: 0.5933\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6787 - accuracy: 0.5933\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6778 - accuracy: 0.5933\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6790 - accuracy: 0.5933\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6793 - accuracy: 0.5933\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6778 - accuracy: 0.5933\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 552us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6787 - accuracy: 0.5933\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 581us/step - loss: 0.6779 - accuracy: 0.5933\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6794 - accuracy: 0.5933\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6796 - accuracy: 0.5933\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 601us/step - loss: 0.6776 - accuracy: 0.5933\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6784 - accuracy: 0.5933\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6778 - accuracy: 0.5933\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6775 - accuracy: 0.5907\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6773 - accuracy: 0.5881\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6791 - accuracy: 0.5933\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6774 - accuracy: 0.5933\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6785 - accuracy: 0.5933\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 666us/step - loss: 0.6782 - accuracy: 0.5933\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6770 - accuracy: 0.5933\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 595us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6768 - accuracy: 0.5933\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6749 - accuracy: 0.5933\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 594us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 656us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 580us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 536us/step - loss: 0.6773 - accuracy: 0.5933\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 679us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 566us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6755 - accuracy: 0.5933\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6772 - accuracy: 0.5933\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 694us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 674us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6756 - accuracy: 0.5933\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 694us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 600us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 536us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 605us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 618us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 668us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 762us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 723us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 692us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 700us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 712us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 668us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 632us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 731us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 777us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 617us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 624us/step - loss: 0.6774 - accuracy: 0.5933\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 664us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 644us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 618us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 630us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 689us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 694us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 701us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 725us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 723us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 686us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 0s 1ms/step - loss: 0.6871 - accuracy: 0.5933\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6883 - accuracy: 0.5933\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 606us/step - loss: 0.6846 - accuracy: 0.5933\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 679us/step - loss: 0.6827 - accuracy: 0.5933\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6844 - accuracy: 0.5933\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 728us/step - loss: 0.6823 - accuracy: 0.5933\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 679us/step - loss: 0.6828 - accuracy: 0.5933\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 705us/step - loss: 0.6800 - accuracy: 0.5933\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 668us/step - loss: 0.6835 - accuracy: 0.5933\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6830 - accuracy: 0.5933\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6771 - accuracy: 0.5933\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6801 - accuracy: 0.5933\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6816 - accuracy: 0.5933\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 602us/step - loss: 0.6803 - accuracy: 0.5933\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6803 - accuracy: 0.5933\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 589us/step - loss: 0.6774 - accuracy: 0.5933\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6824 - accuracy: 0.5933\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6803 - accuracy: 0.5933\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6778 - accuracy: 0.5933\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6785 - accuracy: 0.5933\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6777 - accuracy: 0.5933\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6805 - accuracy: 0.5933\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 571us/step - loss: 0.6805 - accuracy: 0.5933\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6786 - accuracy: 0.5933\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 564us/step - loss: 0.6800 - accuracy: 0.5933\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6785 - accuracy: 0.5933\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6782 - accuracy: 0.5933\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6770 - accuracy: 0.5933\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 572us/step - loss: 0.6788 - accuracy: 0.5933\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6782 - accuracy: 0.5933\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6778 - accuracy: 0.5933\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6772 - accuracy: 0.5933\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6771 - accuracy: 0.5933\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6765 - accuracy: 0.5933\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6755 - accuracy: 0.5933\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6774 - accuracy: 0.5933\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6772 - accuracy: 0.5933\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 594us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6765 - accuracy: 0.5933\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 694us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 703us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 666us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 589us/step - loss: 0.6756 - accuracy: 0.5933\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 698us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 697us/step - loss: 0.6756 - accuracy: 0.5933\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 744us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 710us/step - loss: 0.6755 - accuracy: 0.5933\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 619us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 617us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 622us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 617us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 627us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 640us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 614us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 709us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 679us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 699us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 702us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 689us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 702us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 635us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 684us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 661us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 663us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 671us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 725us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 645us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 746us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 609us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 582us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "43/43 [==============================] - 0s 1ms/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 0s 1ms/step - loss: 0.6881 - accuracy: 0.5518\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6834 - accuracy: 0.5907\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "386/386 [==============================] - 0s 583us/step - loss: 0.6821 - accuracy: 0.5933\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6800 - accuracy: 0.5933\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6843 - accuracy: 0.5933\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6771 - accuracy: 0.5933\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6865 - accuracy: 0.5933\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6843 - accuracy: 0.5933\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6842 - accuracy: 0.5933\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6813 - accuracy: 0.5933\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6865 - accuracy: 0.5933\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6826 - accuracy: 0.5933\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 571us/step - loss: 0.6809 - accuracy: 0.5933\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6781 - accuracy: 0.5933\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6814 - accuracy: 0.5933\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6802 - accuracy: 0.5933\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6782 - accuracy: 0.5933\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 555us/step - loss: 0.6789 - accuracy: 0.5933\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6791 - accuracy: 0.5933\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6799 - accuracy: 0.5933\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6779 - accuracy: 0.5933\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6781 - accuracy: 0.5933\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6768 - accuracy: 0.5933\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6789 - accuracy: 0.5933\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6788 - accuracy: 0.5933\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6774 - accuracy: 0.5933\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6772 - accuracy: 0.5933\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6775 - accuracy: 0.5933\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6778 - accuracy: 0.5933\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6779 - accuracy: 0.5933\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6771 - accuracy: 0.5933\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 566us/step - loss: 0.6772 - accuracy: 0.5933\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6772 - accuracy: 0.5933\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 584us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 555us/step - loss: 0.6768 - accuracy: 0.5933\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6771 - accuracy: 0.5933\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6765 - accuracy: 0.5933\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6768 - accuracy: 0.5933\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6756 - accuracy: 0.5933\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6768 - accuracy: 0.5933\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 572us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 584us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 592us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 576us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 579us/step - loss: 0.6760 - accuracy: 0.5933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 599us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 608us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 617us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 613us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 586us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 572us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "43/43 [==============================] - 0s 1000us/step\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 0s 1ms/step - loss: 0.6816 - accuracy: 0.5933\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6800 - accuracy: 0.5933\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6880 - accuracy: 0.5933\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6886 - accuracy: 0.5933\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 0s 564us/step - loss: 0.6861 - accuracy: 0.5933\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6817 - accuracy: 0.5933\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6840 - accuracy: 0.5933\n",
      "Epoch 8/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6824 - accuracy: 0.5933\n",
      "Epoch 9/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6846 - accuracy: 0.5933\n",
      "Epoch 10/100\n",
      "386/386 [==============================] - 0s 579us/step - loss: 0.6817 - accuracy: 0.5933\n",
      "Epoch 11/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6773 - accuracy: 0.5933\n",
      "Epoch 12/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6806 - accuracy: 0.5933\n",
      "Epoch 13/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6797 - accuracy: 0.5933\n",
      "Epoch 14/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6804 - accuracy: 0.5933\n",
      "Epoch 15/100\n",
      "386/386 [==============================] - 0s 642us/step - loss: 0.6795 - accuracy: 0.5933\n",
      "Epoch 16/100\n",
      "386/386 [==============================] - 0s 697us/step - loss: 0.6787 - accuracy: 0.5933\n",
      "Epoch 17/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6794 - accuracy: 0.5933\n",
      "Epoch 18/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 19/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6782 - accuracy: 0.5933\n",
      "Epoch 20/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6794 - accuracy: 0.5933\n",
      "Epoch 21/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6790 - accuracy: 0.5933\n",
      "Epoch 22/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6774 - accuracy: 0.5933\n",
      "Epoch 23/100\n",
      "386/386 [==============================] - 0s 558us/step - loss: 0.6773 - accuracy: 0.5933\n",
      "Epoch 24/100\n",
      "386/386 [==============================] - 0s 653us/step - loss: 0.6788 - accuracy: 0.5933\n",
      "Epoch 25/100\n",
      "386/386 [==============================] - 0s 741us/step - loss: 0.6776 - accuracy: 0.5933\n",
      "Epoch 26/100\n",
      "386/386 [==============================] - 0s 637us/step - loss: 0.6788 - accuracy: 0.5933\n",
      "Epoch 27/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6782 - accuracy: 0.5933\n",
      "Epoch 28/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 29/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6784 - accuracy: 0.5933\n",
      "Epoch 30/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6778 - accuracy: 0.5933\n",
      "Epoch 31/100\n",
      "386/386 [==============================] - 0s 587us/step - loss: 0.6767 - accuracy: 0.5933\n",
      "Epoch 32/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6771 - accuracy: 0.5933\n",
      "Epoch 33/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6770 - accuracy: 0.5933\n",
      "Epoch 34/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 35/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6766 - accuracy: 0.5933\n",
      "Epoch 36/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6769 - accuracy: 0.5933\n",
      "Epoch 37/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6765 - accuracy: 0.5933\n",
      "Epoch 38/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 39/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6773 - accuracy: 0.5933\n",
      "Epoch 40/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6763 - accuracy: 0.5933\n",
      "Epoch 41/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6764 - accuracy: 0.5933\n",
      "Epoch 42/100\n",
      "386/386 [==============================] - 0s 671us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 43/100\n",
      "386/386 [==============================] - 0s 574us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 44/100\n",
      "386/386 [==============================] - 0s 655us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 45/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 46/100\n",
      "386/386 [==============================] - 0s 710us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 47/100\n",
      "386/386 [==============================] - 0s 629us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 48/100\n",
      "386/386 [==============================] - 0s 614us/step - loss: 0.6753 - accuracy: 0.5933\n",
      "Epoch 49/100\n",
      "386/386 [==============================] - 0s 658us/step - loss: 0.6762 - accuracy: 0.5933\n",
      "Epoch 50/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 51/100\n",
      "386/386 [==============================] - 0s 596us/step - loss: 0.6765 - accuracy: 0.5933\n",
      "Epoch 52/100\n",
      "386/386 [==============================] - 0s 634us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 53/100\n",
      "386/386 [==============================] - 0s 707us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 54/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 55/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6761 - accuracy: 0.5933\n",
      "Epoch 56/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 57/100\n",
      "386/386 [==============================] - 0s 588us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 58/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 59/100\n",
      "386/386 [==============================] - 0s 572us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 60/100\n",
      "386/386 [==============================] - 0s 598us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 61/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 62/100\n",
      "386/386 [==============================] - 0s 626us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 63/100\n",
      "386/386 [==============================] - 0s 557us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 64/100\n",
      "386/386 [==============================] - 0s 611us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 65/100\n",
      "386/386 [==============================] - 0s 712us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 66/100\n",
      "386/386 [==============================] - 0s 633us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 67/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 68/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 69/100\n",
      "386/386 [==============================] - 0s 573us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 70/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6760 - accuracy: 0.5933\n",
      "Epoch 71/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 72/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 73/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 74/100\n",
      "386/386 [==============================] - 0s 585us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 75/100\n",
      "386/386 [==============================] - 0s 593us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 76/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 77/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 78/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 79/100\n",
      "386/386 [==============================] - 0s 580us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 80/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 81/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 82/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 83/100\n",
      "386/386 [==============================] - 0s 591us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 84/100\n",
      "386/386 [==============================] - 0s 567us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 85/100\n",
      "386/386 [==============================] - 0s 560us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 86/100\n",
      "386/386 [==============================] - 0s 570us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 87/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 88/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 89/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 90/100\n",
      "386/386 [==============================] - 0s 561us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "Epoch 91/100\n",
      "386/386 [==============================] - 0s 583us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 92/100\n",
      "386/386 [==============================] - 0s 575us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 93/100\n",
      "386/386 [==============================] - 0s 604us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 94/100\n",
      "386/386 [==============================] - 0s 562us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 95/100\n",
      "386/386 [==============================] - 0s 589us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 96/100\n",
      "386/386 [==============================] - 0s 549us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 97/100\n",
      "386/386 [==============================] - 0s 578us/step - loss: 0.6758 - accuracy: 0.5933\n",
      "Epoch 98/100\n",
      "386/386 [==============================] - 0s 554us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 99/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6757 - accuracy: 0.5933\n",
      "Epoch 100/100\n",
      "386/386 [==============================] - 0s 565us/step - loss: 0.6759 - accuracy: 0.5933\n",
      "43/43 [==============================] - 0s 983us/step\n",
      "Epoch 1/100\n",
      "387/387 [==============================] - 0s 1ms/step - loss: 0.6868 - accuracy: 0.5659\n",
      "Epoch 2/100\n",
      "387/387 [==============================] - 0s 566us/step - loss: 0.6881 - accuracy: 0.5917\n",
      "Epoch 3/100\n",
      "387/387 [==============================] - 0s 597us/step - loss: 0.6835 - accuracy: 0.5917\n",
      "Epoch 4/100\n",
      "387/387 [==============================] - 0s 576us/step - loss: 0.6844 - accuracy: 0.5917\n",
      "Epoch 5/100\n",
      "387/387 [==============================] - 0s 589us/step - loss: 0.6833 - accuracy: 0.5917\n",
      "Epoch 6/100\n",
      "387/387 [==============================] - 0s 563us/step - loss: 0.6841 - accuracy: 0.5917\n",
      "Epoch 7/100\n",
      "387/387 [==============================] - 0s 561us/step - loss: 0.6802 - accuracy: 0.5917\n",
      "Epoch 8/100\n",
      "387/387 [==============================] - 0s 563us/step - loss: 0.6828 - accuracy: 0.5917\n",
      "Epoch 9/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6815 - accuracy: 0.5917\n",
      "Epoch 10/100\n",
      "387/387 [==============================] - 0s 677us/step - loss: 0.6845 - accuracy: 0.5917\n",
      "Epoch 11/100\n",
      "387/387 [==============================] - 0s 672us/step - loss: 0.6814 - accuracy: 0.5917\n",
      "Epoch 12/100\n",
      "387/387 [==============================] - 0s 576us/step - loss: 0.6804 - accuracy: 0.5917\n",
      "Epoch 13/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6814 - accuracy: 0.5917\n",
      "Epoch 14/100\n",
      "387/387 [==============================] - 0s 580us/step - loss: 0.6806 - accuracy: 0.5917\n",
      "Epoch 15/100\n",
      "387/387 [==============================] - 0s 584us/step - loss: 0.6809 - accuracy: 0.5917\n",
      "Epoch 16/100\n",
      "387/387 [==============================] - 0s 563us/step - loss: 0.6800 - accuracy: 0.5917\n",
      "Epoch 17/100\n",
      "387/387 [==============================] - 0s 586us/step - loss: 0.6807 - accuracy: 0.5917\n",
      "Epoch 18/100\n",
      "387/387 [==============================] - 0s 566us/step - loss: 0.6791 - accuracy: 0.5917\n",
      "Epoch 19/100\n",
      "387/387 [==============================] - 0s 566us/step - loss: 0.6777 - accuracy: 0.5917\n",
      "Epoch 20/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6811 - accuracy: 0.5917\n",
      "Epoch 21/100\n",
      "387/387 [==============================] - 0s 543us/step - loss: 0.6789 - accuracy: 0.5917\n",
      "Epoch 22/100\n",
      "387/387 [==============================] - 0s 550us/step - loss: 0.6803 - accuracy: 0.5917\n",
      "Epoch 23/100\n",
      "387/387 [==============================] - 0s 558us/step - loss: 0.6772 - accuracy: 0.5917\n",
      "Epoch 24/100\n",
      "387/387 [==============================] - 0s 585us/step - loss: 0.6790 - accuracy: 0.5917\n",
      "Epoch 25/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6784 - accuracy: 0.5917\n",
      "Epoch 26/100\n",
      "387/387 [==============================] - 0s 721us/step - loss: 0.6786 - accuracy: 0.5917\n",
      "Epoch 27/100\n",
      "387/387 [==============================] - 0s 612us/step - loss: 0.6770 - accuracy: 0.5917\n",
      "Epoch 28/100\n",
      "387/387 [==============================] - 0s 700us/step - loss: 0.6783 - accuracy: 0.5917\n",
      "Epoch 29/100\n",
      "387/387 [==============================] - 0s 732us/step - loss: 0.6770 - accuracy: 0.5917\n",
      "Epoch 30/100\n",
      "387/387 [==============================] - 0s 592us/step - loss: 0.6769 - accuracy: 0.5917\n",
      "Epoch 31/100\n",
      "387/387 [==============================] - 0s 550us/step - loss: 0.6745 - accuracy: 0.5917\n",
      "Epoch 32/100\n",
      "387/387 [==============================] - 0s 581us/step - loss: 0.6812 - accuracy: 0.5917\n",
      "Epoch 33/100\n",
      "387/387 [==============================] - 0s 576us/step - loss: 0.6772 - accuracy: 0.5917\n",
      "Epoch 34/100\n",
      "387/387 [==============================] - 0s 566us/step - loss: 0.6781 - accuracy: 0.5917\n",
      "Epoch 35/100\n",
      "387/387 [==============================] - 0s 703us/step - loss: 0.6770 - accuracy: 0.5917\n",
      "Epoch 36/100\n",
      "387/387 [==============================] - 0s 668us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 37/100\n",
      "387/387 [==============================] - 0s 597us/step - loss: 0.6777 - accuracy: 0.5917\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 0s 630us/step - loss: 0.6769 - accuracy: 0.5917\n",
      "Epoch 39/100\n",
      "387/387 [==============================] - 0s 610us/step - loss: 0.6778 - accuracy: 0.5917\n",
      "Epoch 40/100\n",
      "387/387 [==============================] - 0s 563us/step - loss: 0.6770 - accuracy: 0.5917\n",
      "Epoch 41/100\n",
      "387/387 [==============================] - 0s 579us/step - loss: 0.6771 - accuracy: 0.5917\n",
      "Epoch 42/100\n",
      "387/387 [==============================] - 0s 557us/step - loss: 0.6770 - accuracy: 0.5917\n",
      "Epoch 43/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6767 - accuracy: 0.5917\n",
      "Epoch 44/100\n",
      "387/387 [==============================] - 0s 561us/step - loss: 0.6768 - accuracy: 0.5917\n",
      "Epoch 45/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6769 - accuracy: 0.5917\n",
      "Epoch 46/100\n",
      "387/387 [==============================] - 0s 576us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 47/100\n",
      "387/387 [==============================] - 0s 576us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 48/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 49/100\n",
      "387/387 [==============================] - 0s 543us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 50/100\n",
      "387/387 [==============================] - 0s 602us/step - loss: 0.6762 - accuracy: 0.5917\n",
      "Epoch 51/100\n",
      "387/387 [==============================] - 0s 691us/step - loss: 0.6776 - accuracy: 0.5917\n",
      "Epoch 52/100\n",
      "387/387 [==============================] - 0s 718us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 53/100\n",
      "387/387 [==============================] - 0s 620us/step - loss: 0.6766 - accuracy: 0.5917\n",
      "Epoch 54/100\n",
      "387/387 [==============================] - 0s 682us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 55/100\n",
      "387/387 [==============================] - 0s 718us/step - loss: 0.6767 - accuracy: 0.5917\n",
      "Epoch 56/100\n",
      "387/387 [==============================] - 0s 620us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 57/100\n",
      "387/387 [==============================] - 0s 587us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 58/100\n",
      "387/387 [==============================] - 0s 708us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 59/100\n",
      "387/387 [==============================] - 0s 678us/step - loss: 0.6762 - accuracy: 0.5917\n",
      "Epoch 60/100\n",
      "387/387 [==============================] - 0s 579us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 61/100\n",
      "387/387 [==============================] - 0s 556us/step - loss: 0.6767 - accuracy: 0.5917\n",
      "Epoch 62/100\n",
      "387/387 [==============================] - 0s 690us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 63/100\n",
      "387/387 [==============================] - 0s 662us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 64/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 65/100\n",
      "387/387 [==============================] - 0s 540us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 66/100\n",
      "387/387 [==============================] - 0s 584us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 67/100\n",
      "387/387 [==============================] - 0s 579us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 68/100\n",
      "387/387 [==============================] - 0s 566us/step - loss: 0.6762 - accuracy: 0.5917\n",
      "Epoch 69/100\n",
      "387/387 [==============================] - 0s 561us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 70/100\n",
      "387/387 [==============================] - 0s 586us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 71/100\n",
      "387/387 [==============================] - 0s 558us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 72/100\n",
      "387/387 [==============================] - 0s 543us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 73/100\n",
      "387/387 [==============================] - 0s 558us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 74/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 75/100\n",
      "387/387 [==============================] - 0s 623us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 76/100\n",
      "387/387 [==============================] - 0s 571us/step - loss: 0.6762 - accuracy: 0.5917\n",
      "Epoch 77/100\n",
      "387/387 [==============================] - 0s 563us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 78/100\n",
      "387/387 [==============================] - 0s 558us/step - loss: 0.6765 - accuracy: 0.5917\n",
      "Epoch 79/100\n",
      "387/387 [==============================] - 0s 649us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 80/100\n",
      "387/387 [==============================] - 0s 698us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 81/100\n",
      "387/387 [==============================] - 0s 587us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 82/100\n",
      "387/387 [==============================] - 0s 571us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 83/100\n",
      "387/387 [==============================] - 0s 556us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 84/100\n",
      "387/387 [==============================] - 0s 581us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 85/100\n",
      "387/387 [==============================] - 0s 576us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 86/100\n",
      "387/387 [==============================] - 0s 579us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 87/100\n",
      "387/387 [==============================] - 0s 561us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 88/100\n",
      "387/387 [==============================] - 0s 588us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 89/100\n",
      "387/387 [==============================] - 0s 571us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 90/100\n",
      "387/387 [==============================] - 0s 589us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 91/100\n",
      "387/387 [==============================] - 0s 579us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 92/100\n",
      "387/387 [==============================] - 0s 558us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 93/100\n",
      "387/387 [==============================] - 0s 581us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 94/100\n",
      "387/387 [==============================] - 0s 553us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 95/100\n",
      "387/387 [==============================] - 0s 584us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 96/100\n",
      "387/387 [==============================] - 0s 574us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 97/100\n",
      "387/387 [==============================] - 0s 587us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "Epoch 98/100\n",
      "387/387 [==============================] - 0s 561us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 99/100\n",
      "387/387 [==============================] - 0s 576us/step - loss: 0.6763 - accuracy: 0.5917\n",
      "Epoch 100/100\n",
      "387/387 [==============================] - 0s 566us/step - loss: 0.6764 - accuracy: 0.5917\n",
      "42/42 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 59.21% (1.10%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "429/429 [==============================] - 0s 1ms/step - loss: 0.6897 - accuracy: 0.5711\n",
      "Epoch 2/100\n",
      "429/429 [==============================] - 0s 615us/step - loss: 0.6832 - accuracy: 0.5921\n",
      "Epoch 3/100\n",
      "429/429 [==============================] - 0s 604us/step - loss: 0.6821 - accuracy: 0.5921\n",
      "Epoch 4/100\n",
      "429/429 [==============================] - 0s 628us/step - loss: 0.6833 - accuracy: 0.5921\n",
      "Epoch 5/100\n",
      "429/429 [==============================] - 0s 608us/step - loss: 0.6772 - accuracy: 0.5921\n",
      "Epoch 6/100\n",
      "429/429 [==============================] - 0s 597us/step - loss: 0.6840 - accuracy: 0.5921\n",
      "Epoch 7/100\n",
      "429/429 [==============================] - 0s 615us/step - loss: 0.6814 - accuracy: 0.5921\n",
      "Epoch 8/100\n",
      "429/429 [==============================] - 0s 615us/step - loss: 0.6857 - accuracy: 0.5921\n",
      "Epoch 9/100\n",
      "429/429 [==============================] - 0s 776us/step - loss: 0.6796 - accuracy: 0.5921\n",
      "Epoch 10/100\n",
      "429/429 [==============================] - 0s 646us/step - loss: 0.6815 - accuracy: 0.5921\n",
      "Epoch 11/100\n",
      "429/429 [==============================] - 0s 615us/step - loss: 0.6783 - accuracy: 0.5921\n",
      "Epoch 12/100\n",
      "429/429 [==============================] - 0s 762us/step - loss: 0.6806 - accuracy: 0.5921\n",
      "Epoch 13/100\n",
      "429/429 [==============================] - 0s 678us/step - loss: 0.6788 - accuracy: 0.5921\n",
      "Epoch 14/100\n",
      "429/429 [==============================] - 0s 639us/step - loss: 0.6804 - accuracy: 0.5921\n",
      "Epoch 15/100\n",
      "429/429 [==============================] - 0s 718us/step - loss: 0.6798 - accuracy: 0.5921\n",
      "Epoch 16/100\n",
      "429/429 [==============================] - 0s 690us/step - loss: 0.6817 - accuracy: 0.5921\n",
      "Epoch 17/100\n",
      "429/429 [==============================] - 0s 618us/step - loss: 0.6808 - accuracy: 0.5921\n",
      "Epoch 18/100\n",
      "429/429 [==============================] - 0s 626us/step - loss: 0.6792 - accuracy: 0.5921\n",
      "Epoch 19/100\n",
      "429/429 [==============================] - 0s 629us/step - loss: 0.6790 - accuracy: 0.5921\n",
      "Epoch 20/100\n",
      "429/429 [==============================] - 0s 765us/step - loss: 0.6773 - accuracy: 0.5921\n",
      "Epoch 21/100\n",
      "429/429 [==============================] - 0s 662us/step - loss: 0.6798 - accuracy: 0.5921\n",
      "Epoch 22/100\n",
      "429/429 [==============================] - 0s 620us/step - loss: 0.6781 - accuracy: 0.5921\n",
      "Epoch 23/100\n",
      "429/429 [==============================] - 0s 725us/step - loss: 0.6782 - accuracy: 0.5921\n",
      "Epoch 24/100\n",
      "429/429 [==============================] - 0s 667us/step - loss: 0.6782 - accuracy: 0.5921\n",
      "Epoch 25/100\n",
      "429/429 [==============================] - 0s 622us/step - loss: 0.6776 - accuracy: 0.5921\n",
      "Epoch 26/100\n",
      "429/429 [==============================] - 0s 606us/step - loss: 0.6771 - accuracy: 0.5921\n",
      "Epoch 27/100\n",
      "429/429 [==============================] - 0s 615us/step - loss: 0.6776 - accuracy: 0.5921\n",
      "Epoch 28/100\n",
      "429/429 [==============================] - 0s 685us/step - loss: 0.6794 - accuracy: 0.5921\n",
      "Epoch 29/100\n",
      "429/429 [==============================] - 0s 683us/step - loss: 0.6787 - accuracy: 0.5921\n",
      "Epoch 30/100\n",
      "429/429 [==============================] - 0s 599us/step - loss: 0.6769 - accuracy: 0.5921\n",
      "Epoch 31/100\n",
      "429/429 [==============================] - 0s 622us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 32/100\n",
      "429/429 [==============================] - 0s 601us/step - loss: 0.6764 - accuracy: 0.5921\n",
      "Epoch 33/100\n",
      "429/429 [==============================] - 0s 611us/step - loss: 0.6788 - accuracy: 0.5921\n",
      "Epoch 34/100\n",
      "429/429 [==============================] - 0s 618us/step - loss: 0.6770 - accuracy: 0.5921\n",
      "Epoch 35/100\n",
      "429/429 [==============================] - 0s 636us/step - loss: 0.6768 - accuracy: 0.5921\n",
      "Epoch 36/100\n",
      "429/429 [==============================] - 0s 614us/step - loss: 0.6778 - accuracy: 0.5921\n",
      "Epoch 37/100\n",
      "429/429 [==============================] - 0s 664us/step - loss: 0.6771 - accuracy: 0.5921\n",
      "Epoch 38/100\n",
      "429/429 [==============================] - 0s 769us/step - loss: 0.6776 - accuracy: 0.5921\n",
      "Epoch 39/100\n",
      "429/429 [==============================] - 0s 587us/step - loss: 0.6764 - accuracy: 0.5921\n",
      "Epoch 40/100\n",
      "429/429 [==============================] - 0s 657us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 41/100\n",
      "429/429 [==============================] - 0s 699us/step - loss: 0.6767 - accuracy: 0.5921\n",
      "Epoch 42/100\n",
      "429/429 [==============================] - 0s 685us/step - loss: 0.6772 - accuracy: 0.5921\n",
      "Epoch 43/100\n",
      "429/429 [==============================] - 0s 662us/step - loss: 0.6773 - accuracy: 0.5921\n",
      "Epoch 44/100\n",
      "429/429 [==============================] - 0s 625us/step - loss: 0.6772 - accuracy: 0.5921\n",
      "Epoch 45/100\n",
      "429/429 [==============================] - 0s 660us/step - loss: 0.6773 - accuracy: 0.5921\n",
      "Epoch 46/100\n",
      "429/429 [==============================] - 0s 720us/step - loss: 0.6768 - accuracy: 0.5921\n",
      "Epoch 47/100\n",
      "429/429 [==============================] - 0s 648us/step - loss: 0.6770 - accuracy: 0.5921\n",
      "Epoch 48/100\n",
      "429/429 [==============================] - 0s 552us/step - loss: 0.6769 - accuracy: 0.5921\n",
      "Epoch 49/100\n",
      "429/429 [==============================] - 0s 681us/step - loss: 0.6766 - accuracy: 0.5921\n",
      "Epoch 50/100\n",
      "429/429 [==============================] - 0s 695us/step - loss: 0.6767 - accuracy: 0.5921\n",
      "Epoch 51/100\n",
      "429/429 [==============================] - 0s 802us/step - loss: 0.6764 - accuracy: 0.5921\n",
      "Epoch 52/100\n",
      "429/429 [==============================] - 0s 768us/step - loss: 0.6765 - accuracy: 0.5921\n",
      "Epoch 53/100\n",
      "429/429 [==============================] - 0s 741us/step - loss: 0.6776 - accuracy: 0.5921\n",
      "Epoch 54/100\n",
      "429/429 [==============================] - 0s 678us/step - loss: 0.6766 - accuracy: 0.5921\n",
      "Epoch 55/100\n",
      "429/429 [==============================] - 0s 569us/step - loss: 0.6767 - accuracy: 0.5921\n",
      "Epoch 56/100\n",
      "429/429 [==============================] - 0s 548us/step - loss: 0.6766 - accuracy: 0.5921\n",
      "Epoch 57/100\n",
      "429/429 [==============================] - 0s 566us/step - loss: 0.6764 - accuracy: 0.5921\n",
      "Epoch 58/100\n",
      "429/429 [==============================] - 0s 557us/step - loss: 0.6766 - accuracy: 0.5921\n",
      "Epoch 59/100\n",
      "429/429 [==============================] - 0s 566us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 60/100\n",
      "429/429 [==============================] - 0s 564us/step - loss: 0.6764 - accuracy: 0.5921\n",
      "Epoch 61/100\n",
      "429/429 [==============================] - 0s 559us/step - loss: 0.6765 - accuracy: 0.5921\n",
      "Epoch 62/100\n",
      "429/429 [==============================] - 0s 574us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 63/100\n",
      "429/429 [==============================] - 0s 576us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 64/100\n",
      "429/429 [==============================] - 0s 564us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 65/100\n",
      "429/429 [==============================] - 0s 552us/step - loss: 0.6765 - accuracy: 0.5921\n",
      "Epoch 66/100\n",
      "429/429 [==============================] - 0s 653us/step - loss: 0.6764 - accuracy: 0.5921\n",
      "Epoch 67/100\n",
      "429/429 [==============================] - 0s 696us/step - loss: 0.6761 - accuracy: 0.5921\n",
      "Epoch 68/100\n",
      "429/429 [==============================] - 0s 627us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 69/100\n",
      "429/429 [==============================] - 0s 564us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 70/100\n",
      "429/429 [==============================] - 0s 583us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 71/100\n",
      "429/429 [==============================] - 0s 627us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 72/100\n",
      "429/429 [==============================] - 0s 676us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 73/100\n",
      "429/429 [==============================] - 0s 667us/step - loss: 0.6764 - accuracy: 0.5921\n",
      "Epoch 74/100\n",
      "429/429 [==============================] - 0s 573us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 75/100\n",
      "429/429 [==============================] - 0s 583us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 76/100\n",
      "429/429 [==============================] - 0s 707us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 77/100\n",
      "429/429 [==============================] - 0s 641us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 78/100\n",
      "429/429 [==============================] - 0s 606us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 79/100\n",
      "429/429 [==============================] - 0s 647us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 80/100\n",
      "429/429 [==============================] - 0s 646us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 81/100\n",
      "429/429 [==============================] - 0s 667us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 82/100\n",
      "429/429 [==============================] - 0s 685us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 83/100\n",
      "429/429 [==============================] - 0s 550us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 84/100\n",
      "429/429 [==============================] - 0s 606us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 85/100\n",
      "429/429 [==============================] - 0s 641us/step - loss: 0.6761 - accuracy: 0.5921\n",
      "Epoch 86/100\n",
      "429/429 [==============================] - 0s 562us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 87/100\n",
      "429/429 [==============================] - 0s 634us/step - loss: 0.6761 - accuracy: 0.5921\n",
      "Epoch 88/100\n",
      "429/429 [==============================] - 0s 678us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 89/100\n",
      "429/429 [==============================] - 0s 594us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 90/100\n",
      "429/429 [==============================] - 0s 634us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 91/100\n",
      "429/429 [==============================] - 0s 774us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 92/100\n",
      "429/429 [==============================] - 0s 706us/step - loss: 0.6761 - accuracy: 0.5921\n",
      "Epoch 93/100\n",
      "429/429 [==============================] - 0s 746us/step - loss: 0.6761 - accuracy: 0.5921\n",
      "Epoch 94/100\n",
      "429/429 [==============================] - 0s 613us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 95/100\n",
      "429/429 [==============================] - 0s 615us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 96/100\n",
      "429/429 [==============================] - 0s 776us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 97/100\n",
      "429/429 [==============================] - 0s 594us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 98/100\n",
      "429/429 [==============================] - 0s 562us/step - loss: 0.6763 - accuracy: 0.5921\n",
      "Epoch 99/100\n",
      "429/429 [==============================] - 0s 548us/step - loss: 0.6762 - accuracy: 0.5921\n",
      "Epoch 100/100\n",
      "429/429 [==============================] - 0s 573us/step - loss: 0.6762 - accuracy: 0.5921\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2279714d648>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(X_train, y_train) # https://stackoverflow.com/questions/39467496/error-when-using-keras-sk-learn-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 203us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int8)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: nan% (nan%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "TypeError: __call__() missing 1 required positional argument: 'inputs'\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(pipeline, X, y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.594, Test: 0.594\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(X, y, validation_data=(X_train, y_train), epochs=100, verbose=0)\n",
    "# evaluate the model\n",
    "_, train_mse = model.evaluate(X, y, verbose=0)\n",
    "_, test_mse = model.evaluate(X, y, verbose=0)\n",
    "print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n",
    "# plot loss during training\n",
    "plt.subplot(211)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "# plot mse during training\n",
    "plt.subplot(212)\n",
    "plt.title('Mean Squared Error')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/datadriveninvestor/building-neural-network-using-keras-for-classification-3a3656c726c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
